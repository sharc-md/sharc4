\documentclass[a4paper,10pt,DIV=15,openany]{scrbook}

% everything inside \tthdump is ignored by tth, but processed by LaTeX
\newcommand{\tthdump}[1]{#1}

% #==================================================================
% |
% | If TTH (tex to html) is used instead of TTM (tex to mathml/xml)
% | do the following replacements (with search and replace):
% |
% | for TTH      for TTM
% | \tthdump <-> \ttmdump
% | %%tth:   <-> %%ttm: 
% |
% |
% | ************************************************
% | DO NOT DELETE THE %%tth: COMMENTS IN THIS FILE!!
% | ************************************************
% |
% |
% # =================================================================



% include most packages
\tthdump{
  \usepackage[utf8]{inputenc}
  \usepackage[T1]{fontenc}
}
\usepackage{
  listings,
  braket,
  fancyvrb,
  booktabs,
  colortbl,
  mystuff,
  lastpage,
  array,
  longtable,
  framed,
  amsmath,
  amssymb,
  multirow,
  url,
  framed,
  flafter,
  microtype}
\tthdump{
  \DisableLigatures[-]{family=tt*}
}
\usepackage[colorlinks=true,urlcolor=V,linkcolor=B,citecolor=B]{hyperref}
\usepackage[square, sort&compress, numbers]{natbib}
% hide some packages from tth
\tthdump{
  \usepackage[version=3]{mhchem}
  \usepackage{tikz,pgfplots}
  \usetikzlibrary{arrows}
  \pgfplotsset{compat=1.3}
}
% fonts
% \usepackage{pxfonts}
\usepackage{libertine}
\usepackage[libertine]{newtxmath}
% \usepackage[scaled=0.95]{inconsolata}
\usepackage[scaled=0.8]{beramono}
% \renewcommand*{\sfdefault}{lmss}



% Bold caption labels
\tthdump{
  \setkomafont{captionlabel}{\bfseries}
}



% Code for external link with symbol (little box with diagonal arrow)
\tthdump{
  \newcommand{\ExternalLink}{%
      \tikz[x=1.2ex, y=1.2ex, baseline=-0.05ex]{% 
          \begin{scope}[x=1ex, y=1ex]
              \clip (-0.1,-0.1) 
                  --++ (-0, 1.2) 
                  --++ (0.6, 0) 
                  --++ (0, -0.6) 
                  --++ (0.6, 0) 
                  --++ (0, -1);
              \path[draw, 
                  line width = 0.5, 
                  rounded corners=0.5] 
                  (0,0) rectangle (1,1);
          \end{scope}
          \path[draw, line width = 0.5] (0.5, 0.5) 
              -- (1, 1);
          \path[draw, line width = 0.5] (0.6, 1) 
              -- (1, 1) -- (1, 0.6);
          }
      }
}
\makeatletter
\tthdump{
  \newcommand*{\link}{\begingroup\@makeother\#\@link}
  \newcommand*{\@link}[2]{%
    \href{#1}{\ExternalLink\ifthenelse{\equal{#2}{}}{#1}{#2}}%
    \endgroup}
}
\makeatother
%%tth: \newcommand{\link}[2]{\href{#1}{#2}}



% Paragraph settings
\setlength{\parindent}{0pt}
\setlength{\parskip}{\smallskipamount}
% use a different parsep for lists
\usepackage{enumitem}
\setlist[itemize]{parsep=0pt}
\setlist[enumerate]{parsep=0pt}



% no clubs and widows
\clubpenalty = 2000
\widowpenalty = 2000 
\displaywidowpenalty = 2000
\renewcommand{\textfraction}{0.01}
\renewcommand{\topfraction}{0.9}
\renewcommand{\bottomfraction}{0.8}
\renewcommand{\dbltopfraction}{0.9}
\setcounter{topnumber}{2}
\setcounter{bottomnumber}{2}




% =============================== TITLE ==========================
%%tth: \title{SHARC: Surface Hopping in the Adiabatic Representation Including Arbitrary Couplings---Manual}
\newenvironment{tpage}{%
\KOMAoptions{twoside = false}
\addtokomafont{title}{\rmfamily}
  \begin{titlepage}
    \title{\hspace{1cm}\includegraphics[width=0.8\textwidth,keepaspectratio=true]{img/sharc_new.pdf}\\[0.5cm]
    SHARC2.0:\\ Surface Hopping Including\\ Arbitrary Couplings}
%     SHARC: Surface Hopping in the Adiabatic Representation Including Arbitrary Couplings}
    \subtitle{Manual\\[1cm]Version 2.0}
    \date{Vienna, \today}
    \author{AG Gonz\'alez\\
Institute of Theoretical Chemistry\\
University of Vienna, Austria
\vspace{1cm}
\\
\includegraphics[width=0.4\textwidth,keepaspectratio=true]{img/univie.pdf}}

    \maketitle
  \end{titlepage}
}{}



% =============================== HEADER AND FOOT ==========================
\tthdump{
  \usepackage[automark]{scrpage2}
  \pagestyle{scrheadings}
  \clearscrheadfoot
}

% =============================== KEYWORDS ==========================

\newcommand{\sharc}{\textsc{Sharc}}

\newcommand{\todo}[1]{\textcolor{RL}{#1}}

\newcommand{\ttt}[1]{\textbf{\texttt{#1}}}

\newcommand{\E}{\ensuremath{\mathrm{e}}}
\newcommand{\I}{\ensuremath{\mathrm{i}}}
\newcommand{\D}{\ensuremath{\mathrm{d}}}
%%tth: \newcommand{\VEC}[1]{\mathbf{#1}}
\tthdump{
  \newcommand{\VEC}[1]{\ensuremath{\mathbf{#1}}}
}
%%tth: \def\l{l}
%%tth: \def\middle{}
%%tth: \def\mathfrak#1{\mathcal{#1}}
%%tth: \def\ce#1{#1}

% unnumbered things
\newenvironment{unnumbered}%
{\setcounter{secnumdepth}{-1}}
{\setcounter{secnumdepth}{2}}

% shaded boxes
\definecolor{shadecolor}{HTML}{BBDDFF}

\newenvironment{example}{
  \setlength{\OuterFrameSep}{3pt}
  \vspace{0mm}
  \definecolor{shadecolor}{HTML}{E4F4FF}
  \begin{shaded}
}{
  \end{shaded}
}

\usepackage{etoolbox}
\makeatletter
\preto{\@verbatim}{\topsep=0pt \partopsep=0pt }
\makeatother

% ========================================================================================================= %
% ========================================================================================================= %
% ========================================================================================================= %

\begin{document}

\tthdump{
  \pdfbookmark[0]{Title Page}{titlebookmark}
}
\tpage

\tthdump{
  \begin{shaded}
    \pdfbookmark[0]{Contact}{contactbookmark}
    \textbf{Contact:}

    \begin{tabular}{ll}
      \\
      \multicolumn{2}{l}{AG Gonz\'alez}\\
      \multicolumn{2}{l}{Institute of Theoretical Chemistry, University of Vienna}\\
      \multicolumn{2}{l}{W\"ahringer Stra\ss{}e 17}\\
      \multicolumn{2}{l}{1090 Vienna, Austria}\\
      \\
      Website: &\link{http://sharc-md.org}{sharc-md.org}\\
      Email: &\link{mailto:sharc@univie.ac.at}{sharc@univie.ac.at}\\
    \end{tabular}
  \end{shaded}
}

\newpage
\ihead{\textsc{Sharc} Manual}
\ohead{\leftmark\quad {\normalfont|} \quad\rightmark}
\ofoot[\pagemark]{\pagemark}

% ========================================================================================================= %
% ========================================================================================================= %
% ========================================================================================================= %

\tthdump{
  \pdfbookmark[0]{Contents}{tocbookmark}
}
\tableofcontents
% \layout

% ========================================================================================================= %
% ========================================================================================================= %
% ========================================================================================================= %

\chapter{Introduction}

% what is nonadiabatic dynamics
When a molecule is irradiated by light, a number of dynamical processes can take place, in which the molecule redistributes the energy among different electronic and vibrational degrees of freedom. Kasha's rule \cite{Kasha1950DFS} states that radiationless transfer from higher excited singlet states to the lowest-lying excited singlet state ($S_1$) is faster than fluorescence (F). This radiationless transfer is called internal conversion (IC) and involves a changes between electronic states of the same multiplicity. If a transition occurs between electronic states of different spin, the process is called intersystem crossing (ISC). A typical ISC process is from a singlet to a triplet state, and once the lowest triplet is populated, phosphorescence (P) can take place. In figure~\ref{fig:jablonski}, radiative (F and P) and radiationless (IC and ISC) processes are summarized in a so-called Jab{\l}onski diagram.

\begin{figure}[!ht]
  \centering
  \includegraphics[scale=1.6]{img/jablonski/jablonski.pdf}
  \caption[Jab{\l}onski diagram showing the conceptual photophysical processes.]{Jab{\l}onski diagram showing the conceptual photophysical processes. Straight arrows show radiative processes: absorption ($h\nu$), fluorescence (F), and phosphorescence (P); wavy arrows show radiationless processes: internal conversion (IC) and intersystem crossing (ISC). }
  \label{fig:jablonski}
\end{figure}

The non-radiative IC and ISC processes are fundamental concepts which play a decisive role in photochemistry and photobiology. IC processes are present in the excited-state dynamics of many organic and inorganic molecules, whose applications range from solar energy conversion to drug therapy. Even many, very small molecules, for example \ce{O2} and \ce{O3}, \ce{SO2}, \ce{NO2} and other nitrous oxides, show efficient IC, which has important consequences in atmospheric chemistry and the study of the environment and pollution. IC is also the first step of the biological process of visual perception, where the retinal moiety of rhodopsin absorbs a photon and non-radiatively performs a torsion around one of the double bonds, changing the conformation of the protein and inducing a neural signal. Similarly, protection of the human body from the influence of UV light is achieved through very efficient IC in DNA, proteins and melanins. Ultrafast IC to the electronic ground state allows quickly converting the excitation energy of the UV photons into nuclear kinetic energy, which is spread harmlessly as heat to the environment.

ISC processes are completely forbidden in the frame of the non-relativistic Schr\"odinger equation, but they become allowed when including spin-orbit couplings, a relativistic effect~\cite{Marian2012WCMS}. Spin-orbit coupling depends on the nuclear charge and becomes stronger for heavy atoms, therefore it is typically known as a ''heavy atom'' effect. However, it has been recently recognized that even for molecules with only first- and second-row atoms, ISC might be relevant and can be competitive in time scales with IC. A small selection of the growing number of molecules where efficient ISC in a sub-ps time scale has been predicted are \ce{SO2}~\cite{Wilkinson2014JCP,Mai2014JCP_SO2,Leveque2014JCP_ISC}, benzene~\cite{Penfold2012JCP}, aromatic nitrocompounds~\cite{Vogt2013JPC} or DNA nucleobases and derivatives~\cite{Crespo-Hernandez2004CR, Richter2012JPCL, Martinez-Fernandez2012CC, Mai2013C, Reichardt2010CC}.

% dynamics simulations
Theoretical simulations can greatly contribute to understand non-radiative processes by following the nuclear motion on the excited-state potential energy surfaces (PES) in real time. These simulations are called excited-state dynamics simulations. 
Since the Born-Oppenheimer approximation is not applicable for this kind of dynamics, nonadiabatic effects need to be incorporated into the simulations.

The principal methodology to tackle excited-state dynamics simulations is to numerically integrate the time-dependent Schr\"odinger equation, which is usually called full quantum dynamics simulations (QD). Given accurate PESs, QD is able to match experimental accuracy. However, the need for the ''a priori'' knowledge of the full multi-dimensional PES renders this type of simulations quickly unfeasible for more than few degrees of freedom. Several alternative methodologies are possible to alleviate this problem. One of the most popular ones is to use surface hopping nonadiabatic dynamics.

% surface hopping, advantages, history
Surface hopping was originally devised by Tully~\cite{Tully1971JCP} and greatly improved later by the ``fewest-switches criterion''\cite{Tully1990JCP} and it has been reviewed extensively since then, see e.g.~\cite{Barbatti2011WCMS,Subotnik2016ARPC,Wang2016JPCL,Doltsinis2006,Doltsinis2002JTCC}.
In surface hopping, the motion of the excited-state wave packet is approximated by the motion of an ensemble of many independent, classical trajectories. Each trajectory is at every instant of time tied to one particular PES, and the nuclear motion is integrated using the gradient of this PES. However, nonadiabatic population transfer can lead to the switching of a trajectory from one PES to another PES. This switching (also called ``hopping'', which is the origin of the name ``surface hopping'') is based on a stochastic algorithm, taking into account the change of the electronic population from one time step to the next one.

The advantages of the surface hopping methodology and thus its popularity are well summarized in Ref.~\cite{Barbatti2011WCMS}:
\begin{itemize}
  \item The method is conceptually simple, since it is based on classical mechanics. The nuclear propagation is based on Newton's equations and can be performed in Cartesian coordinates, avoiding any problems with curved coordinate systems as in QD.
  \item For the propagation of the trajectories only local information of the PESs is needed. This avoids the calculation of the full, multi-dimensional PES in advance, which is the main bottleneck of QD methods. In surface hopping dynamics, all degrees of freedom can be included in the simulation. Additionally, all necessary quantities can be calculated on-demand, usually called ``on-the-fly'' in this context.
  \item The independent trajectories can be trivially parallelized.
\end{itemize}
The strongest of these points of course is the fact that all degrees of freedom can be included easily in the calculations, allowing to describe large systems.
%like DNA nucleobases, transition metal complexes and even large DNA strands and solvated molecules (by means of QM/MM schemes).
One should note, however, that surface hopping methods in the standard formulation~\cite{Tully1971JCP, Tully1990JCP}---due to  the classical nature of the trajectories---do not allow to treat some purely quantum-mechanical effects like tunneling, (tunneling for selected degrees of freedom is possible \cite{Hammes-Schiffer1994JCP}). Additionally, quantum coherence between the electronic states is usually described poorly, because of the independent-trajectory ansatz. This can be treated with some ad-hoc corrections, e.g., in \cite{Granucci2007JCP}.

% SHARC
In the original surface hopping method, only nonadiabatic couplings are considered, only allowing for population transfer between electronic states of the same multiplicity (IC). 
The \sharc\ methodology is a generalization of standard surface hopping since it allows to include any type of coupling. Beyond nonadiabatic couplings (for IC), spin-orbit couplings (for ISC) or interactions of dipole moments with electric fields (to explicitly describe laser-induced processes) can be included.
A number of methodologies for surface hopping including one or the other type of potential couplings have been proposed in references~\cite{Thachuk1996JCP,Maiti2004JPCA,Jones2008JPCA,Mitric2009PRA,Granucci2012JCP,Curchod2013C,Cui2014JCP}, but \sharc\ can include all types of potential couplings on the same footing. 

The \sharc\ methodology is an extension to standard surface hopping which allows to include these kinds of couplings. The central idea of \sharc\ is to obtain a fully diagonal Hamiltonian, which is adiabatic with respect to all couplings. The diagonal Hamiltonian is obtained by unitary transformation of the Hamiltonian including all couplings. Surface hopping is conducted on the transformed electronic states.
This has a number of advantages over the standard surface hopping methodology, where no diagonalization is performed:
\begin{itemize}
  \item Potential couplings (like spin-orbit couplings and laser-dipole couplings) are usually delocalized. Surface hopping, however, rests on the assumption that the couplings are localized and hence surface hops only occur in the small region where the couplings are large. Within \sharc, by transforming away the potential couplings, additional terms of nonadiabatic (kinetic) couplings arise, which are localized. 
  \item The potential couplings have an influence on the gradients acting on the nuclei. To a good approximation, within \sharc\ it is possible to include this influence in the dynamics.
  \item When including spin-orbit couplings for states of higher multiplicity, diagonalization solves the problem of rotational invariance of the multiplet components (see~\cite{Granucci2012JCP}). 
\end{itemize}

The \sharc\ suite of programs is an implementation of the \sharc\ method. Besides the core dynamics code, it comes with a number of tools aiding in the setup, maintenance and analysis of the trajectories. 

\section{Capabilities}

The main features of the \sharc{}2.0 suite are:
\begin{itemize}
  \item Non-adiabatic dynamics based on the surface hopping methodology able to describe internal conversion and intersystem crossing with any number of states (singlets, doublets, triplets, or higher multiplicities).
  \item Algorithms for stable wave function propagation in the presence of very small or very large couplings.
  \item Inclusion of interactions with laser fields in the long-wavelength limit. The derivatives of the dipole moments can be included in strong-field applications.
  \item Propagation using either nonadiabatic couplings vectors $\langle\alpha|\frac{\partial}{\partial \mathbf{R}}|\beta\rangle$ or wave function overlaps $\langle\alpha(t_0)|\beta(t)\rangle$ (via the local diabatization procedure \cite{Granucci2007JCP}).
  \item Gradients including the effects of spin-orbit couplings (with the approximation that the diabatic spin-orbit couplings are slowly varying).
  \item Flexible interface to quantum chemistry programs. Existing interfaces to:
  \begin{itemize}
    \item \textsc{Molpro} 2010 and 2012: SA-CASSCF
    \item \textsc{Openmolcas} 18.0: SA-CASSCF, SS-CASPT2, MS-CASPT2, SA-CASSCF+QM/MM
    \item \textsc{Columbus} 7: SA-CASSCF, SA-RASSCF, MR-CISD
    \item \textsc{ADF} 2017+: TD-DFT, TD-DFT+QM/MM
    \item \textsc{Turbomole} 7: ADC(2), CC2
    \item \textsc{Gaussian} 09 and 16: TD-DFT
    \item \textsc{Orca} 4.1: TD-DFT
    \item Interface for analytical potentials
    \item Interface for linear-vibronic coupling (LVC) models
  \end{itemize}
  \item Energy-difference-based partial coupling approximation to speed up calculations \cite{Pittner2009CP}.
  \item Energy-based decoherence correction \cite{Granucci2007JCP} or augmented-FSSH decoherence correction \cite{Jain2016JCTC}.
  \item Calculation of Dyson norms for single-photon ionization spectra (for most interfaces) \cite{Ruckenbauer2016SR}.
  \item On-the-fly wave function analysis with TheoDORE \cite{Plasser2014JCP1, Plasser2014JCP2, Plasser2017TheoDORE} (for some interfaces).
  \item Suite of auxiliary Python scripts for all steps of the setup procedure and for various analysis tasks.
  \item Comprehensive tutorial.
\end{itemize}

\subsection{New features in \sharc\ Version 2.0}

These features are new in \sharc\ Version 2.0 (2018):
\begin{itemize}
  \item Dynamics program \ttt{sharc.x}:
  \begin{itemize}
    \item New methods: AFSSH for decoherence, GFSH for hopping probabilities, reflection after frustrated hop.
    \item Atom masking for size-extensive decoherence and rescaling.
    \item Improved wave function and $\VEC{U}$ matrix phase tracking.
    \item Support for on-the-fly computation of Dyson norms and \textsc{TheoDORE} descriptors.
    \item Option to gracefully stop trajectories after any time step.
  \end{itemize}
  \item Fully integrated, efficient wave function overlap program \ttt{wfoverlap.x}
  \item Quantum chemistry interfaces:
  \begin{itemize}
    \item \textsc{Molpro}: overhauled, uses \ttt{wfoverlap.x}, gives consistent phase between CASSCF and CI wave functions, can do Dyson norms, parallelizes independent job parts.
    \item \textsc{Molcas}: overhauled, can do (MS)-CASPT2 (only numerical gradients), QM/MM, Cholesky decomposition, Dyson norms, parallelizes independent job parts, works with \textsc{Openmolcas} version 18.
    \item \textsc{Columbus}: overhauled, uses \ttt{wfoverlap.x}, can use \textsc{Dalton} integrals, can use \textsc{Molcas} orbitals, can do Dyson norms.
    \item \textsc{Analytical}: ---
    \item \textsc{Turbomole}: new interface, can do ADC(2) and CC2; has SOC (for ADC(2)), uses \ttt{wfoverlap.x}, works with \textsc{TheoDORE}.
    \item \textsc{ADF}: new interface, can do TD-DFT; has SOC, uses \ttt{wfoverlap.x}, Dyson norms, has QM/MM, works with \textsc{TheoDORE}.
    \item \textsc{Gaussian}: new interface, can do TD-DFT; uses \ttt{wfoverlap.x}, has Dyson norms, works with \textsc{TheoDORE}.
    \item LVC: new interface, can do (analytical) linear vibronic coupling models.
  \end{itemize}
  \item Auxilliary scripts:
  \begin{itemize}
    \item \ttt{wigner.py}: elevated temperature sampling, LVC model setup.
    \item \ttt{amber\_to\_initconds.py}: new script, converts \textsc{Amber} trajectories to \sharc\ initial conditions.
    \item \ttt{sharctraj\_to\_initconds.py}: new script, converts \sharc\ trajectories to \sharc\ initial conditions.
    \item \ttt{spectrum.py}: log-normal convolution, density of state spectra.
    \item \ttt{data\_extractor.x}: new quantities to extract.
    \item \ttt{diagnostics.py}: new script, checks all trajectories prior to analysis.
    \item \ttt{populations.py}: new analysis modes.
    \item \ttt{transition.py}: new script, computes total number of hops in ensemble.
    \item \ttt{make\_fitscript.py}: new script, prepares kinetic model fits to obtain time constants from populations.
    \item \ttt{bootstrap.py}: new script, calculates error estimates for time constants.
    \item \ttt{trajana\_essdyn.py}: new script, performs essential dynamics analysis.
    \item \ttt{trajana\_nma.py}: new script, performs normal mode analysis.
    \item \ttt{data\_collector.py}: new script, performs generic data analysis (collecting, smoothing, convolution, integration).
    \item \ttt{orca\_External}: new script, allows optimization with \textsc{Orca} and \sharc.
    \item Several input generation helpers.
  \end{itemize}
  \item Reworked tutorial using \textsc{Openmolcas} (which is available at no cost).
\end{itemize}


\subsection{New features in \sharc\ Version 2.1}

These features are new in \sharc\ Version 2.1 (2019):
\begin{itemize}
  \item Dynamics program \ttt{sharc.x}:
  \begin{itemize}
    \item New methods: Rescaling along gradient difference vector.
  \end{itemize}
  \item Quantum chemistry interfaces:
  \begin{itemize}
    \item \textsc{Molcas}: works with \textsc{Openmolcas} version 18 or \textsc{Molcas} 8.4.
    \item \textsc{Orca}: new interface, can do TD-DFT; uses \ttt{wfoverlap.x}, has Dyson norms, works with \textsc{TheoDORE}.
  \end{itemize}
  \item Auxilliary scripts:
  \begin{itemize}
    \item \ttt{make\_fit.py}: new script, more powerful and integrated script for kinetic model fits.
  \end{itemize}
\end{itemize}





%The following features are planned for future versions:
%\begin{itemize}
%  \item QM/MM calculations.
%  \item Dynamics in the Floquet picture.
%  \item Description of ionization processes by more advanced methods than Dyson norms.
%\end{itemize}

\section{References}

The following references should be cited when using the \sharc\ suite:
{
\newcommand{\enquote}[1]{``#1''}
\begin{shaded}
  \begin{itemize}
    \item \cite{Richter2011JCTC} \bibentry{Richter2011JCTC}.
%     \item \cite{Mai2015IJQC} \bibentry{Mai2015IJQC}.
    \item \cite{Mai2018WCMS} \bibentry{Mai2018WCMS}.
    \item \cite{Mai2018SHARC} \bibentry{Mai2018SHARC}.
  \end{itemize}
\end{shaded}
}

Details can be found in the following references:

The theoretical background of \sharc\ is described in Refs.~\cite{Richter2011JCTC, Richter2012JCTC_erratum, Bajo2012JPCA, Marquetand2011FD, Mai2015IJQC, Mai2018WCMS, Mai2018}.

Applications of the \sharc\ code can be found in Refs.~\cite{Richter2012JPCL, Mai2013C, Mai2014TCC, Mai2014JCP_SO2, Gonzalez2014, Richter2014PCCP,Martinez-Fernandez2014JCTC, Corrales2014PCCP, Crespo-Hernandez2015JACS, Marazzi2016JPCL, Richter2016JCTC, Cao2016CP, Banerjee2016PCCP, Mai2016JPCL, Mai2016NC, Peccati2016PTRSA, Murillo-Sanchez2017CPL, Borin2017PCCP, Mai2017CP, Siouri2017JPCA, Bellshaw2017CPL, Sun2017JPCA, Rauer2016JACS, Atkins2017JPCL, Schnappinger2017PCCP, Mai2017JCP, Zobel2018CEJ, Rauer2018MC, Cao2018JMS, Squibb2018NC}.

Other features implemented in the \sharc\ suite are described in the following references:
\begin{itemize}
  \item Energy-based decoherence correction: \cite{Granucci2007JCP}.
  \item Augmented-FSSH decoherence correction: \cite{Jain2016JCTC}.
  \item Global flux SH: \cite{Wang2014JCTC}.
  \item Local diabatization and wave function overlap calculation: \cite{Granucci2001JCP, Plasser2012JCP, Plasser2016JCTC}.
  \item Sampling of initial conditions from a quantum-mechanical harmonic Wigner distribution: \cite{Dahl1988JCP, Schinke1995, Barbatti2016IJQC}.
  \item Excited state selection for initial condition generation: \cite{Barbatti2007JPPA}.
  \item Calculation of ring puckering parameters and their classification: \cite{Cremer1975JACS, Boeyens1976JCMS}.
  \item Normal mode analysis \cite{Kurtz2001JCP,Plasser2009} and essential dynamics analysis: \cite{Amadei1993PSFB,Plasser2009}.
  \item Bootstrapping for error estimation: \cite{Nangia2004JCP}.
  \item Crossing point optimization: \cite{Bearpark1994CPL, Levine2008JPCB}
  \item Computation of ionization spectra: \cite{Ruckenbauer2016SR,Ruckenbauer2016JCP}.
  \item Wave function comparison with overlaps: \cite{Plasser2016JCP}.
\end{itemize}

The quantum chemistry programs to which interfaces with \sharc\ exist are described in the following sources:
\begin{itemize}
  \item \textsc{Molpro}: \cite{Werner2012WCMS, Werner2012},
  \item \textsc{Molcas}: \cite{Karlstrom2003CMS, Aquilante2010JCC, Aquilante2015JCC},
  \item \textsc{Columbus}: \cite{Lischka2011WCMS, Lischka2012, Yabushita1999JPCA, Mai2014JCP_reindex},
  \item \textsc{ADF}: \cite{ADF2017},
  \item \textsc{Turbomole}: \cite{TURBOMOLE70},
  \item \textsc{Gaussian}: \cite{Gaussian09, Gaussian16}.
  \item \textsc{Orca}: \cite{Neese2017WCMS}.
\end{itemize}

Others:
\begin{itemize}
  \item \textsc{TheoDORE}: \cite{Plasser2014JCP1, Plasser2014JCP2, Plasser2017TheoDORE}
  \item \textsc{WFoverlap}: \cite{Plasser2016JCTC, Plasser2016JCP}
\end{itemize}


\section{Authors}

The current version of the \sharc\ suite has been programmed by Sebastian Mai, Martin Richter, Moritz Heindl, Maximilian F. S. J. Menger, Andrew Atkins, Felix Plasser, and Philipp Marquetand of the AG Gonz\'alez of the Institute of Theoretical Chemistry of the University of Vienna with contributions from Jes\'us Gonz\'alez-V\'azquez, Matthias Ruckenbauer, Markus Oppel, Patrick J.\ Zobel, and Leticia Gonz\'alez.

\section{Suggestions and Bug Reports}

\begin{shaded}
Bug reports and suggestions for possible features can be submitted to \link{mailto:sharc@univie.ac.at}{sharc@univie.ac.at}.
\end{shaded}

\section{Notation in this Manual}

\paragraph{Names of programs}

The \sharc\ suite consists of Fortran90 programs as well as Python and Shell scripts. The executable Fortran90 programs are denoted by the extension \ttt{.x}, the Python scripts have the extension \ttt{.py} and the Shell scripts \ttt{.sh}. Within this manual, all program names are given in \ttt{bold monospaced font}.

\tthdump{
\paragraph{Shaded Sections}

Important sections are given in blue boxes like the following one:
\begin{shaded}
Important sections are given in blue boxes like this one.
\end{shaded}

On the other hand, examples of input files and command lines are marked like this:
\begin{example}
  \texttt{ user@host$>$ example example.dat}
\end{example}
}



% ========================================================================================================= %
% ========================================================================================================= %
% ========================================================================================================= %

\chapter{Installation}

\section{How To Obtain}

\sharc\ can be obtained from the \sharc\ homepage \link{http://sharc-md.org}{www.sharc-md.org}. In the Download section, register with your e-mail adress and affiliation. You will receive a download link to the stated e-mail adress. Clicking on the link in the email will download the archive file containing the \sharc\ package. Note that the link is active only for 24 h and the number of downloads is limited.
% \todo{From the homepage, also the wfoverlap code is available, required for many functionalities of \sharc.}

Note that you must accept the Terms of Use given in the following section in order to download \sharc.

\section{Terms of Use}

\newcounter{licenseparacount}
\newcommand{\licensepara}[1]{\vspace{\bigskipamount}\addtocounter{licenseparacount}{1}\textbf{\thelicenseparacount.\quad#1}\newline}
{
\definecolor{shadecolor}{HTML}{BBDDFF}
\begin{shaded}

\sharc\ Program Suite

Copyright \textcopyright{}2018, University of Vienna

SHARC is free software: you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.

SHARC is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.

A copy of the GNU General Public License is given below.
It is also available at \link{http://www.gnu.org/licenses/}{www.gnu.org/licenses/}.



% \documentclass[11pt]{article}
% 
% \title{GNU GENERAL PUBLIC LICENSE}
% \date{Version 3, 29 June 2007}
% 
% \begin{document}
% \maketitle

% \begin{center}
% {\parindent 0in
% 
% Copyright \copyright\  2007 Free Software Foundation, Inc. \texttt{https://fsf.org/}
% 
% \bigskip
% Everyone is permitted to copy and distribute verbatim copies of this
% 
% license document, but changing it is not allowed.}
% 
% \end{center}

\begin{center}
  {\bfseries\large GNU General Public License}
\end{center}
\vspace{-3mm}

\licensepara{Preamble}

% \renewcommand{\abstractname}{Preamble}
% \begin{abstract}
The GNU General Public License is a free, copyleft license for
software and other kinds of works.

The licenses for most software and other practical works are designed
to take away your freedom to share and change the works.  By contrast,
the GNU General Public License is intended to guarantee your freedom to
share and change all versions of a program--to make sure it remains free
software for all its users.  We, the Free Software Foundation, use the
GNU General Public License for most of our software; it applies also to
any other work released this way by its authors.  You can apply it to
your programs, too.

When we speak of free software, we are referring to freedom, not
price.  Our General Public Licenses are designed to make sure that you
have the freedom to distribute copies of free software (and charge for
them if you wish), that you receive source code or can get it if you
want it, that you can change the software or use pieces of it in new
free programs, and that you know you can do these things.

To protect your rights, we need to prevent others from denying you
these rights or asking you to surrender the rights.  Therefore, you have
certain responsibilities if you distribute copies of the software, or if
you modify it: responsibilities to respect the freedom of others.

For example, if you distribute copies of such a program, whether
gratis or for a fee, you must pass on to the recipients the same
freedoms that you received.  You must make sure that they, too, receive
or can get the source code.  And you must show them these terms so they
know their rights.

Developers that use the GNU GPL protect your rights with two steps:
(1) assert copyright on the software, and (2) offer you this License
giving you legal permission to copy, distribute and/or modify it.

For the developers' and authors' protection, the GPL clearly explains
that there is no warranty for this free software.  For both users' and
authors' sake, the GPL requires that modified versions be marked as
changed, so that their problems will not be attributed erroneously to
authors of previous versions.

Some devices are designed to deny users access to install or run
modified versions of the software inside them, although the manufacturer
can do so.  This is fundamentally incompatible with the aim of
protecting users' freedom to change the software.  The systematic
pattern of such abuse occurs in the area of products for individuals to
use, which is precisely where it is most unacceptable.  Therefore, we
have designed this version of the GPL to prohibit the practice for those
products.  If such problems arise substantially in other domains, we
stand ready to extend this provision to those domains in future versions
of the GPL, as needed to protect the freedom of users.

Finally, every program is threatened constantly by software patents.
States should not allow patents to restrict development and use of
software on general-purpose computers, but in those that do, we wish to
avoid the special danger that patents applied to a free program could
make it effectively proprietary.  To prevent this, the GPL assures that
patents cannot be used to render the program non-free.

The precise terms and conditions for copying, distribution and
modification follow.
% \end{abstract}

% \begin{center}
% {\Large \sc Terms and Conditions}
% \end{center}

\licensepara{Terms and Conditions}

\begin{enumerate}

\addtocounter{enumi}{-1}

\item Definitions.

``This License'' refers to version 3 of the GNU General Public License.

``Copyright'' also means copyright-like laws that apply to other kinds of
works, such as semiconductor masks.

``The Program'' refers to any copyrightable work licensed under this
License.  Each licensee is addressed as ``you''.  ``Licensees'' and
``recipients'' may be individuals or organizations.

To ``modify'' a work means to copy from or adapt all or part of the work
in a fashion requiring copyright permission, other than the making of an
exact copy.  The resulting work is called a ``modified version'' of the
earlier work or a work ``based on'' the earlier work.

A ``covered work'' means either the unmodified Program or a work based
on the Program.

To ``propagate'' a work means to do anything with it that, without
permission, would make you directly or secondarily liable for
infringement under applicable copyright law, except executing it on a
computer or modifying a private copy.  Propagation includes copying,
distribution (with or without modification), making available to the
public, and in some countries other activities as well.

To ``convey'' a work means any kind of propagation that enables other
parties to make or receive copies.  Mere interaction with a user through
a computer network, with no transfer of a copy, is not conveying.

An interactive user interface displays ``Appropriate Legal Notices''
to the extent that it includes a convenient and prominently visible
feature that (1) displays an appropriate copyright notice, and (2)
tells the user that there is no warranty for the work (except to the
extent that warranties are provided), that licensees may convey the
work under this License, and how to view a copy of this License.  If
the interface presents a list of user commands or options, such as a
menu, a prominent item in the list meets this criterion.

\item Source Code.

The ``source code'' for a work means the preferred form of the work
for making modifications to it.  ``Object code'' means any non-source
form of a work.

A ``Standard Interface'' means an interface that either is an official
standard defined by a recognized standards body, or, in the case of
interfaces specified for a particular programming language, one that
is widely used among developers working in that language.

The ``System Libraries'' of an executable work include anything, other
than the work as a whole, that (a) is included in the normal form of
packaging a Major Component, but which is not part of that Major
Component, and (b) serves only to enable use of the work with that
Major Component, or to implement a Standard Interface for which an
implementation is available to the public in source code form.  A
``Major Component'', in this context, means a major essential component
(kernel, window system, and so on) of the specific operating system
(if any) on which the executable work runs, or a compiler used to
produce the work, or an object code interpreter used to run it.

The ``Corresponding Source'' for a work in object code form means all
the source code needed to generate, install, and (for an executable
work) run the object code and to modify the work, including scripts to
control those activities.  However, it does not include the work's
System Libraries, or general-purpose tools or generally available free
programs which are used unmodified in performing those activities but
which are not part of the work.  For example, Corresponding Source
includes interface definition files associated with source files for
the work, and the source code for shared libraries and dynamically
linked subprograms that the work is specifically designed to require,
such as by intimate data communication or control flow between those
subprograms and other parts of the work.

The Corresponding Source need not include anything that users
can regenerate automatically from other parts of the Corresponding
Source.

The Corresponding Source for a work in source code form is that
same work.

\item Basic Permissions.

All rights granted under this License are granted for the term of
copyright on the Program, and are irrevocable provided the stated
conditions are met.  This License explicitly affirms your unlimited
permission to run the unmodified Program.  The output from running a
covered work is covered by this License only if the output, given its
content, constitutes a covered work.  This License acknowledges your
rights of fair use or other equivalent, as provided by copyright law.

You may make, run and propagate covered works that you do not
convey, without conditions so long as your license otherwise remains
in force.  You may convey covered works to others for the sole purpose
of having them make modifications exclusively for you, or provide you
with facilities for running those works, provided that you comply with
the terms of this License in conveying all material for which you do
not control copyright.  Those thus making or running the covered works
for you must do so exclusively on your behalf, under your direction
and control, on terms that prohibit them from making any copies of
your copyrighted material outside their relationship with you.

Conveying under any other circumstances is permitted solely under
the conditions stated below.  Sublicensing is not allowed; section 10
makes it unnecessary.

\item Protecting Users' Legal Rights From Anti-Circumvention Law.

No covered work shall be deemed part of an effective technological
measure under any applicable law fulfilling obligations under article
11 of the WIPO copyright treaty adopted on 20 December 1996, or
similar laws prohibiting or restricting circumvention of such
measures.

When you convey a covered work, you waive any legal power to forbid
circumvention of technological measures to the extent such circumvention
is effected by exercising rights under this License with respect to
the covered work, and you disclaim any intention to limit operation or
modification of the work as a means of enforcing, against the work's
users, your or third parties' legal rights to forbid circumvention of
technological measures.

\item Conveying Verbatim Copies.

You may convey verbatim copies of the Program's source code as you
receive it, in any medium, provided that you conspicuously and
appropriately publish on each copy an appropriate copyright notice;
keep intact all notices stating that this License and any
non-permissive terms added in accord with section 7 apply to the code;
keep intact all notices of the absence of any warranty; and give all
recipients a copy of this License along with the Program.

You may charge any price or no price for each copy that you convey,
and you may offer support or warranty protection for a fee.

\item Conveying Modified Source Versions.

You may convey a work based on the Program, or the modifications to
produce it from the Program, in the form of source code under the
terms of section 4, provided that you also meet all of these conditions:
  \begin{enumerate}
  \item The work must carry prominent notices stating that you modified
  it, and giving a relevant date.

  \item The work must carry prominent notices stating that it is
  released under this License and any conditions added under section
  7.  This requirement modifies the requirement in section 4 to
  ``keep intact all notices''.

  \item You must license the entire work, as a whole, under this
  License to anyone who comes into possession of a copy.  This
  License will therefore apply, along with any applicable section 7
  additional terms, to the whole of the work, and all its parts,
  regardless of how they are packaged.  This License gives no
  permission to license the work in any other way, but it does not
  invalidate such permission if you have separately received it.

  \item If the work has interactive user interfaces, each must display
  Appropriate Legal Notices; however, if the Program has interactive
  interfaces that do not display Appropriate Legal Notices, your
  work need not make them do so.
\end{enumerate}
A compilation of a covered work with other separate and independent
works, which are not by their nature extensions of the covered work,
and which are not combined with it such as to form a larger program,
in or on a volume of a storage or distribution medium, is called an
``aggregate'' if the compilation and its resulting copyright are not
used to limit the access or legal rights of the compilation's users
beyond what the individual works permit.  Inclusion of a covered work
in an aggregate does not cause this License to apply to the other
parts of the aggregate.

\item Conveying Non-Source Forms.

You may convey a covered work in object code form under the terms
of sections 4 and 5, provided that you also convey the
machine-readable Corresponding Source under the terms of this License,
in one of these ways:
  \begin{enumerate}
  \item Convey the object code in, or embodied in, a physical product
  (including a physical distribution medium), accompanied by the
  Corresponding Source fixed on a durable physical medium
  customarily used for software interchange.

  \item Convey the object code in, or embodied in, a physical product
  (including a physical distribution medium), accompanied by a
  written offer, valid for at least three years and valid for as
  long as you offer spare parts or customer support for that product
  model, to give anyone who possesses the object code either (1) a
  copy of the Corresponding Source for all the software in the
  product that is covered by this License, on a durable physical
  medium customarily used for software interchange, for a price no
  more than your reasonable cost of physically performing this
  conveying of source, or (2) access to copy the
  Corresponding Source from a network server at no charge.

  \item Convey individual copies of the object code with a copy of the
  written offer to provide the Corresponding Source.  This
  alternative is allowed only occasionally and noncommercially, and
  only if you received the object code with such an offer, in accord
  with subsection 6b.

  \item Convey the object code by offering access from a designated
  place (gratis or for a charge), and offer equivalent access to the
  Corresponding Source in the same way through the same place at no
  further charge.  You need not require recipients to copy the
  Corresponding Source along with the object code.  If the place to
  copy the object code is a network server, the Corresponding Source
  may be on a different server (operated by you or a third party)
  that supports equivalent copying facilities, provided you maintain
  clear directions next to the object code saying where to find the
  Corresponding Source.  Regardless of what server hosts the
  Corresponding Source, you remain obligated to ensure that it is
  available for as long as needed to satisfy these requirements.

  \item Convey the object code using peer-to-peer transmission, provided
  you inform other peers where the object code and Corresponding
  Source of the work are being offered to the general public at no
  charge under subsection 6d.
  \end{enumerate}

A separable portion of the object code, whose source code is excluded
from the Corresponding Source as a System Library, need not be
included in conveying the object code work.

A ``User Product'' is either (1) a ``consumer product'', which means any
tangible personal property which is normally used for personal, family,
or household purposes, or (2) anything designed or sold for incorporation
into a dwelling.  In determining whether a product is a consumer product,
doubtful cases shall be resolved in favor of coverage.  For a particular
product received by a particular user, ``normally used'' refers to a
typical or common use of that class of product, regardless of the status
of the particular user or of the way in which the particular user
actually uses, or expects or is expected to use, the product.  A product
is a consumer product regardless of whether the product has substantial
commercial, industrial or non-consumer uses, unless such uses represent
the only significant mode of use of the product.

``Installation Information'' for a User Product means any methods,
procedures, authorization keys, or other information required to install
and execute modified versions of a covered work in that User Product from
a modified version of its Corresponding Source.  The information must
suffice to ensure that the continued functioning of the modified object
code is in no case prevented or interfered with solely because
modification has been made.

If you convey an object code work under this section in, or with, or
specifically for use in, a User Product, and the conveying occurs as
part of a transaction in which the right of possession and use of the
User Product is transferred to the recipient in perpetuity or for a
fixed term (regardless of how the transaction is characterized), the
Corresponding Source conveyed under this section must be accompanied
by the Installation Information.  But this requirement does not apply
if neither you nor any third party retains the ability to install
modified object code on the User Product (for example, the work has
been installed in ROM).

The requirement to provide Installation Information does not include a
requirement to continue to provide support service, warranty, or updates
for a work that has been modified or installed by the recipient, or for
the User Product in which it has been modified or installed.  Access to a
network may be denied when the modification itself materially and
adversely affects the operation of the network or violates the rules and
protocols for communication across the network.

Corresponding Source conveyed, and Installation Information provided,
in accord with this section must be in a format that is publicly
documented (and with an implementation available to the public in
source code form), and must require no special password or key for
unpacking, reading or copying.

\item Additional Terms.

``Additional permissions'' are terms that supplement the terms of this
License by making exceptions from one or more of its conditions.
Additional permissions that are applicable to the entire Program shall
be treated as though they were included in this License, to the extent
that they are valid under applicable law.  If additional permissions
apply only to part of the Program, that part may be used separately
under those permissions, but the entire Program remains governed by
this License without regard to the additional permissions.

When you convey a copy of a covered work, you may at your option
remove any additional permissions from that copy, or from any part of
it.  (Additional permissions may be written to require their own
removal in certain cases when you modify the work.)  You may place
additional permissions on material, added by you to a covered work,
for which you have or can give appropriate copyright permission.

Notwithstanding any other provision of this License, for material you
add to a covered work, you may (if authorized by the copyright holders of
that material) supplement the terms of this License with terms:
  \begin{enumerate}
  \item Disclaiming warranty or limiting liability differently from the
  terms of sections 15 and 16 of this License; or

  \item Requiring preservation of specified reasonable legal notices or
  author attributions in that material or in the Appropriate Legal
  Notices displayed by works containing it; or

  \item Prohibiting misrepresentation of the origin of that material, or
  requiring that modified versions of such material be marked in
  reasonable ways as different from the original version; or

  \item Limiting the use for publicity purposes of names of licensors or
  authors of the material; or

  \item Declining to grant rights under trademark law for use of some
  trade names, trademarks, or service marks; or

  \item Requiring indemnification of licensors and authors of that
  material by anyone who conveys the material (or modified versions of
  it) with contractual assumptions of liability to the recipient, for
  any liability that these contractual assumptions directly impose on
  those licensors and authors.
  \end{enumerate}

All other non-permissive additional terms are considered ``further
restrictions'' within the meaning of section 10.  If the Program as you
received it, or any part of it, contains a notice stating that it is
governed by this License along with a term that is a further
restriction, you may remove that term.  If a license document contains
a further restriction but permits relicensing or conveying under this
License, you may add to a covered work material governed by the terms
of that license document, provided that the further restriction does
not survive such relicensing or conveying.

If you add terms to a covered work in accord with this section, you
must place, in the relevant source files, a statement of the
additional terms that apply to those files, or a notice indicating
where to find the applicable terms.

Additional terms, permissive or non-permissive, may be stated in the
form of a separately written license, or stated as exceptions;
the above requirements apply either way.

\item Termination.

You may not propagate or modify a covered work except as expressly
provided under this License.  Any attempt otherwise to propagate or
modify it is void, and will automatically terminate your rights under
this License (including any patent licenses granted under the third
paragraph of section 11).

However, if you cease all violation of this License, then your
license from a particular copyright holder is reinstated (a)
provisionally, unless and until the copyright holder explicitly and
finally terminates your license, and (b) permanently, if the copyright
holder fails to notify you of the violation by some reasonable means
prior to 60 days after the cessation.

Moreover, your license from a particular copyright holder is
reinstated permanently if the copyright holder notifies you of the
violation by some reasonable means, this is the first time you have
received notice of violation of this License (for any work) from that
copyright holder, and you cure the violation prior to 30 days after
your receipt of the notice.

Termination of your rights under this section does not terminate the
licenses of parties who have received copies or rights from you under
this License.  If your rights have been terminated and not permanently
reinstated, you do not qualify to receive new licenses for the same
material under section 10.

\item Acceptance Not Required for Having Copies.

You are not required to accept this License in order to receive or
run a copy of the Program.  Ancillary propagation of a covered work
occurring solely as a consequence of using peer-to-peer transmission
to receive a copy likewise does not require acceptance.  However,
nothing other than this License grants you permission to propagate or
modify any covered work.  These actions infringe copyright if you do
not accept this License.  Therefore, by modifying or propagating a
covered work, you indicate your acceptance of this License to do so.

\item Automatic Licensing of Downstream Recipients.

Each time you convey a covered work, the recipient automatically
receives a license from the original licensors, to run, modify and
propagate that work, subject to this License.  You are not responsible
for enforcing compliance by third parties with this License.

An ``entity transaction'' is a transaction transferring control of an
organization, or substantially all assets of one, or subdividing an
organization, or merging organizations.  If propagation of a covered
work results from an entity transaction, each party to that
transaction who receives a copy of the work also receives whatever
licenses to the work the party's predecessor in interest had or could
give under the previous paragraph, plus a right to possession of the
Corresponding Source of the work from the predecessor in interest, if
the predecessor has it or can get it with reasonable efforts.

You may not impose any further restrictions on the exercise of the
rights granted or affirmed under this License.  For example, you may
not impose a license fee, royalty, or other charge for exercise of
rights granted under this License, and you may not initiate litigation
(including a cross-claim or counterclaim in a lawsuit) alleging that
any patent claim is infringed by making, using, selling, offering for
sale, or importing the Program or any portion of it.

\item Patents.

A ``contributor'' is a copyright holder who authorizes use under this
License of the Program or a work on which the Program is based.  The
work thus licensed is called the contributor's ``contributor version''.

A contributor's ``essential patent claims'' are all patent claims
owned or controlled by the contributor, whether already acquired or
hereafter acquired, that would be infringed by some manner, permitted
by this License, of making, using, or selling its contributor version,
but do not include claims that would be infringed only as a
consequence of further modification of the contributor version.  For
purposes of this definition, ``control'' includes the right to grant
patent sublicenses in a manner consistent with the requirements of
this License.

Each contributor grants you a non-exclusive, worldwide, royalty-free
patent license under the contributor's essential patent claims, to
make, use, sell, offer for sale, import and otherwise run, modify and
propagate the contents of its contributor version.

In the following three paragraphs, a ``patent license'' is any express
agreement or commitment, however denominated, not to enforce a patent
(such as an express permission to practice a patent or covenant not to
sue for patent infringement).  To ``grant'' such a patent license to a
party means to make such an agreement or commitment not to enforce a
patent against the party.

If you convey a covered work, knowingly relying on a patent license,
and the Corresponding Source of the work is not available for anyone
to copy, free of charge and under the terms of this License, through a
publicly available network server or other readily accessible means,
then you must either (1) cause the Corresponding Source to be so
available, or (2) arrange to deprive yourself of the benefit of the
patent license for this particular work, or (3) arrange, in a manner
consistent with the requirements of this License, to extend the patent
license to downstream recipients.  ``Knowingly relying'' means you have
actual knowledge that, but for the patent license, your conveying the
covered work in a country, or your recipient's use of the covered work
in a country, would infringe one or more identifiable patents in that
country that you have reason to believe are valid.

If, pursuant to or in connection with a single transaction or
arrangement, you convey, or propagate by procuring conveyance of, a
covered work, and grant a patent license to some of the parties
receiving the covered work authorizing them to use, propagate, modify
or convey a specific copy of the covered work, then the patent license
you grant is automatically extended to all recipients of the covered
work and works based on it.

A patent license is ``discriminatory'' if it does not include within
the scope of its coverage, prohibits the exercise of, or is
conditioned on the non-exercise of one or more of the rights that are
specifically granted under this License.  You may not convey a covered
work if you are a party to an arrangement with a third party that is
in the business of distributing software, under which you make payment
to the third party based on the extent of your activity of conveying
the work, and under which the third party grants, to any of the
parties who would receive the covered work from you, a discriminatory
patent license (a) in connection with copies of the covered work
conveyed by you (or copies made from those copies), or (b) primarily
for and in connection with specific products or compilations that
contain the covered work, unless you entered into that arrangement,
or that patent license was granted, prior to 28 March 2007.

Nothing in this License shall be construed as excluding or limiting
any implied license or other defenses to infringement that may
otherwise be available to you under applicable patent law.

\item No Surrender of Others' Freedom.

If conditions are imposed on you (whether by court order, agreement or
otherwise) that contradict the conditions of this License, they do not
excuse you from the conditions of this License.  If you cannot convey a
covered work so as to satisfy simultaneously your obligations under this
License and any other pertinent obligations, then as a consequence you may
not convey it at all.  For example, if you agree to terms that obligate you
to collect a royalty for further conveying from those to whom you convey
the Program, the only way you could satisfy both those terms and this
License would be to refrain entirely from conveying the Program.

\item Use with the GNU Affero General Public License.

Notwithstanding any other provision of this License, you have
permission to link or combine any covered work with a work licensed
under version 3 of the GNU Affero General Public License into a single
combined work, and to convey the resulting work.  The terms of this
License will continue to apply to the part which is the covered work,
but the special requirements of the GNU Affero General Public License,
section 13, concerning interaction through a network will apply to the
combination as such.

\item Revised Versions of this License.

The Free Software Foundation may publish revised and/or new versions of
the GNU General Public License from time to time.  Such new versions will
be similar in spirit to the present version, but may differ in detail to
address new problems or concerns.

Each version is given a distinguishing version number.  If the
Program specifies that a certain numbered version of the GNU General
Public License ``or any later version'' applies to it, you have the
option of following the terms and conditions either of that numbered
version or of any later version published by the Free Software
Foundation.  If the Program does not specify a version number of the
GNU General Public License, you may choose any version ever published
by the Free Software Foundation.

If the Program specifies that a proxy can decide which future
versions of the GNU General Public License can be used, that proxy's
public statement of acceptance of a version permanently authorizes you
to choose that version for the Program.

Later license versions may give you additional or different
permissions.  However, no additional obligations are imposed on any
author or copyright holder as a result of your choosing to follow a
later version.

\item Disclaimer of Warranty.

\begin{sloppypar}
 THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY
 APPLICABLE LAW.  EXCEPT WHEN OTHERWISE STATED IN WRITING THE
 COPYRIGHT HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM ``AS IS''
 WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED,
 INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF
 MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE.  THE ENTIRE
 RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM IS WITH YOU.
 SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL
 NECESSARY SERVICING, REPAIR OR CORRECTION.
\end{sloppypar}

\item Limitation of Liability.

 IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN
 WRITING WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES
 AND/OR CONVEYS THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR
 DAMAGES, INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL
 DAMAGES ARISING OUT OF THE USE OR INABILITY TO USE THE PROGRAM
 (INCLUDING BUT NOT LIMITED TO LOSS OF DATA OR DATA BEING RENDERED
 INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A FAILURE
 OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS), EVEN IF SUCH
 HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH
 DAMAGES.

\item Interpretation of Sections 15 and 16.

If the disclaimer of warranty and limitation of liability provided
above cannot be given local legal effect according to their terms,
reviewing courts shall apply local law that most closely approximates
an absolute waiver of all civil liability in connection with the
Program, unless a warranty or assumption of liability accompanies a
copy of the Program in return for a fee.

% \begin{center}
% {\Large\sc End of Terms and Conditions}
% 
% \bigskip
% How to Apply These Terms to Your New Programs
% \end{center}
% 
% If you develop a new program, and you want it to be of the greatest
% possible use to the public, the best way to achieve this is to make it
% free software which everyone can redistribute and change under these terms.
% 
% To do so, attach the following notices to the program.  It is safest
% to attach them to the start of each source file to most effectively
% state the exclusion of warranty; and each file should have at least
% the ``copyright'' line and a pointer to where the full notice is found.
% 
% {\footnotesize
% \begin{verbatim}
% <one line to give the program's name and a brief idea of what it does.>
% 
% Copyright (C) <textyear>  <name of author>
% 
% This program is free software: you can redistribute it and/or modify
% it under the terms of the GNU General Public License as published by
% the Free Software Foundation, either version 3 of the License, or
% (at your option) any later version.
% 
% This program is distributed in the hope that it will be useful,
% but WITHOUT ANY WARRANTY; without even the implied warranty of
% MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
% GNU General Public License for more details.
% 
% You should have received a copy of the GNU General Public License
% along with this program.  If not, see <https://www.gnu.org/licenses/>.
% \end{verbatim}
% }
% 
% Also add information on how to contact you by electronic and paper mail.
% 
% If the program does terminal interaction, make it output a short
% notice like this when it starts in an interactive mode:
% 
% {\footnotesize
% \begin{verbatim}
% <program>  Copyright (C) <year>  <name of author>
% 
% This program comes with ABSOLUTELY NO WARRANTY; for details type `show w'.
% This is free software, and you are welcome to redistribute it
% under certain conditions; type `show c' for details.
% \end{verbatim}
% }
% 
% The hypothetical commands {\tt show w} and {\tt show c} should show
% the appropriate
% parts of the General Public License.  Of course, your program's commands
% might be different; for a GUI interface, you would use an ``about box''.
% 
% You should also get your employer (if you work as a programmer) or
% school, if any, to sign a ``copyright disclaimer'' for the program, if
% necessary.  For more information on this, and how to apply and follow
% the GNU GPL, see \texttt{https://www.gnu.org/licenses/}.
% 
% The GNU General Public License does not permit incorporating your
% program into proprietary programs.  If your program is a subroutine
% library, you may consider it more useful to permit linking proprietary
% applications with the library.  If this is what you want to do, use
% the GNU Lesser General Public License instead of this License.  But
% first, please read \texttt{https://www.gnu.org/licenses/why-not-lgpl.html}.

\end{enumerate}

\end{shaded}
}


\section{Installation}

In order to install and run \sharc\ under Linux (Windows and OS X are currently not supported), you need the following:
\begin{itemize}
  \item A Fortran90 compiler (This release is tested against \link{https://gcc.gnu.org/fortran/}{GNU Fortran} 4.4.7 and \link{https://software.intel.com/en-us/fortran-compilers}{Intel Fortran} 15.0).
  \item The \link{http://www.netlib.org/blas/}{BLAS}, \link{http://www.netlib.org/lapack/}{LAPACK} and \link{http://http://www.fftw.org/}{FFTW3} libraries.
  \item \link{https://www.python.org/downloads/release/python-278/}{Python 2} (This release is tested against Python 2.6 and 2.7).
  \item \ttt{make}.
\end{itemize}

The source code of the  \sharc\ suite is distributed as a tar archive file. In order to install it, first extract the content of the archive to a suitable directory:
\begin{example}
\verb|tar -xzvf sharc.tgz|
\end{example}
This should create a new directory called \ttt{sharc/} which contains all the necessary subdirectories and files. 
In figure~\ref{fig:installation} the directory structure of the complete \sharc\ directory is shown.

\begin{figure}[htb]
  \centering
  \includegraphics[scale=1]{img/dirs_SHARC/dirs_SHARC.pdf}
  \caption{Directory tree containing a complete \sharc\ installation.}
  \label{fig:installation}
\end{figure}

To compile the Fortran90 programs of the \sharc\ suite, go to the \ttt{source/} directory.
\begin{example}
\verb|cd source/|
\end{example}
and edit the \ttt{Makefile} by adjusting the \ttt{F90} variable to point to the Fortran compiler of your choice. 
Issuing the command:
\begin{example}
\verb|make|
\end{example}
will compile the source and create all the binaries.
\begin{example}
\verb|make install|
\end{example}
will copy the binary files into the \ttt{sharc/bin/} directory of the 
\sharc\ distribution, which already contains all the python scripts which
come with \sharc.

In order to use the \sharc\ suite, set the environment variable \ttt{\$SHARC} to the \ttt{bin/} directory of the \sharc\ installation. This ensures that all programs of the \sharc\ suite find the other executables and all calls are successful. For example, if you have unpacked \sharc\ into your home directory, 
just set: 
\begin{example}
\verb|export SHARC=~/sharc/bin|\quad (for bourne shell users)
\end{example}
or
\begin{example}
\verb|setenv SHARC $HOME/sharc/bin|\quad (for c-shell type users)
\end{example}
Note that it is advisable to put this line into your shell's login
scripts.

\subsection{\textsc{WFoverlap} Program}

The \sharc\ package contains as a submodule the program \textsc{WFoverlap}, which is necessary for many functionalities of \sharc.
In order to install and test this program, see section~\ref{sec:int:wfoverlap}.

\subsection{Libraries}

\sharc\ requires the BLAS, LAPACK and FFTW3 libraries. During the installation, it might be necessary to alter the \ttt{LDFLAGS} string in the \ttt{Makefile}, depending on where the relevant libraries are located on your system. In this way, it is for example possible to use vendor-provided libraries like the \link{https://software.intel.com/en-us/intel-mkl}{Intel MKL}. For more details see the \ttt{INSTALL} file which is included in the \sharc\ distribution.


\subsection{Test Suite}\label{sec:tests.py}

After the installation, it is advisable to first execute the test suite of \sharc, which will test the fundamental functionality of SHARC.
Change to an empty directory and execute
\begin{example}
\verb|$SHARC/tests.py|
\end{example}
The interactive script will first verify the Python installation (no message will appear if the Python installation is fine). 
Subsequently, the script prompts the user to enter which tests should be executed. 
The script will also ask for a number of environment variables, which are listed in Table~\ref{tab:test_vars}.

Their is at least one test for each of the auxiliary scripts and interfaces. 
Tests whose names start with \ttt{scripts\_} test the functionality of the auxiliary programs in the \sharc\ suite.
Tests whose names start with \ttt{ADF\_}, \ttt{Analytical\_}, \ttt{COLUMBUS\_}, \ttt{GAUSSIAN\_}, \ttt{LVC\_}, \ttt{MOLCAS\_} \ttt{MOLPRO\_}, or \ttt{TURBOMOLE\_} run short trajectories, testing whether the main dynamics code, the interfaces, the quantum chemistry programs, and auxiliary programs (\textsc{TheoDORE}, \textsc{WFoverlap}, \textsc{Orca}, \textsc{Tinker}) work together correctly.


If the installation was successful and Python is installed correctly, \ttt{Analytical\_overlap}, \ttt{LVC\_overlap}, and most tests named \ttt{scripts\_<NAME>} should execute without error. 

The test calculations involving the quantum chemistry programs can be used to check that \sharc\ can correctly call these programs and that they are installed correctly.

If any of the tests show differences between output and reference output, it is advisable to check the respective files (i.e., compare \ttt{\$SHARC/../tests/RESULTS/<job>/} to \ttt{./RUNNING\_TESTS/<job>/}). Note that small differences (different sign of values or small numerical deviations) in the output can already occur when using a different version of the quantum chemistry programs, different compilers, different libraries, or different parallization schemes.
It should be noted that along trajectories, these small changes can add up to notably influence the trajectories, but across the ensemble these small changes will likely cancel out.


\begin{table}
  \centering
  \caption[Environment variables for \sharc\ test jobs.]{Environment variables for \sharc\ test jobs. These variables need to be set before the test job execution.}
  \label{tab:test_vars}
  \begin{tabular}{>{\ttfamily}lp{13cm}}
  \hline
  Keyword       &Description\\
  \hline
  \$ADF         &Points to the main directory of the ADF installation, which contains the file \ttt{adfrc.sh}.\\
  \$COLUMBUS    &Points to the directory containing the \textsc{Columbus} executables, e.g., \ttt{runls}.\\
  \$GAUSSIAN    &Points to the main directory of the \textsc{Gaussian} installation, which contains the \textsc{Gaussian} executables (e.g., \ttt{g09}/\ttt{g16} or \ttt{l9999.exe}).\\
  \$MOLCAS      &Points to the main directory of the \textsc{Openmolcas} installation, containing \ttt{molcas.rte} and directories \ttt{basis\_library/} and \ttt{bin/}.\\
  \$MOLPRO      &Points to the \ttt{bin/} directory of the \textsc{Molpro} installation, which contains the \ttt{molpro.exe} file.\\
  \$TURBOMOLE   &Points to the main directory of the \textsc{Turbomole} installation, which contains subdirectories like \ttt{basen/}, \ttt{bin/}, or \ttt{scripts/}.\\
  \$ORCA        &Points to the directory containing the \textsc{Orca} executables, e.g., \ttt{orca}, \ttt{orca\_gtoint}, or \ttt{orca\_scf}.\\
  \$THEODORE    &Points to the main directory of the \textsc{TheoDORE} installation. \ttt{\$THEODORE/bin/} should contain \ttt{analyze\_tden.py}.\\
  \$TINKER      &Points to the \ttt{bin/} directory of a \textsc{Molcas}-modified \textsc{Tinker} installation. It contains files like \ttt{tkr2qm\_s}.\\
  \$molcas      &Should point to the same location as \ttt{\$MOLCAS}, or another \textsc{Molcas} installation. Note that \ttt{\$molcas} is only used by some \textsc{Columbus} test jobs. Also note that \ttt{\$molcas} does not need to point to the \textsc{Molcas} installation interfaced to \textsc{Columbus}.\\
  \hline
  \end{tabular}
\end{table}


\subsection{Additional Programs}

For full functionality of the \sharc\ suite, several additional programs are recommended (all of these programs are currently freely available, except for \textsc{Amber}):

\begin{itemize}
  \item The Python package \link{http://www.numpy.org/}{\textsc{NumPy}}.
  \\
  Optimally, within your Python installation the \textsc{NumPy} package (which provides many numerical methods, e.g., matrix diagonalization) should be available. If \textsc{NumPy} is not available, the \sharc\ suite is still functional, and the affected scripts will fall back to use a small Fortran code (front-end for LAPACK) within the \sharc\ package. Since in the Python scripts no large-scale matrix calculations are carried out, there should be no significant performance loss if \textsc{NumPy} is not available.
  \medskip
  \item The Python package \link{https://matplotlib.org/}{\textsc{Matplotlib}}.
  \\
  If the \textsc{Matplotlib} package, some auxiliary scripts can automatically generate certain plots.
  \medskip
  \item The \link{http://www.gnuplot.info/}{\textsc{Gnuplot}} plotting software.
  \\
  \textsc{Gnuplot} is not strictly necessary, since all output files could be plotted using other plotting programs. However, a number of scripts from the \sharc\ suite automatically generate \textsc{Gnuplot} scripts after data processing, allowing to quickly plot the output files or carry out data fits.
  \medskip
  \item A molecular visualization software able to read xyz files (e.g.\ \link{http://www.cmbi.ru.nl/molden/molden.html}{\textsc{Molden}}, \link{http://gabedit.sourceforge.net/}{\textsc{Gabedit}}, \link{http://molekel.cscs.ch/wiki/pmwiki.php}{\textsc{Molekel}} or \link{http://www.ks.uiuc.edu/Research/vmd/}{VMD}).
  \\
  Molecular visualization software is needed in order to visualize molecular motion in the dynamics.
  \medskip
  \item The \link{http://maxima.sourceforge.net/}{\textsc{Maxima}} computer algebra system.
  \\
  The computer algebra system \textsc{Maxima} is necessary for the use of \ttt{make\_fitscript.py}, a program which allows to perform global fits of chemical kinetics models to populations data.
  \medskip
  \item The \link{http://theodore-qc.sourceforge.net/}{\textsc{TheoDORE}} wave function analysis suite.
  \\
  The wave function analysis package \textsc{TheoDORE} allows to compute various descriptors of electronic wave functions (supported by some interfaces), which is helpful to follow the state characters along trajectories.
  \medskip
  \item The \link{http://ambermd.org/}{\textsc{Amber}} molecular dynamics package.
  \\
  \textsc{Amber} can be used to prepare initial conditions based on ground state molecular dynamics simulations (instead of using a Wigner distribution), which is especially useful for large systems.
  \medskip
  \item  The \link{https://orcaforum.kofo.mpg.de}{\textsc{Orca}} ab initio package.
  \\
  \textsc{Orca} can be employed as external optimizer. In combination with the \sharc\ interfaces, it is possible to perform optimizations of minima, conical intersections, and crossing points for any method interfaced to \sharc.
\end{itemize}




\subsection{Quantum Chemistry Programs}

Even though \sharc\ comes with two interfaces for analytical potentials (and hence can be used without any quantum chemistry program), the main application of \sharc\ is certainly on-the-fly ab initio dynamics. Hence, one of the following interfaced quantum chemistry programs is necessary:
\begin{itemize}
  \item \link{http://www.molpro.net/}{\textsc{Molpro}} (this release was checked against \textsc{Molpro} 2010 and 2012).
  \item \link{https://gitlab.com/Molcas/OpenMolcas/}{\textsc{Openmolcas}} (this release was checked against \textsc{Openmolcas} 18).
  \begin{itemize}
    \item \link{http://dasher.wustl.edu/tinker/}{\textsc{Tinker}}, \link{http://www.molcas.org/documentation/manual/node79.html#SECTION052122010000000000000}{interfaced to \textsc{Openmolcas} 18}, for QM/MM dynamics.
  \end{itemize}
  \item \link{http://www.univie.ac.at/columbus/docs_COL70/documentation_main.html}{\textsc{Columbus} 7}
  \begin{itemize}
    \item \link{http://www.univie.ac.at/columbus/docs_COL70/columbus_molcas_link.html}{\textsc{Columbus-Molcas} interface} for spin-orbit couplings.
  \end{itemize}
  \item \link{http://www.scm.com/ADF}{\textsc{Amsterdam Density Functional}} (ADF 2017 needed for overlaps, ADF 2018 for all QM/MM functionality)
  \item \link{http://www.turbomole.com}{\textsc{Turbomole}} (this release was checked against \textsc{Turbomole} 6.6 and 7.0; version 7.1 is currently not supported).
  \begin{itemize}
    \item \link{https://orcaforum.kofo.mpg.de}{\textsc{Orca}} (version 3 or 4) for spin-orbit couplings.
  \end{itemize}
  \item \link{http://www.gaussian.com}{\textsc{Gaussian}} (this release was checked against \textsc{Gaussian} 09 and 16).
  \item \link{https://orcaforum.kofo.mpg.de}{\textsc{Orca}} (version 4.1 or higher).
\end{itemize}

See the relevant sections in chapter~\ref{chap:interfaces} for a description of the quantum chemical methods available with each of these programs.

% ========================================================================================================= %
% ========================================================================================================= %
% ========================================================================================================= %

\chapter{Execution}

The \sharc\ suite consists of the main dynamics code \ttt{sharc.x} and a number of auxiliary programs, like setup scripts and analysis tools. Additionally, the suite comes with interfaces to several quantum chemistry software: \textsc{Molpro}, \textsc{Molcas}, \textsc{Columbus}, \textsc{Turbomole}, \textsc{ADF}, \textsc{Gaussian}, and \textsc{Orca}. 

In the following, first it is explained how to run a single trajectory by setting up all necessary input for the dynamics code \ttt{sharc.x} manually. Afterwards, the usage of the auxiliary scripts is explained. 
Detailed infos on the \sharc\ input files is given in chapter~\ref{chap:input}.
Chapter~\ref{chap:output} documents the different output files \sharc\ produces.
The interfaces are described in chapter~\ref{chap:interfaces} and the auxiliary scripts in chapter~\ref{chap:aux}. 
All relevant theoretical background is given in chapter~\ref{chap:met}.

\section{Running a single trajectory}

\subsection{Input files}

\sharc\ requires a number of input files, which contain the settings for the dynamics simulation (\ttt{input}), the initial geometry (\ttt{geom}), the initial velocity (\ttt{veloc}), the initial coefficients (\ttt{coeff}) and the laser field (\ttt{laser}). Only the first two (\ttt{input}, \ttt{geom}) are mandatory, the others are optional. The necessary files are shown in figure~\ref{fig:dir_traj}. 
The content of the main input file is explained in detail in section~\ref{sec:inputfile}, the geometry file is specified in section~\ref{sec:geomfile}. The specifications of the velocity, coefficient and laser files are given in sections~\ref{sec:velocfile}, \ref{sec:coefffile} and \ref{sec:laserfile}, respectively.

\begin{figure}[htb]
  \centering
  \includegraphics[scale=1]{img/dir_traj/dir_traj.pdf}
  \caption[Input files for a \sharc\ dynamics simulation.]{Input files for a \sharc\ dynamics simulation. Directories are in blue, executable scripts in green, regular files in white and optional files in grey.}
  \label{fig:dir_traj}
\end{figure}

Additionally, the directory \ttt{QM/} and the script \ttt{QM/runQM.sh} need to be present, since the on-the-fly ab initio calculations are implemented through these files. The script \ttt{QM/runQM.sh} is called each time \sharc\ performs an on-the-fly calculation of electronic properties (usually by a quantum chemistry program). In order to do so, \sharc\ first writes the request for the calculation to \ttt{QM/QM.in}, then calls \ttt{QM/runQM.sh}, waits for the script to finish and then reads the requested quantities from \ttt{QM/QM.out}. The script \ttt{QM/runQM.sh} is fully responsible to generate the requested results from the provided input. 
In virtually all cases, this task is handled by the \sharc-quantum chemistry interfaces (see chapter~\ref{chap:interfaces}), so that the script \ttt{QM/runQM.sh} has a particularly simple form:
\begin{example}
\verb|cd QM/|

\verb|$SHARC/<INTERFACE> QM.in |
\end{example}
with the corresponding interface name given. Note that the interfaces in all cases need additional input files, which must be present in \ttt{QM/}. Those input files contain the specifications for the quantum chemistry information, e.g., basis set, active and reference space, memory settings, path to the quantum chemistry program, path to scratch directories; or for \ttt{SHARC\_Analytical.py} and \ttt{SHARC\_LVC.py} the parameters for the analytical potentials. For each interface, the input files are slightly different. See sections~\ref{sec:int:molpro}, \ref{sec:int:molcas}, \ref{sec:int:columbus}, \ref{sec:int:analytical}, \ref{sec:int:ricc2}, \ref{sec:int:adf}, \ref{sec:int:lvc}, \ref{sec:int:gaussian} or \ref{sec:int:orca} for the necessary information.

\subsection{Running the dynamics code}

Given the necessary input files, \sharc\ can be started by executing
\begin{example}
\verb|user@host> $SHARC/sharc.x input|
\end{example}
Note that besides the input file, at least the geometry file needs to be present (see chapter~\ref{chap:input} for details).

A running trajectory can be stopped after the current time step by creating an empty file \ttt{STOP}:
\begin{example}
\verb|user@host> touch STOP|
\end{example}
This is usually preferable to simply killing \sharc, because the current time step is properly finished and all files are correctly written for analysis and restart.

\subsection{Output files}

Figure~\ref{fig:dir_traj_after} shows the content of a trajectory directory after the execution of \sharc. There will be six new files. These files are \ttt{output.log}, \ttt{output.lis}, \ttt{output.dat} and \ttt{output.xyz}, as well as \ttt{restart.ctrl} and \ttt{restart.traj}.

The file \ttt{output.log} contains mainly a listing of the chosen options and the resulting dynamics settings. At higher print levels, the log file contains also information per time step (useful for debugging). \ttt{output.lis} contains a table with one line per time step, giving active states, energies and expectation values. \ttt{output.dat} contains a list of all important matrices and vectors at each time step. This information can be extracted with \ttt{data\_extractor.x} to yield plottable table files. \ttt{output.xyz} contains the geometries of all time steps (the comments to each geometry give the active state).
For details about the content of the output files, see chapter~\ref{chap:output}.

The restart files contain the full state of a trajectory and its control variables from the last successful time step. These files are needed in order to restart a trajectory at this time step (either because the calculation failed, or in order to extend the simulation time beyond the original maximum simulation time). 

\begin{figure}[htb]
  \centering
  \includegraphics[scale=1]{img/dir_traj/dir_traj_after.pdf}
  \caption[Files of a \sharc\ dynamics simulation after running.]{Files of a \sharc\ dynamics simulation after running. Directories are in blue, executable scripts in green, regular files in white and optional files in grey. Output files are in yellow.}
  \label{fig:dir_traj_after}
\end{figure}




\section{Typical workflow for an ensemble of trajectories}
\label{sec:typical_workflow}

Usually, one is not interested in running only a single trajectory, since a single trajectory cannot reproduce the branching of a wave packet into different reaction channels. In order to do so, within surface hopping an ensemble of independent trajectories is employed. 

When dealing with a (possibly large) ensemble of trajectories, setup and analysis need to be automatized. Hence, the \sharc\ suite contains a number of scripts fulfilling different tasks in the usual workflow of setting up ensembles of trajectories.
The typical workflow is given schematically in figure~\ref{fig:workflow}.

\begin{figure}[htb]
  \centering
  \includegraphics[scale=1]{img/workflow/prepare.pdf}
  \caption{Typical workflow for conducting excited-state dynamics simulations with \sharc.}
  \label{fig:workflow}
\end{figure}

\subsection{Initial condition generation}

In the typical workflow, the user will first create a set of suitable initial conditions. In the context of the \sharc\ package, an initial condition is a set of an initial geometry, initial velocity, initial occupied state and initial wave function coefficients. 
Many such sets are needed in order to setup physically sound dynamics simulations.

\paragraph{Generation of initial geometries and velocities}

Currently, within the \sharc\ suite, initial geometries and velocities can be generated based on a quantum harmonic oscillator Wigner distribution. The theoretical background is given in section~\ref{met:wigner}. The calculation is performed by \ttt{wigner.py}, which is explained in section~\ref{sec:wigner.py}. 

As given in figure~\ref{fig:workflow}, \ttt{wigner.py} needs as input the result of a frequency calculation in \textsc{Molden} format. The calculation can be performed by any quantum chemistry program and any method the user sees fit (there are some scripts which can aid in the frequency calculation, see sections~\ref{sec:molpro_input.py}, \ref{sec:molcas_input.py}, \ref{sec:ADF_freq.py}). 
\ttt{wigner.py} produces the file \ttt{initconds}, which contains a list of initial conditions ready for further processing.

Alternatively, initial geometries and velocities can be extracted from molecular dynamics simulations in the ground state.
Currently, it is possible to either convert restart files \textsc{Amber} to an \ttt{initconds} file (using \ttt{amber\_to\_initconds.py}, see section~\ref{sec:amber_to_initconds.py}) or to randomly sample snapshots from \sharc\ trajectories (using \ttt{sharctraj\_to\_initconds.py}, see section~\ref{sec:sharctraj_to_initconds.py}).

\paragraph{Generation of initial coefficients and states}

In the second preparation step, for each of the sampled initial geometries it has to be decided which excited state should be the initial one. In simple cases, the user may manually choose the initial excited state using \ttt{excite.py} (optionally after diabatization; see~\ref{sec:excite.py}). Alternatively, the selection of initial states can be performed based on the excitation energies and oscillator strengths of the excited states at each initial geometry (this approximately simulates a delta-pulse excitation). 

The latter options (diabatization or energies/oscillator strengths) make it necessary to carry out vertical excitation calculation before the selection of the initial states.
The calculations can be set up with \ttt{setup\_init.py} (see section~\ref{sec:setup_init.py}). This script prepares for each initial condition in the \ttt{initconds} file a directory with the necessary input to perform the calculation. The user should then execute the run script (\ttt{run.sh}) in each of the directories (either manually or through a batch queueing system).

After the vertical excitation calculations are completed, the vertical excitation energies and oscillator strengths of each calculation are collected by \ttt{excite.py} (see~\ref{sec:excite.py}). The same script then performs the selection of the initial electronic state for each initial geometry. The results are written to a new file, \ttt{initconds.excited}. This file contains all information needed to setup the ensemble. 

Additionally, \ttt{spectrum.py} (\ref{sec:spectrum.py}) can calculate absorption spectra based on the \ttt{initconds.excited} file. This may be useful to verify that the level of theory chosen is appropriate (e.g., by comparing to an experimental spectrum), or to choose a suitable excitation window for the determination of the initial state.

\subsection{Running the dynamics simulations}

Based on the initial conditions given in \ttt{initconds.excited}, the input for all trajectories in the ensemble can be setup by \ttt{setup\_traj.py} (see section~\ref{sec:setup_traj.py}). The script produces one directory for each trajectory, containing the input files for \sharc\ and the selected interface.

In order to run a particular trajectory, the user should execute the run script (\ttt{run.sh}) in the directory of the trajectory. Since those calculations can run between minutes and several weeks (depending on the level of theory used and the number of time steps), it is advisable to submit the run scripts to a batch queueing system. 

The progress of the simulations can be monitored most conveniently in the \ttt{output.lis} files. If the calculations are running in some temporary directory, the output files can be copied to the local directory (where they were setup) with the \ttt{scp} wrapper \ttt{retrieve.sh} (see \ref{sec:retrieve}). This allows to perform ensemble analysis while the trajectories are still running.

If a trajectory fails, the temporary directory where the calculation is running is not deleted. The file \ttt{README} will be created in the trajectory's directory, giving the time of the failure and the location of the temporary data, so that the case can be investigated. 

In order to signal \sharc\ to terminate a trajectory after the current time step is completed, the user can create a (possibly empty) file \ttt{STOP} in the working directory of the trajectory (the directory where \ttt{sharc.x} is running).

The status of the ensemble of trajectories can be checked with \ttt{diagnostics.py} (section~\ref{sec:diagnostics.py}). This script checks the presence and integrity of all relevant files, the progress of all trajectories, and warns if trajectories behave unexpectedly (non-conversion of total energy, intruder states, etc).

\subsection{Analysis of the dynamics results}

Each trajectory can be analyzed independently by inspecting the output files (see chapter~\ref{chap:output}). Most importantly, calling \ttt{data\_extractor.x} (\ref{sec:data_extractor.x}) on the \ttt{output.dat} file of a trajectory creates a number of formatted files which can be plotted with the help of \ttt{make\_gnuscript.py} (\ref{sec:make_gnuscript.py}) and \textsc{Gnuplot}.
The nuclear geometries in \ttt{output.xyz} file can be analyzed in terms of internal coordinates (bond lengths, angles, ring conformations, etc.) using \ttt{geo.py} (\ref{sec:geo.py}).
The manual analysis of all individual trajectories is usually a good idea to verify that the trajectories are correctly executing, and in order to find general reaction pathways.
The manual analysis often permits to formulate some hypotheses, which can then be verified with the statistical analysis tools.

For the complete ensemble, the first step should usually be to run \ttt{diagnostics.py} (section~\ref{sec:diagnostics.py}).
This script will determine how long the different trajectories are, and, more importantly, will check the trajectories for file integrity, conservation of total energy, and continuity of potential/kinetic energy.
Based on a set of customizable criteria, the script determines for each trajectory a ``maximum usable time''.
The script then can mark all trajectories with maximum usable time below a given threshold to be excluded from analysis (by creating a file \ttt{DONT\_ANALYZE} in the trajectory's directory).
The other analysis scripts will then ignore trajectories marked by \ttt{diagnostics.py}.
Trajectories can also be manually excluded from analysis, by creating a file called \ttt{CRASHED}, \ttt{DEAD}, or \ttt{RUNNING} in the respective directory.

After the trajectories were checked and unsuitable ones excluded, the statistical analysis scripts can be used.
The script \ttt{populations.py} (section~\ref{sec:populations.py}) can calculate average excited-state populations. 
The script \ttt{transition.py} (section~\ref{sec:transition.py}) can analyze the total number of hops between all pairs of states in an ensemble, allowing to identify relevant relaxation routes in the dynamics.
Using the script \ttt{make\_fitscript.py} (\ref{sec:make_fitscript.py}), it is possible to make elaborate global fits of chemical kinetics models to the populations data, allowing to extract rate constants from the populations.
With \ttt{bootstrap.py} (\ref{sec:bootstrap.py}) one can compute errors for these rate constants.
The fitting and bootstrapping functionalities are also available in the stand-alone script \ttt{make\_fit.py} (\ref{sec:make_fit.py}).
The script~\ttt{crossing.py} (\ref{sec:crossing.py}) can find and extract notable geometries, e.g., those geometries where a surface hop between two particular states occurred.
Using \ttt{trajana\_essdyn.py} (\ref{sec:trajana_essdyn.py}) and \ttt{trajana\_nma.py} (\ref{sec:trajana_nma.py}) it is furthermore possible to perform essential dynamics analysis and normal mode analysis.
Finally, \ttt{data\_collector.py} can merge arbitrary tabulated data from the trajectories and perform various analysis procedures (compute mean/standard deviation, data convolution, summation, integration), which can be used to compute, e.g., time-dependent distribution functions or time-dependent spectra.


\section{Auxilliary Programs and Scripts}

The following tables list the auxiliary programs in the \sharc\ suite. The rightmost column gives the section where the program is documented.

\subsection{Setup}

\begin{tabular}{>{\ttfamily}lp{10.5cm}r}
  wigner.py             &Creates initial conditions from Wigner distribution.                   &\ref{sec:wigner.py}\\
  amber\_to\_initconds.py       &Creates initial conditions from Amber restart files.           &\ref{sec:amber_to_initconds.py}\\
  sharctraj\_to\_initconds.py   &Creates initial conditions from \sharc\ trajectories.          &\ref{sec:sharctraj_to_initconds.py}\\
  setup\_init.py        &Sets up initial vertical excitation calculations.                      &\ref{sec:setup_init.py}\\
  excite.py             &Generates excited state lists for initial conditions and selects initial states.                 &\ref{sec:excite.py}\\
  setup\_traj.py        &Sets up the dynamics simulations based on the initial conditions.      &\ref{sec:setup_traj.py}\\
  laser.x               &Prepares files containing laser fields.                                &\ref{sec:laser.x}\\
  molpro\_input.py      &Prepares \textsc{Molpro} input files and template files for the \textsc{Molpro} interface.                  &\ref{sec:molpro_input.py}\\
  molcas\_input.py      &Prepares \textsc{Molcas} input files and template files for the \textsc{Molcas} interface.             &\ref{sec:molcas_input.py}\\
  ADF\_input.py         &Prepares ADF input files.                     &\ref{sec:ADF_input.py}\\
  ADF\_freq.py          &Converts ADF output files of frequency calculations to Molden format.                     &\ref{sec:ADF_freq.py}\\
  QMout2LVC.py          &Converts output from a single point calculation to template for the LVC interface.             &\ref{sec:setup_LVCparam.py}\\
\end{tabular}

\subsection{Analysis}

\begin{tabular}{>{\ttfamily}lp{10.5cm}r}
  spectrum.py           &Generates absorption spectra from initial conditions files.            &\ref{sec:spectrum.py}\\
  retrieve.sh           &\ttt{scp} wrapper to retrieve dynamics output during the simulation.   &\ref{sec:retrieve}\\
  data\_extractor.x     &Extracts plottable results from the \sharc\ output data file.          &\ref{sec:data_extractor.x}\\
  make\_gnuscript.py    &Creates gnuplot scripts to plot trajectory data.                       &\ref{sec:make_gnuscript.py}\\
  diagnostics.py        &Checks ensembles for integrity, progress, energy conservation.         &\ref{sec:diagnostics.py}\\
  populations.py        &Calculates ensemble populations.                                       &\ref{sec:populations.py}\\
  transition.py         &Calculates total number of hops within an ensemble.                    &\ref{sec:transition.py}\\
  make\_fitscript.py    &Performs kinetic model fits and bootstrapping.                         &\ref{sec:make_fit.py}\\
  make\_fitscript.py    &Creates \textsc{Gnuplot} scripts to fit kinetic models to population data.      &\ref{sec:make_fitscript.py}\\
  bootstrap.py          &Computes errors for kinetic model fits.                                &\ref{sec:bootstrap.py}\\
  crossing.py           &Extracts specific geometries from ensembles.                           &\ref{sec:crossing.py}\\
  geo.py                &Calculates internal coordinates from xyz files.                        &\ref{sec:geo.py}\\
  trajana\_essdyn.py    &Performs an essential dynamics analysis for an ensemble.               &\ref{sec:trajana_essdyn.py}\\
  trajana\_nma.py       &Performs a normal mode analysis for an ensemble.                       &\ref{sec:trajana_nma.py}\\
  data\_collector.py    &Collects data from tabular files and performs various analyses         &\ref{sec:data_collector.py}\\
\end{tabular}

\subsection{Interfaces}

\begin{tabular}{>{\ttfamily}lp{10.5cm}r}
  SHARC\_MOLPRO.py      &Calculates SOCs, gradients, nonadiabatic couplings, overlaps,     dipole moments, and Dyson norms at the CASSCF level of theory (using \textsc{Molpro}). Symmetry or RASSCF are not supported. Only segmented basis sets are possible.   &\ref{sec:int:molpro}\\
  SHARC\_MOLCAS.py      &Calculates SOCs, gradients,                         overlaps,     dipole moments,     Dyson norms, dipole moment derivatives, and spin-orbit coupling derivatives at the CASSCF and (MS-)CASPT2 level of theory (using \textsc{Molcas}). Symmetry is not supported. Numerical differentiation is used for some tasks.   &\ref{sec:int:molcas}\\
  SHARC\_COLUMBUS.py    &Calculates SOCs, gradients, nonadiabatic couplings, overlaps,     dipole moments, and Dyson norms at the CASSCF, RASSCF, and MRCISD levels of theory (using \textsc{Columbus}). Symmetry is not supported. Works with the either \textsc{Dalton} or \textsc{Seward} integrals (through the \textsc{Columbus}-\textsc{Molcas} interface), but SOCs are only available with \textsc{Seward} integrals. Nonadiabatic couplings are only available with \textsc{Dalton} integrals.                &\ref{sec:int:columbus}\\
  SHARC\_Analytical.py  &Calculates SOCs, gradients,                         overlaps,     dipole moments, and              dipole moment derivatives based on analytical expressions of diabatic matrix elements defined in Cartesian coordinates.       &\ref{sec:int:analytical}\\
  SHARC\_ADF.py         &Calculates SOCs, gradients,                         overlaps,     dipole moments, and Dyson norms at the TD-DFT level of theory with GGA and hybrid functionals (using \textsc{ADF}). Symmetry is not supported.  &\ref{sec:int:adf}\\
  SHARC\_RICC2.py       &Calculates SOCs, gradients,                         overlaps, and dipole moments at the ADC(2) and CC2 levels of theory (using \textsc{Turbomole}). Symmetry is not supported. For SOCs, only ADC(2) can be used and \textsc{Orca} has to be installed in addition to \textsc{Turbomole}.        &\ref{sec:int:ricc2}\\
  SHARC\_LVC.py         &Calculates SOCs, gradients, nonadiabatic couplings, overlaps, and dipole moments based on linear-vibronic coupling models defined in mass-weighted normal mode coordinates.         &\ref{sec:int:lvc}\\
  SHARC\_GAUSSIAN.py    &Calculates       gradients,                         overlaps,     dipole moments, and Dyson norms at the TD-DFT level of theory (using Gaussian). Symmetry is not supported.  &\ref{sec:int:gaussian}\\
  SHARC\_ORCA.py        &Calculates SOCs, gradients,                         overlaps,     dipole moments, and Dyson norms at the TD-DFT level of theory with GGA and hybrid functionals (using \textsc{Orca}). Symmetry is not supported.  &\ref{sec:int:orca}\\
\end{tabular}


\subsection{Others}

\begin{tabular}{>{\ttfamily}lp{10.5cm}r}
  tests.py              &Script to automatically run the \sharc\ test suite.                                                    &\ref{sec:tests.py}\\
  wfoverlap.x           &Program to compute wave function overlaps, used by most interfaces.                                    &\ref{sec:int:wfoverlap}\\
  Orca\_External        &Script to carry out optimizations with \textsc{Orca} as optimizer and \sharc\ as gradient provider.    &\ref{sec:Orca_External}\\
  setup\_orca\_opt.py   &Script to setup optimizations with \textsc{Orca} as optimizer and \sharc\ as gradient provider.        &\ref{sec:Orca_External}\\
  setup\_single\_point.py   &Script to setup single point calculations with \sharc\ interfaces.        &\ref{sec:setup_single_point.py}\\
  QMout\_print.py       &Script to convert a \ttt{QM.out} file to a table with energies and oscillator strengths.               &\ref{sec:QMout_print.py}\\
  diagonalizer.x        &Helper program for \ttt{excite.py}. Only required if \textsc{NumPy} is not available.                  &\ref{sec:diagonalizer.x}\\
\end{tabular}

% ========================================================================================================= %
% ========================================================================================================= %
% ========================================================================================================= %

\chapter{Input files}\label{chap:input}

In this chapter, the format of all \sharc\ input files are presented. Those are the main input file (here called \ttt{input}), the geometry file, the velocity file, the coefficients file, the laser file, and the atom mask file. Only the first two are mandatory, the others are optional input files. All input files are ASCII text files.

% ========================================================================================================= %

\section{Main input file}\label{sec:inputfile}

This section presents the format and all input keywords for the main \sharc\ input. Note that when using \ttt{setup\_traj.py}, full knowledge of the \sharc\ input keywords is not required.

\subsection{General remarks}

The input file has a relatively flexible structure. With very few exceptions, each single line is independent. An input line starts with a keyword, followed optionally by a number of arguments to this keyword. Example:

\begin{example}
  \verb|stepsize 0.5|
\end{example}

Here, \ttt{stepsize} is the keyword, referring to the size of the time steps for the nuclear motion in the dynamics. \ttt{0.5} gives the size of this time step, in this example 0.5~fs.

A number of keywords have no arguments and act as simple switches (e.g., \ttt{restart}, \ttt{gradcorrect}, \ttt{grad\_select}, \ttt{nac\_select}, \ttt{ionization}, \ttt{track\_phase}, \ttt{dipole\_gradient}). Those keywords can be prefixed with \ttt{no} to explicitly deactivate the option (e.g., \ttt{norestart} deactivates restarts).

In each line a trailing comment can be added in the input file, by using the special character \ttt{\#}. Everything after \ttt{\#} is ignored by the input parser of \sharc. The input file also can contain arbitrary blank lines and lines containing only comments. All input is case-insensitive.

The input file is read by \sharc\ by subsequently searching the file for all known keywords. Hence, unknown or misspelled keywords are ignored. Also, the order of the keywords is completely arbitray. Note however, that if a keyword is repeated in the input only the \textit{first} instance is used by the program. 

\subsection{Input keywords}

In Table~\ref{tab:input}, all input keywords for the \sharc\ input file are listed.

\clearpage
{
%%tth: \newcommand{\DEFAULT}[1]{\textbf{\textcolor{PineGreen}{#1}}}
\tthdump{
  \newcommand{\DEFAULT}[1]{\textbf{\textcolor{G}{#1}}}
}
\begin{longtable}{|>{\ttfamily}l|l|p{8.5cm}|}
  \caption[Input keywords for \ttt{sharc.x}.]{Input keywords for \ttt{sharc.x}. The first column gives the name of the keyword, the second lists possible arguments and the third line provides an explanation. Defaults are marked like \DEFAULT{this}. \$$n$ denotes the $n$-th argument to the keyword. }  \label{tab:input}\\

% ========================================

    \hline
    \rmfamily Keyword     &Arguments    &Explanation\\
    \hline
  \endfirsthead

% ========================================

\tthdump{
    \multicolumn{3}{c}{{\bfseries \tablename\ \thetable{} \mdseries-- Continued from previous page}} \\
    \hline
    \rmfamily Keyword     &Arguments    &Explanation\\
    \hline
  \endhead
}

% ========================================

\tthdump{
    \hline 
    \multicolumn{3}{r}{{Continued on next page}} \\ 
%     \hline
  \endfoot
}
  
% =======================================

\tthdump{
    \hline
  \endlastfoot
}

% ========================================

  \multicolumn{3}{|c|}{\cellcolor{black!10}--- General control keywords ---}\\
  \hline
  printlevel            &\textbf{integer}                    &Controls the verbosity of the log file.\\
                        &\$1=0                               &{\footnotesize Log file is empty}\\
                        &\$1=1                               &{\footnotesize + List of internal steps}\\
                        &\$1\DEFAULT{=2}                     &{\footnotesize + Input parsing information}\\
                        &\$1=3                               &{\footnotesize + Some information per time step}\\
                        &\$1=4                               &{\footnotesize + More information per time step}\\
                        &\$1=5                               &{\footnotesize + Much more information per time step}\\
  \hline
  restart               &                                    &Dynamics is resumed from restart files.\\
  \DEFAULT{norestart}   &                                    &Dynamics is initialized from input files.\\
                        &                                    &{\footnotesize \ttt{norestart} takes precedence.}\\
  \hline
  rngseed               &\textbf{integer}                    &Seed for the random number generator.\\
                        &\DEFAULT{10997279}                  &{\footnotesize Used for surface hopping and AFSSH decoherence.}\\
  compatibility         &\$1=\DEFAULT{0}                     &Compatibility mode disabled.\\
                        &\$1=1                               &Do not draw a second random number per step (for decoherence).\\
  \hline


  \multicolumn{3}{|c|}{\cellcolor{black!10}--- Input file keywords ---}\\
  \hline
  geomfile              &\textbf{quoted string}              &File name containing the initial geometry.\\
                        &\DEFAULT{"geom"}                    &\\
  \hline
  velocfile             &\textbf{quoted string}              &File containing the initial velocities.\\
                        &\DEFAULT{"veloc"}                   &{\footnotesize Only read if \ttt{veloc external}.}\\
  \hline
  coefffile             &\textbf{quoted string}              &File containing the initial wave function coefficients.\\
                        &\DEFAULT{"coeff"}                   &{\footnotesize Only read if \ttt{coeff external}.}\\
  \hline
  laserfile             &\textbf{quoted string}              &File containing the laser field.\\
                        &\DEFAULT{"laser"}                   &{\footnotesize Only read if \ttt{laser external}.}\\
  \hline
  atommaskfile          &\textbf{quoted string}              &File containing the atom mask.\\
                        &\DEFAULT{"atommask"}                &{\footnotesize Only read if \ttt{atommask external}.}\\
  \hline


  \multicolumn{3}{|c|}{\cellcolor{black!10}--- Trajectory initialization keywords ---}\\
  \hline
  veloc                 &\textbf{string}                     &Sets the initial velocities.\\
                        &\$1\DEFAULT{=zero}                  &{\footnotesize Initial velocities are zero.}\\
                        &\$1=random \$2 \textbf{float}       &{\footnotesize Random initial velocities with \$2 eV kinetic energy per atom.}\\
                        &\$1=external                        &{\footnotesize Initial velocities are read from file.}\\
  \hline
  nstates               &list of \textbf{integer}s           &Number of states per multiplicity.\\
                        &\$1 (\DEFAULT{1})                   &{\footnotesize Number of singlet states}\\
                        &\$2 (\DEFAULT{0})                   &{\footnotesize Number of doublet states}\\
                        &\$3 (\DEFAULT{0})                   &{\footnotesize Number of triplet states}\\
                        &\$\dots (\DEFAULT{0})               &{\footnotesize Number of states of higher multiplicities}\\
  \hline
  actstates             &list of \textbf{integer}s           &Number of active states per multiplicity.\\
                        &\DEFAULT{same as \ttt{nstates}}     &{\footnotesize By default, all states are active.}\\
  \hline
  state                 &\textbf{integer}, \textbf{string}   &Specifies the initial state.\\
                        &                                    &(no default; \sharc\ exits if \ttt{state} is missing).\\
                        &\$1                                 &{\footnotesize Initial state.}\\
                        &\$2=MCH                             &{\footnotesize Initial state and coefficients are given in MCH representation.}\\
                        &\$2=diag                            &{\footnotesize Initial state and coefficients are given in diagonal representation.}\\
  \hline
  coeff                 &\textbf{string}                     &Sets the wave function coefficients.\\
                        &\$1\DEFAULT{=auto}                  &{\footnotesize Initial coefficient are determined automatically from initial state.}\\
                        &\$1=external                        &{\footnotesize Initial coefficients are read from file.}\\
  \hline


  \multicolumn{3}{|c|}{\cellcolor{black!10}--- Laser field keywords ---}\\
  \hline
  laser                 &\textbf{string}                     &Sets the laser field.\\
                        &\$1\DEFAULT{=none}                  &{\footnotesize No laser field is applied.}\\
                        &\$1=internal                        &{\footnotesize Laser field is calculated at each time step from internal function.}\\
                        &\$1=external                        &{\footnotesize Laser field for each time step is read during initialization.}\\
  \hline
  laserwidth            &\textbf{float}                      &Laser bandwidth used to detect induced hops.\\
                        &\DEFAULT{1.0 eV}                    &\\
  \hline


  \multicolumn{3}{|c|}{\cellcolor{black!10}--- Time step keywords ---}\\
  \hline
  stepsize              &\textbf{float}                      &Length of the nuclear dynamics time steps in fs.\\
                        &\DEFAULT{0.5 fs}                    &\\
  \hline
  nsubsteps             &\textbf{integer}                    &Number of substeps for the integration of the electronic equation of motion.\\
                        &\DEFAULT{25}                        &\\
  \hline
  nsteps                &\textbf{integer}                    &Number of simulation steps.\\
                        &\DEFAULT{3}                         &\\
  \hline
  tmax                  &\textbf{float}                      &Total length of the simulation in fs.\\
                        &                                    &{\footnotesize No effect if \ttt{nsteps} is present.}\\
  \hline
  killafter             &\textbf{float}                      &Terminates the trajectory after \$1 fs in the lowest state. \\
                        &\DEFAULT{-1}                        &{\footnotesize If \$1$<$0, trajectories are never killed.}\\
  \hline


  \multicolumn{3}{|c|}{\cellcolor{black!10}--- Surface hopping setting keywords ---}\\
  \hline
  surf                  &\textbf{string}                     &Potential energy surfaces used in surface hopping.\\
                        &\$1\DEFAULT{=diagonal,sharc}        &{\footnotesize Uses diagonal potentials.}\\
                        &\$1=MCH                             &{\footnotesize Uses MCH potentials.}\\
  \hline
  coupling              &\textbf{string}                     &Quantities describing the nonadiabatic couplings.\\
                        &\$1=ddr,nacdr                       &{\footnotesize Uses vectorial nonadiabatic couplings $\langle\psi_\alpha|\partial/\partial R|\psi_\beta\rangle$.}\\
                        &\$1=ddt,nacdt                       &{\footnotesize Uses temporal nonadiabatic couplings $\langle\psi_\alpha|\partial/\partial t|\psi_\beta\rangle$.}\\
                        &\$1=\DEFAULT{overlap}               &{\footnotesize Uses the overlaps $\langle\psi_\alpha(t_0)|\psi_\beta(t)\rangle$ (local diabatization).}\\
  \hline
  gradcorrect           &                                    &Include $(E_\alpha-E_\beta)\langle\psi_\alpha|\partial/\partial R|\psi_\beta\rangle$ in gradient transformation.\\
  \DEFAULT{nogradcorrect}&                                   &Transform only the gradient matrix.\\
  \hline
  ekincorrect           &\textbf{string}                     &Adjustment of the kinetic energy after a surface hop.\\
                        &\$1=none                            &{\footnotesize Kinetic energy is not adjusted. Jumps are never frustrated.}\\
                        &\$1\DEFAULT{=parallel\_vel}         &{\footnotesize Velocity is rescaled to adjust kinetic energy.}\\
                        &\$1=parallel\_nac                   &{\footnotesize Only the velocity component in the direction of $\langle\psi_\alpha|\partial/\partial R|\psi_\beta\rangle$ is rescaled.}\\
                        &\$1=parallel\_diff                  &{\footnotesize Only the velocity component in the direction of $\Delta\nabla E$ is rescaled.}\\
  \hline
  reflect\_frustrated   &\textbf{string}                     &Reflection of trajectory after frustrated hop.\\
                        &\$1\DEFAULT{=none}                  &{\footnotesize No reflection.}\\
                        &\$1=parallel\_vel                   &{\footnotesize Full velocity vector is reflected.}\\
                        &\$1=parallel\_nac                   &{\footnotesize Only the velocity component in the direction of $\langle\psi_\alpha|\partial/\partial R|\psi_\beta\rangle$ is reflected.}\\
                        &\$1=parallel\_diff                  &{\footnotesize Only the velocity component in the direction of $\Delta\nabla E$ is reflected.}\\
  \hline
  decoherence\_scheme     &\textbf{string}                     &Method for decoherence correction.\\
                        &\$1\DEFAULT{=none}                  &{\footnotesize No decoherence correction.}\\
                        &\$1=edc                             &{\footnotesize Energy-difference based correction.\cite{Granucci2010JCP}}\\
                        &\$1=afssh                           &{\footnotesize Augmented FSSH.\cite{Jain2016JCTC}}\\

  \hline
  decoherence\_param    &\textbf{float}                      &Value $\alpha$ in EDC decoherence (in Hartree).\\
                        &\DEFAULT{0.1}                       &{\footnotesize \$1$>0.0$}\\
  \hline
  decoherence           &                                    &Applies decoherence correction (EDC by default).\\
  \DEFAULT{nodecoherence}&                                   &No decoherence correction.\\
                        &                                    &{\footnotesize \ttt{nodecoherence} takes precedence.}\\
  \hline
  hopping\_procedure    &\textbf{string}                     &Method for hopping probabilities.\\
                        &\$1=off                             &{\footnotesize No hops (same as \ttt{no\_hops}).}\\
                        &\$1\DEFAULT{=sharc,standard}        &{\footnotesize Standard \sharc\ hopping probabilities.}\\
                        &\$1=gfsh                            &{\footnotesize Global flux SH hopping probabilities.\cite{Wang2014JCTC}}\\
  \hline
  no\_hops              &                                    &Disables surface hopping.\\
                        &                                    &{\footnotesize \ttt{no\_hops} takes precedence.}\\
  \hline
  atommask              &\textbf{string}                     &Activates masking of atoms (for EDC, \ttt{parallel\_vel}).\\
                        &\$1\DEFAULT{=none}                  &{\footnotesize No atoms are masked.}\\
                        &\$1=external                        &{\footnotesize Atom mask is read from external file.}\\
  \hline


  \multicolumn{3}{|c|}{\cellcolor{black!10}--- Energy control keywords ---}\\
  \hline
  ezero                 &\textbf{float}                      &Energy shift for Hamiltonian diagonal elements (Hartree).\\
                        &\DEFAULT{0.0}                       &{\footnotesize Is not determined automatically!}\\
  \hline
  scaling               &\textbf{float}                      &Scaling factor for Hamiltonian matrix and gradients.\\
                        &\DEFAULT{1.0}                       &{\footnotesize $0.<$\$1}\\
  \hline
  dampeddyn             &\textbf{float}                      &Scaling factor for kinetic energy at each time step.\\
                        &\DEFAULT{1.0}                       &{\footnotesize $0.\le$\$1$\le1.$}\\
  \hline


  \multicolumn{3}{|c|}{\cellcolor{black!10}--- Gradient and NAC selection keywords ---}\\
  \hline
  grad\_select          &                                    &Only some gradients are calculated at every time step.\\
  \DEFAULT{grad\_all}   &                                    &All gradients are calculated at every time step (Alias: \ttt{nograd\_select}).\\
                        &                                    &{\footnotesize \ttt{grad\_all} takes precedence.}\\
  \hline
  nac\_select           &                                    &Only some $\langle\psi_\alpha|\partial/\partial R|\psi_\beta\rangle$ are calculated at every time step.\\
  \DEFAULT{nac\_all}    &                                    &All $\langle\psi_\alpha|\partial/\partial R|\psi_\beta\rangle$ are calculated at every time step (Alias: \ttt{nonac\_select}).\\
                        &                                    &{\footnotesize \ttt{nac\_all} takes precedence.}\\
  \hline
  eselect               &\textbf{float}                      &Parameter for selection of gradients and NACs (in eV).\\
                        &\DEFAULT{0.5 eV}                    &\\
  \hline
  select\_directly      &                                    &Do not do a second QM calculation for gradients and NACs.\\
  \hline


  \multicolumn{3}{|c|}{\cellcolor{black!10}--- Phase tracking keywords ---}\\
  \hline
  \DEFAULT{track\_phase}&                                    &Track the phase of the transformation matrix $\mathbf{U}$.\\
  notrack\_phase        &                                    &No phase tracking of $\mathbf{U}$ (only for debugging).\\
  \hline
  \DEFAULT{phases\_from\_interface}   &                      &Request phase information from interface.\\
  nophases\_from\_interface           &                      &Try to recover phase information from QM data.\\
  \hline
  phases\_at\_zero      &                                    &Request phase from interface at $t=0$.\\
  \hline


  \multicolumn{3}{|c|}{\cellcolor{black!10}--- Property computation keywords ---}\\
  \hline
  \DEFAULT{spinorbit}   &                                    &Include spin-orbit couplings into the Hamiltonian.\\
  nospinorbit           &                                    &Neglect spin-orbit couplings.\\
  \hline
  dipole\_gradient              &                            &Include dipole moments derivatives in the gradients.\\
  \DEFAULT{nodipole\_gradient}  &                            &Neglect dipole moments derivatives.\\
  \hline
  ionization            &                                    &Calculate ionization probabilities on-the-fly.\\
  \DEFAULT{noionization}&                                    &No ionization probabilities.\\
  \hline
  ionization\_step      &\textbf{integer}                    &Calculate ionization probabilities every \$1 time step.\\
                        &\DEFAULT{1}                         &{\footnotesize By default calculated every time step (if \ttt{ionization}).}\\
  \hline
  theodore              &                                    &Calculate wavefunction descriptors on-the-fly.\\
  \DEFAULT{notheodore}  &                                    &No wavefunction descriptors.\\
  \hline
  theodore\_step        &\textbf{integer}                    &Calculate wavefunction descriptors every \$1 time step.\\
                        &\DEFAULT{1}                         &{\footnotesize By default calculated every time step (if \ttt{theodore}).}\\
  \hline
  n\_property1d         &\textbf{integer}                    &Allocate for that many 1D properties.\\
                        &\DEFAULT{1}                         &{\footnotesize }\\
  \hline
  n\_property2d         &\textbf{integer}                    &Allocate for that many 2D properties.\\
                        &\DEFAULT{1}                         &{\footnotesize }\\
  \hline


  \multicolumn{3}{|c|}{\cellcolor{black!10}--- Output control keywords ---}\\
  \hline
%   \DEFAULT{write\_soc}                &                      &Writes SOCs to \ttt{output.dat}.\\
%   nowrite\_soc                        &                      &\todo{does nothing!}\\
%   \hline
  write\_grad                         &                      &Writes gradients to \ttt{output.dat}.\\
  \DEFAULT{nowrite\_grad}             &                      &\\
  \hline
  write\_nacdr                        &                      &Writes NACs to \ttt{output.dat}.\\
  \DEFAULT{nowrite\_nacdr}            &                      &\\
  \hline
  \DEFAULT{write\_overlap}            &                      &Writes overlaps to \ttt{output.dat}.\\
  nowrite\_overlap                    &                      &{\footnotesize Not written if not requested.}\\
  \hline
  write\_property1d                   &\DEFAULT{on if \ttt{theodore}}                      &Writes 1D properties to \ttt{output.dat}.\\
  nowrite\_property1d                 &                      &\\
  \hline
  write\_property2d                   &\DEFAULT{on if \ttt{ionization}}                      &Writes 2D properties to \ttt{output.dat}.\\
  nowrite\_property2d                 &                      &\\
  \hline


\end{longtable}
}

\subsection{Detailed Description of the Keywords}\label{ssec:input:keywords}

\paragraph{Printlevel}

The \ttt{printlevel} keyword controls the verbosity of the log file. The data output file (\ttt{output.dat}) and the listing file (\ttt{output.lis}) are not affected by the print level. The print levels are described in section~\ref{sec:logfile}.

\paragraph{Restart}

There are two keywords controlling trajectory restarting. The keyword \ttt{restart} enables restarting, while \ttt{norestart} disables restart. If both keywords are present, \ttt{norestart} takes precedence. The default is no restart.

When restarting, all control variables are read from the restart file instead of the input file. The only exceptions are \ttt{nsteps} and \ttt{tmax}. In this way, a trajectory which ran for the full simulation time can easily be restarted to extend the simulation time.

Note that none of the auxiliary scripts adds the \ttt{restart} keyword to the input file. The user has to manually add the restart file to the input files of the relevant trajectories.

\paragraph{RNG Seed}

The RNG seed is used to initialize the random number generator, which provides the random numbers for the surface hopping procedure (and the AFSSH decoherence scheme). For details how the seed is used internally, see section~\ref{met:seed}.

Note that in the case of a restart, the random number generator is seeded normally, and then the appropriate number of random numbers is drawn so that the random number sequence is consistent.

\paragraph{Geometry Input}

The initial geometry must be given in a second file in the  \link{http://www.univie.ac.at/columbus/docs_COL70/documentation_main.html}{input format also used by \textsc{Columbus}}. The default name for this file is \ttt{geom}. The geometry filename can be given in the input file with the \ttt{geomfile} keyword. Note that the filename has to be enclosed in single or double quotes. See section~\ref{sec:geomfile} for more details.

\paragraph{Velocity Input}

Using the \ttt{veloc} keyword, the initial velocities can be either set to zero, determined randomly or read from a file. Random determination of the velocities is such that each atom has the same kinetic energy, which must be specified after \ttt{veloc random} in units of eV. Determination of the random velocities is detailed in~\ref{met:veloc}. Note that after the initial velocities are generated, the RNG is reseeded (i.e., the sequence of random numbers in the surface hopping procedure is independent of whether random initial velocities are used).

Alternatively, the initial velocities can be read from a file. 
The default velocity filename is \ttt{veloc}, but the filename can be specified with the \ttt{velocfile} keyword. Note that the filename has to be enclosed in single or double quotes. The file must contain the Cartesian components of the velocity for each atom on a new line, in the same order as in the geomety file. The velocity is interpreted in terms of atomic units (bohr/atu). See section~\ref{sec:velocfile} for more details.

\paragraph{Number of States and Active States}

The keyword \ttt{nstates} controls how many states are taken into account in the dynamics. The keyword arguments specify the number of singlet, doublet, triplet, etc.\ states. There is no hard-coded maximum multiplicity in the \sharc\ code, however, some interfaces may restrict the maximum multiplicity. 

Using the \ttt{actstates} keyword, the dynamics can be restricted to some lowest states in each multiplicity. For each multiplicity, the number of active states must not be larger than the number of states. All couplings between the active states and the frozen states are deleted. These couplings include off-diagonal elements in the $H^{\text{MCH}}$ matrix, in the overlap matrix, and in the matrix containing the nonadiabatic couplings. Freezing states can be useful if transient absorption spectra are to be calculated without increasing computational cost due to the large number of states.

Note that the initial state must not be frozen.

\paragraph{Initial State}

The initial state can be given either in MCH or diagonal representation. The keyword \ttt{state} is followed by an integer specifying the initial state and either the string \ttt{mch} or \ttt{diag}. For the MCH representations, states are enumerated according to the canonical state ordering, see~\ref{met:ordering}. The diagonal states are ordered according to energy. Note that the initial state must be active. 

If the initial state is given in the MCH basis but the dynamics is carried out in the diagonal basis, determination of the initial diagonal state is carried out after the initial QM calculation.

\paragraph{Initial Coefficients}

The initial coefficients can be determined automatically from the initial state, using \ttt{coeff auto} in the input file. If the initial state is given in the diagonal representation as $i$, the initial coefficients are $c^{\text{diag}}_j=\delta_{ij}$. If the initial state is, however, given in the MCH representation, then $c^{\text{MCH}}_j=\delta_{ij}$ and the determination of $\VEC{c}^{\text{diag}}=\VEC{U}^\dagger\VEC{c}^{\text{MCH}}$ is carried out after the initial QM calculation. 
Currently, \ttt{coeff auto} is always used by the automatic setup scripts.

Besides automatic determination, the initial coefficients can be read from a file. The default filename is \ttt{coeff}, but the filename can be given with the keyword \ttt{coefffile}. Note that the filename has to be enclosed in single or double quotes. The file must contain the real and imaginary part of the initial coefficients, one line per state with no blank lines inbetween. These coefficients are interpreted to be in the same representation as the initial state, i.e.\ the \ttt{state} keyword influences the initial coefficients. For details on the file format, see section~\ref{sec:coefffile}.
Note that the setup scripts currently cannot setup trajectories with \ttt{coeff external}, so this can be considered an expert option.

\paragraph{Laser Input}

The keyword \ttt{laser} controls whether a laser field is included in the dynamics (influencing the coefficient propagation and the energies/gradients by means of the Stark effect). 

The input of an external laser field uses the file \ttt{laser}. This file is specified in \ref{sec:laserfile}.

In order to detect laser-induced hops, \sharc\ compares the instantaneous central laser energy with the energy gap between the old and new states. If the difference between the laser energy and the energy gap is smaller than the laser bandwidth (given with the \ttt{laserwidth} keyword), the hop is classified as laser-induced. Those hops are never frustrated and the kinetic energy is not scaled to preserve total energy (instead, the kinetic energy is preserved).

\paragraph{Simulation Timestep}

The keyword \ttt{stepsize} controls the length of a time step (in fs) for the dynamics. The nuclear motion is integrated using the Velocity-Verlet algorithm with this time step. Surface hopping is performed once per time step and 1--3 quantum chemistry calculations are performed per time step (depending on the selection schedule). Each time step is divided in \ttt{nsubsteps} substeps for the integration of the electronic equation-of-motion. Since integration is performed in the MCH representation, the default of 25 substeps is usually sufficient, even if very small potential couplings are encountered. A larger number of substeps might be necessary if high-frequency laser fields are included or if the energy shift (\ttt{ezero}) is not well-chosen.

\paragraph{Simulation Time}

The keyword \ttt{nsteps} controls the total length of the simulation. The total simulation time is \ttt{nsteps} times \ttt{stepsize}. \ttt{nsteps} does not include the initial quantum chemistry calculation. Instead of the number of steps, the total simulation time can be given directly (in fs) using the keyword \ttt{tmax}. In this case, \ttt{nsteps} is calculated as \ttt{tmax} divided by \ttt{stepsize}. If both keywords (\ttt{nsteps} and \ttt{tmax}) are present, \ttt{nsteps} is used. All setup scripts will generally use the \ttt{tmax} keyword.

Using the keyword \ttt{killafter}, the dynamics can be terminated before the full simulation time. \ttt{killafter} specifies (in fs) the time the trajectory can move in the lowest-energy state before the simulation is terminated. By default, simulations always run to the full simulation time and are not terminated prematurely.

\paragraph{Surface Treatment}

The keyword \ttt{surf} controls whether the dynamics runs on diagonal potential energy surfaces (which makes it a \sharc\ simulation) or on the MCH PESs (which corresponds to a spin-diabatic \cite{Granucci2012JCP} or FISH \cite{Mitric2009PRA} simulation, or a regular surface hopping simulation). Internally, dynamics on the MCH potentials is conducted by setting the $U$ matrix equal to the unity matrix at each time step. 

\paragraph{Description of Non-adiabatic Coupling}

The code allows propagating the electronic wave function using three different quantities describing nonadiabatic effects, see~\ref{met:propagate}. The keyword \ttt{coupling} controls which of these quantities is requested from the QM interfaces and used in the propagation. The first option is \ttt{nacdr}, which requires the nonadiabatic coupling vectors $\langle\psi_\alpha|\partial/\partial \VEC{R}|\psi_\beta\rangle$. For the wave function propagation, the scalar product of these vectors and the nuclear velocity is calculated to obtain the matrix $\langle\psi_\alpha|\partial/\partial t|\psi_\beta\rangle$. During the propagation, this matrix is interpolated linearly within each classical time step. Currently, only few \sharc\ interfaces can provide these couplings.

Alternatively, one can directly request the matrix elements $\langle\psi_\alpha|\partial/\partial t|\psi_\beta\rangle$, which can be used for the propagation. The corresponding argument to \ttt{coupling} is \ttt{nacdt}. In this case, the matrix is taken as constant throughout each classical time step. Currently, none of the interfaces in \sharc\ can deliver these couplings, because they are computed via overlaps, and if overlaps are known it is preferable to use local diabatization.

The third possibility is the use of the overlap matrix, requested with \ttt{coupling overlaps} (this is the default). The overlap matrix is used subsequently in the local diabatization algorithm for the wave function propagation. Currently, all \sharc\ interfaces can provide these couplings.

\paragraph{Correction to the Diagonal Gradients}

As detailed in~\ref{met:gradtra}, the correct transformation of the gradients to the diagonal representation includes contributions from the nonadiabatic coupling vectors. Using \ttt{gradcorrect}, these contributions are included. In this case \sharc\ will request the calculation of the nonadiabatic coupling vectors, even if they are not used in the wave function propagation. In order to explicitly turn off this gradient correction, use the \ttt{nogradcorrect} keyword.

\paragraph{Frustrated Hops and Adjustment of the Kinetic Energy}

The keyword \ttt{ekincorrect} controls how the kinetic energy is adjusted after a surface hop to preserve total energy. \ttt{ekincorrect none} deactivates the adjustment, so that the total energy is not preserved after a hop. Using this option, jumps can never be frustrated and are always performed according to the hopping probabilities. 
Using \ttt{ekincorrect parallel\_vel}, the kinetic energy is adjusted by simply rescaling the nuclear velocities so that the new kinetic energy is $E_{\text{tot}}-E_{\text{pot}}$. Jumps are frustrated if the new potential energy would exceed the total energy.
Finally, using \ttt{ekincorrect parallel\_nac}, the kinetic energy is adjusted by rescaling the component of the nuclear velocities parallel to the nonadiabatic coupling vector between the old and new state. The hop is frustrated if there is not enough kinetic energy in this direction to conserve total energy. Note that \ttt{ekincorrect parallel\_nac} implies the calculation of the nonadiabatic coupling vector, even if they are not used for the wave function propagation.

The keyword \ttt{reflect\_frustrated} furthermore controls whether the velocities are inverted after a frustrated hop.
With \ttt{reflect\_frustrated none} (the default), after a frustrated hop, the velocity vector is not modified.
Using \ttt{reflect\_frustrated parallel\_vel}, the full velocity vector is inverted when a frustrated hop is encountered.
With the third option, \ttt{reflect\_frustrated parallel\_nac}, only the velocity component parallel to the nonadiabatic coupling vector between the active and frustrated states is inverted. This implies the calculation of the nonadiabatic coupling vector, even if they are not used for the wave function propagation.

\paragraph{Decoherence Correction Scheme}

There are three options for the decoherence correction (see~\ref{met:decoherence}) in \sharc, which can be selected with the \ttt{decoherence\_scheme} keyword. 

With the default \ttt{decoherence\_scheme none}, no decoherence correction is applied.
The energy-difference based decoherence (EDC) scheme of Granucci et al.~\cite{Granucci2010JCP} can be activated with \ttt{decoherence\_scheme edc}. 
The keyword \ttt{decoherence\_param} can be used to change the relevant parameter $\alpha$ (see~\ref{met:decoherence}). The default is 0.1~Hartree, which is the value recommended by Granucci et al.~\cite{Granucci2010JCP}.
Alternatively, the AFSSH (augmented fewest-switches surface hopping) scheme of Jain et al.~\cite{Jain2016JCTC} can be employed. This scheme does not use any parameters, so the keyword \ttt{decoherence\_param} will have no effect.
Note that in any case, the decoherence correction is applied to the states in the representation chosen with the \ttt{surf} keyword.

The keywords \ttt{decoherence} (activates EDC decoherence) and \ttt{nodecoherence} are present for backwards compatibility.

\paragraph{Surface Hopping Scheme}

There are three options for the computation of the hopping probabilities (see~\ref{met:hopping}) in \sharc, which can be selected with the \ttt{hopping\_procedure} keyword. 

Using \ttt{hopping\_procedure off}, surface hopping will be disabled, such that the active state (in the representation chosen with the \ttt{surf} keyword) will never change.
With the default, \ttt{hopping\_procedure sharc}, the standard hopping probability equation from Ref.~\cite{Mai2015IJQC} will be used.
Alternatively, one can use the global flux surface hopping scheme~\cite{Wang2014JCTC}, which might be advantageous in super-exchange situations.

One can also turn off surface hopping with the \ttt{no\_hops} keyword.

\paragraph{Atom Masking}

Some of the above surface hopping settings might not be fully size consistent: (i) in \ttt{ekincorrect parallel\_vel}, all atoms are uniformly accelerated/slowed during velocity rescaling; (ii) in \ttt{reflect\_frustrated parallel\_vel}, the velocities of all atoms are inverted; (iii) with \ttt{decoherence\_scheme edc}, the kinetic energy of all atoms determines the decoherence rate. In large systems (e.g., in solution), these effects might be unrealistic, because, e.g., a surface hop in the chromophore should not uniformly slow down all water molecules.

The \ttt{atommask} keyword can then be used to exclude certain atoms from the three mentioned procedures. With \ttt{atommask external}, the list of masked and active atoms is read from the file specified with the \ttt{atommaskfile} keyword (default \ttt{"atommask"}). The format of this file is described in section~\ref{sec:atommaskfile}. With the other possible option, \ttt{atommask none} (the default), all atoms are considered for these procedures.

Note that the \ttt{atommask} keyword has no effect on \ttt{ekincorrect parallel\_nac}, \ttt{reflect\_frustrated parallel\_nac}, and \ttt{decoherence\_scheme afssh}, because these procedures are size consistent by themselves.

\paragraph{Reference Energy}

The keyword \ttt{ezero} gives the energy shift for the diagonal elements of the Hamiltonian. The shift should be chosen so that the shifted diagonal elements are reasonably small (large diagonal elements in the Hamiltonian lead to rapidly changing coefficients, requiring extremely short subtime steps). 

Note that the energy shift default is 0, i.e., \sharc\ does not choose an energy shift based on the energies at the first time step (this would lead to each trajectory having a different energy shift).

\paragraph{Scaling and Damping}

The scaling factor for the energies and gradients must be positive (not zero), see section~\ref{met:scaling}.

The damping factor must be in the interval $[0,1]$ (first, since the kinetic energy is always positive; second, because a damping factor larger than 1 would lead to exponentially growing kinetic energy). Also see section~\ref{met:damping}.

\paragraph{Selection of Gradients and Non-Adiabatic Couplings}

\sharc\ allows to selectively calculate only certain gradients and nonadiabatic coupling vectors at each time step. Those gradients and nonadiabatic coupling vectors not selected are not requested from the interfaces, thus decreasing the computational cost. The selection procedure is detailed in~\ref{met:selection}.
Selection of gradients is activated by \ttt{grad\_select}, selection for nonadiabatic couplings by \ttt{nac\_select}. Selection is turned off by default. 

The selection procedure picks only states which are closer in energy to the classically occupied state than a given threshold. The threshold is 0.5~eV by default and can be adjusted using the \ttt{eselect} keyword.

By default, if \sharc\ performs such selection it will do two quantum chemistry calls per time step. In the first call, all quantities are requested except for the ones to be selected. The energies are used to determine which gradients and NACs to calculate in a second quantum chemistry call. The keyword \ttt{select\_directly} tells \sharc\ instead to use the energies of the last time step, so that only one call per time step is necessary.

\paragraph{Phase Tracking}

Phase tracking is an important ingredient in \sharc. It is necessary for two reasons: (i) the columns of the transformation matrix $\VEC{U}$ are determined only up to an arbitrary phase factor $\mathrm{e}^{\mathrm{i}\phi}$ (and additional mixing angles in case of degeneracy), and (ii) the wave functions produced by any quantum chemistry code are determined only up to an arbitrary sign.
Both kind of phases need to be tracked in \sharc\ in order to obtain smoothly varying matrix elements which can be properly integrated.

By default, \sharc\ automatically tracks the phases in the $\VEC{U}$ matrix (explicit keyword: \ttt{track\_phase}), because all required information is always available. This phase tracking can be deactivated with the \ttt{notrack\_phase} keyword, but this option should only be used for debugging purposes.

The tracking of the wave function signs depends on the interfaces, because only they have access to the explicit form of the wave functions. \sharc\ by default (explicit keyword: \ttt{phases\_from\_interface}) requests that the interface tracks the signs and reports any sign changes to \sharc. Currently, all interfaces can provide this phase information, but all of them need to perform overlap calculations to do so. The \ttt{nophases\_from\_interface} keyword can be used to deactivate these requests.

In some situations, it might be necessary to have consistent wave function signs between different trajectories. In this case, the \ttt{phases\_at\_zero} keyword can be used to compute sign information at $t=0$; this requires that the relevant wave function data of the reference is already located in the \ttt{restart/} directory before the trajectory is started. Note that \ttt{phases\_at\_zero} is therefore an expert option.

\paragraph{Spin-Orbit Couplings}

Using the keyword \ttt{nospinorbit} the calculation of spin-orbit couplings is disabled. \sharc\ will only request the diagonal elements of the Hamiltonian from the interfaces. If the interface returns a non-diagonal Hamiltonian anyways, the off-diagonal elements are deleted.

The keyword \ttt{spinorbit} (which is the default) enables spin-orbit couplings.

\paragraph{Dipole Moment Gradients}

The derivatives of the dipole moments can be included in the gradients. This can be activated with the keyword \ttt{dipole\_gradient}. Currently, only the analytical and \textsc{Molcas} interfaces can deliver these quantities.

\paragraph{Ionization}

The keyword \ttt{ionization} activates (\ttt{noionization} deactivates) the on-the-fly calculation of ionization transition properties. If the keyword is given, by default these properties are calculated every time step. The keyword \ttt{ionization\_step} can be used to calculate these properties only every $n$-th time step. 
If the keyword is given, \sharc\ will request the calculation of the ionization properties from the interface, which needs to be able to calculate them.

The ionization probabilities are treated as one 2D property matrix, hence \ttt{n\_property2d} should be at least 1.

\paragraph{\textsc{TheoDORE}}

The keyword \ttt{theodore} activates (\ttt{notheodore} deactivates) the on-the-fly calculation of wave function descriptors with the \textsc{TheoDORE} program. This can be very useful to track the wave function character of the states on-the-fly.
The interface must be able to execute and \textsc{TheoDORE} and return its output to \sharc\ (currently, the ADF, \textsc{Gaussian}, \textsc{Turbomole}, and \textsc{Orca} interfaces can do this). The keyword \ttt{theodore\_step} can be used to calculate these descriptors only every $n$-th time step. 

The \textsc{TheoDORE} descriptors are treated as one 1D property vector for each descriptor, and \ttt{n\_property1d} should be at least as large as the number of descriptors computed by the interface.

\paragraph{Output control}

There are a number of keywords which control what information is written to the \ttt{output.dat} file.
These keywords are \ttt{write\_grad}, \ttt{write\_nacdr}, \ttt{write\_overlap}, \ttt{write\_property1d}, and \ttt{write\_property2d} (and the inverse of each one, e.g., \ttt{nowrite\_grad}).
Only \ttt{write\_overlap} is activated by default, because it does not enlarge the data file by much, and contains important information which is read by \ttt{data\_extractor.x}.
\ttt{write\_grad} and \ttt{write\_nacdr} are turned off by default; they are primarily intended for users who want to keep all quantum chemical data, e.g., for training in machine learning.
The keywords \ttt{write\_property1d} and \ttt{write\_property2d} are automatically activated if \ttt{theodore} or \ttt{ionization} (respectively) are activated.




\subsection{Example}

The following input sample shows a typical input for excited-state dynamics including IC within a singlet manifold plus intersystem crossing to triplet states. It includes a large number of excited singlet states in order to calculate transient absorption spectra. Only the lowest three singlet states actually participate in the dynamics. 

\begin{example}
  \begin{verbatim}
nstates   8 0 3       # many singlet states for transient absorption
actstates 3 0 3       # only few states to reduce gradient costs

stepsize 0.5          # typical time step for a molecule containing H
tmax 1000.0           # one picosecond

surf diagonal
state 3 mch                  # start on the S2 singlet state
coeff auto                   # coefficient of S2 will be set to one
coupling overlap             # \
decoherence_scheme edc       # | typical settings
ekincorrect parallel_vel     # |
gradcorrect                  # /
grad_select           # \
nac_select            # | improve performance
eselect 0.3           # /

veloc external        # velocities come from file "veloc"
velocfile "veloc"     #

RNGseed 65435
ezero -399.41494751   # ground state energy of molecule
\end{verbatim}
\end{example}



\section{Geometry file}\label{sec:geomfile}

The geometry file (default file name is \ttt{geom}) contains the initial coordinates of all atoms. This file must be present when starting a new trajectory.

The format is based on the \link{http://www.univie.ac.at/columbus/docs_COL70/documentation_main.html}{\textsc{Columbus} geometry file format} (however, \sharc\ is more flexible with the formatting of the numbers). For each atom, the file contains one line, giving the chemical symbol (a string), the atomic number (a real number), the $x$, $y$ and $z$ coordinates of the atom in Bohrs (three real numbers), and the relative atomic weight of the atom (a real number). The six items must be separated by spaces. The real numbers are read in using Fortran list-directed I/O, and hence are free format (can have any numbers of decimals, exponential notation, etc.). Element symbols can have at most 2 characters.

The following is an example of a \ttt{geom} file for \ce{CH2}:
\begin{example}
  \begin{verbatim}
C 6.0  0.0 0.0  0.0 12.000
H 1.0  1.7 0.0 -1.2  1.008
H 1.0  1.7 0.0  3.7  1.008
\end{verbatim}
\end{example}

\section{Velocity file}\label{sec:velocfile}

The velocity file (default \ttt{veloc}) contains the initial nuclear velocities (e.g., from a Wigner distribution sampling). This file is optional (the velocities can be initialized with the \ttt{veloc} input keyword). 

The file contains one line of input for each atom, where the order of atoms must be the same as in the \ttt{geom} file. Each line consists of three items, separated by spaces, where the first is the $x$ component of the nuclear velocity, followed by the $y$ and $z$ components (three real numbers). The input is interpreted in atomic units (Bohr/atu).

The following is an example of a \ttt{veloc} file:
\begin{example}
\begin{verbatim}
 0.0001  0.0000  0.0002
 0.0002  0.0000  0.0012
 0.0003  0.0000 -0.0007
\end{verbatim}
\end{example}

\section{Coefficient file}\label{sec:coefffile}

The coefficient file contains the initial wave function coefficients. The file contains one line per state (total number of states, i.e., multiplets count multiple times). Each line specifies the initial coefficient of one state. If the initial state is specified in the MCH representation (input keyword \ttt{state}), then the order of the initial coefficients must be as given by the canonical ordering (see section~\ref{met:ordering}). If the initial state is given in diagonal representation, then the initial coefficients correspond to the states given in energetic ordering, starting with the lowest state.
Each line contains two real numbers, giving first the real and then the imaginary part of the initial coefficient of the respective state.
Note that after read-in, the coefficient vector is normalized to one.

Example:
\begin{example}
  \begin{verbatim}
0.0 0.0
1.0 0.0
0.0 0.0
\end{verbatim}
\end{example}

\section{Laser file}\label{sec:laserfile}

The laser file contains a table with the amplitude of the laser field $\boldsymbol{\epsilon}(t)$ at each time step of the \textit{electronic} propagation. Given a laser field of the general form:
\begin{equation}
  \boldsymbol{\epsilon}(t)=
  \begin{pmatrix}
    \Re(\epsilon_x(t))+\I \Im(\epsilon_x(t))\\
    \Re(\epsilon_y(t))+\I \Im(\epsilon_y(t))\\
    \Re(\epsilon_z(t))+\I \Im(\epsilon_z(t))
  \end{pmatrix}
\end{equation}
each line consists of 8 elements: $t$ (in fs), $\Re(\epsilon_x(t))$, $\Im(\epsilon_x(t))$, $\Re(\epsilon_y(t))$, $\Im(\epsilon_y(t))$, $\Re(\epsilon_z(t))$, $\Im(\epsilon_z(t))$, (all in atomic units), and finally the instantaneous central frequency (also atomic units).

The time step in the laser file must exactly match the time step used for the electronic propagation, which is the time step used for the nuclear propagation (keyword \ttt{stepsize}) divided by the number of substeps (keyword \ttt{nsubsteps}). The first line of the laser file must correspond to $t$=0 fs.

Example:
\begin{example}
\begin{verbatim}
0.00E+00 -0.68E-03  0.00E+00  0.00E+00  0.00E+00  0.00E+00  0.00E+00  0.31E+00
0.10E-02 -0.77E-02  0.00E+00  0.00E+00  0.00E+00  0.00E+00  0.00E+00  0.33E+00
0.20E-02 -0.13E-01  0.00E+00  0.00E+00  0.00E+00  0.00E+00  0.00E+00  0.35E+00
     ...       ...       ...       ...       ...       ...       ...       ...
\end{verbatim}
\end{example}

\section{Atom mask file}\label{sec:atommaskfile}

The atom mask file contains for each atom a line with a Boolean entry ("T" or "F"), which indicates whether the atom should be considered in the relevant procedures. Specifically, the atom masking settings affect the options \ttt{ekincorrect parallel\_vel}, \ttt{reflect\_frustrated parallel\_vel}, and \ttt{decoherence\_scheme edc}.
In all cases, "T" indicates that the atom should be considered (as if \ttt{atommask} was not given), whereas "F" indicates that the atom should be ignored for these procedures.

Example:
\begin{example}
\begin{verbatim}
 T
 T
 F
 F
...
\end{verbatim}
\end{example}


% ========================================================================================================= %
% ========================================================================================================= %
% ========================================================================================================= %

\chapter{Output files}\label{chap:output}

This chapter documents the content of the output files of \sharc. Those output files are \ttt{output.log}, \ttt{output.lis}, \ttt{output.dat} and \ttt{output.xyz}.

\section{Log file}\label{sec:logfile}

The log file \ttt{output.log} contains general information about all steps of the \sharc\ simulation, e.g., about the parsing of the input files, results of quantum chemistry calls, internal matrices and vectors, etc. The content of the log file can be controlled with the keyword \ttt{printlevel} in the \sharc\ main input file.

In the following, all printlevels are explained.

\paragraph{Printlevel 0}

At printlevel 0, only the execution infos (date, host and working directory at execution start) and build infos (compiler, compile date, building host and working directory) are given.

\paragraph{Printlevel 1}

At printlevel 1, also the content of the input file (cleaned of comments and blank lines) is echoed in the log file. Also, the start of each time step is given.

\paragraph{Printlevel 2}

At printlevel 2, the log file also contains information about the parsing of the input files (echoing all enabled options, initial geometry, velocity and coefficients, etc.) and about the initialization of the coefficients after the first quantum chemistry calculation. This printlevel is recommended for production calculations, since it is the highest printlevel where no output per time step is written to the log file.

\paragraph{Printlevel 3}

This and higher printlevels add output per time step to the log file. At printlevel 3, the log file contains at each time step the data from the velocity-Verlet algorithm (old and new acceleration, velocity and geometry), the old and new coefficients, the surface hopping probabilities and random number, the occupancies before and after decoherence correction as well as the kinetic, potential and total energies.

\paragraph{Printlevel 4}

At printlevel 4, additionally the log file contains information on the quantum chemistry calls (file names, which quantities were read, gradient and nonadiabatic coupling vector selection) and the propagator matrix.

\paragraph{Printlevel 5}

At printlevel 5, additionally the log file contains the results of each quantum chemistry calls (all matrices and vectors), all matrices involved in the propagation as well as the matrices involved in the gradient transformation. This is the highest printlevel currently implemented.

\section{Listing file}\label{sec:lisfile}

The listing file \ttt{output.lis} is a tabular summary of the progress of the dynamics simulation. At the end of each time step (including the initial time step), one line with 11 elements is printed. These are, from left to right:
\begin{enumerate}
  \item current step (counting starts at zero for the initial step),
  \item current simulation time (fs),
  \item current state in the diagonal representation,
  \item approximate corresponding MCH state (see subsection~\ref{ssec:state_transform}),
  \item kinetic energy (eV),
  \item potential energy (eV),
  \item total energy (eV),
  \item current gradient norm (in eV/\AA),
  \item current expectation values of the state dipole moment (Debye),
  \item current expectation values of total spin,
  \item wallclock time needed for the time step.
\end{enumerate}
The listing file also contains one extra line for each surface hopping event. For accepted hops, the old and new states (in diagonal representation) and the random number are given. Frustrated hops and resonant hops are also mentioned. Note that the extra line for surface hopping occurs before the regular line for the time step. 

The listing file can be plotted with standard tools like \textsc{Gnuplot} and can be read with \ttt{data\_collector.py}. 

\paragraph{Energies}

The kinetic energy is calculated at the end of each time step (i.e., after surface hopping events and the corresponding adjustments). The potential energy is the energy of the currently active diagonal state. The total energy is the sum of those two.

\paragraph{Expectation values}

The gradient norms given in the listing file is calculated as follows:
\begin{equation}
  g_\text{list}=\sqrt{\frac{1}{3N_\text{atom}}\sum\limits_a^{N_\text{atom}}\sum_{d=x,y,z} g_{ad}^2}
\end{equation}
which is then transformed to eV/\AA.

The expectation values of the dipole moment for the active state $\beta$ is calculated from:
\begin{equation}
  \mu=\sqrt{\sum\limits_{p=x,y,z} 
  \left(
    \sum\limits_\sigma\sum\limits_\tau
    \Re\left[
      U_{\beta\sigma}^\dagger \mu_{\sigma\tau}^p U_{\tau\beta}
    \right]
  \right)^2}
\end{equation}

The expectation value of the total spin of the active state $\beta$ is calculated as follows:
\begin{equation}
  S=\sum_\alpha |U_{\alpha\beta}|^2 S_\alpha
\end{equation}
where $S_\alpha$ is the total spin of the MCH state with index $\alpha$.

\section{Data file}\label{sec:datfile}

The data file \ttt{output.dat} contains all relevant data from the simulation for all time steps, in ASCII format. Accordingly, this file can become quite large for long trajectories or if many states are included, but for most file systems it is easier to deal with a single large file than with many small files.

Usually, after the simulation is finished the data file is processed by \ttt{data\_extractor.x} to obtain a number of tabular files which can be plotted or post-processed (e.g., with \ttt{data\_collector.py}). For this, see sections~\ref{sec:diagonalizer.x} for the data extractor, \ref{sec:make_gnuscript.py} for plotting, and \ref{sec:data_collector.py} for post-processing.

\subsection{Specification of the data file}

The data file format was changed from the first release version of \sharc. The new format uses a different header, which is keyword-based (like the \ttt{input} file) and starts with \ttt{SHARC\_version 2.0}. The general structure of the time step data is the same as in the first release version.

The data file contains a short header followed by the data per time step. All quantities are commented in the data file.

The header is keyword-based and contains at least the following entries:
\begin{enumerate}
  \item number of states per multiplicity,
  \item number of atoms,
  \item number of 1D properties,
  \item number of 2D properties,
  \item time step,
  \item \ttt{write\_overlap},
  \item \ttt{write\_grad},
  \item \ttt{write\_nacdr},
  \item \ttt{write\_property1d},
  \item \ttt{write\_property2d},
  \item information whether a laser field is included.
%   \item 
%   \item 
%   \item energy shift,
%   \item flag (overlap matrices),
%   \item flag (laser field),
%   \item number of steps,
%   \item number of substeps,
%   \item full laser field for all substeps (only if flag is set).
\end{enumerate}

At the end of the header, the data file contains a header array section. Currently, this includes:
\begin{enumerate}
  \item atomic numbers,
  \item elements,
  \item masses,
  \item full laser field for all substeps (only if flag is set).
\end{enumerate}

The entry for each time step contains:
\begin{enumerate}
  \item step index
  \item Hamiltonian in MCH representation,
  \item transformation matrix $\VEC{U}$,
  \item MCH dipole moment matrices ($x$, $y$, $z$),
  \item overlap matrix in MCH representation (only if flag is set),
  \item coefficients in the diagonal representation,
  \item hopping probablities in the diagonal representation
  \item kinetic energy,
  \item currently active state in diagonal representation and approximate state in MCH representation,
  \item random number for surface hopping,
  \item wallclock time (in seconds)
  \item geometry (Bohrs),
  \item velocities (atomic units),
  \item 2D property matrices (only if flag is set),
  \item 1D property vectors (only if flag is set),
  \item gradient vectors (only if flag is set),
  \item nonadiabatic coupling vectors (only if flag is set).
\end{enumerate}

\section{XYZ file}\label{sec:xyzfile}

The file \ttt{output.xyz} contains the geometries of all time steps in standard xyz file format. It can be used with visualization programs like \textsc{Molden}, \textsc{Gabedit} or \textsc{Molekel} to create movies of the molecular motion, or with \ttt{geo.py} (see~\ref{sec:geo.py}) to calculate internal coordinates for each time step. Furthermore, \ttt{trajana\_nma.py} (see~\ref{sec:trajana_nma.py}) and \ttt{trajana\_essdyn.py} (see~\ref{sec:trajana_essdyn.py}) read this file.

The comments of the geometries (given in the second line of each geometry block) contain information about the simulation time and the active state (first in diagonal basis, then in MCH basis).


% ========================================================================================================= %
% ========================================================================================================= %
% ========================================================================================================= %

\chapter{Interfaces}\label{chap:interfaces}

This chapter describes the interface between \sharc\ and quantum chemistry programs. In the first section, the interface is specified (e.g., for users who attempt to create their own interfaces). The description of the currently existing interfaces takes the remainder of this chapter.

% ========================================================================================================= %

\section{Interface Specifications}

From the \sharc\ point of view, quantum chemical calculation proceeds as follows in the \ttt{QM} directory:
\begin{enumerate}
  \item write a file called \ttt{QM/QM.in}
  \item call a script called \ttt{QM/runQM.sh}
  \item read the output from a file called \ttt{QM/QM.out}
\end{enumerate}
For specifications of the formats of these two files (\ttt{QM.in} and \ttt{QM.out}) see below. The executable script \ttt{QM/runQM.sh} must accomplish that all necessary quantum chemical output is available in \ttt{QM/QM.out}.

\begin{figure}[htb]
  \centering
  \includegraphics[scale=1]{img/interfaces/general.pdf}
  \caption{Communication between \ttt{sharc.x}, the interfaces, and the quantum chemistry codes.}
  \label{fig:interface_general}
\end{figure}

\subsection{\ttt{QM.in} Specification}\label{intf:qmin}

The \ttt{QM.in} file is written by \sharc\ every time a quantum chemistry calculation is necessary. It contains all information available to \sharc. This information includes the current geometry (and velocity), the time step, the number of states, the time step and the unit used to specify the atomic coordinates. The file also contains \textit{control} keywords and \textit{request} keywords. 

The file format is consistent with a standard xyz file. The first line contains the number of atoms, the second line is a comment. \sharc\ writes the trajectory ID (a hash of all \sharc\ input files) to this line. The following lines specify the atom positions. As a fourth, fifth and sixth column, these lines may contain the atomic velocities.
All following lines contain keywords, one per line and possibly with arguments. Comments can be inserted with '\#', and empty lines are permitted. Comments and empty lines are only permitted below the xyz file part.
An examplary \ttt{QM.in} file is given in the following:
\begin{example}
  \begin{verbatim}
3
Jobname
S     0.0     0.0     0.0    0.000  -0.020   0.002
H     0.0     0.9     1.2    0.000  -0.030   0.000
H     0.0    -0.9     1.2    0.000   0.010  -0.000
# This is a comment
Init
States 3 0 2
Unit Angstrom
SOC
DM
GRAD 1 2
OVERLAP
NACDR select
  1 2
  end
\end{verbatim}
\end{example}

There exist two types of keywords, \textit{control} keywords and \textit{request} keywords. Control keywords pass some information to the interface. Request keywords tell the interface to provide a quantity in the \ttt{QM.out} file. Table~\ref{tab:int_ctrl} contains all control keywords while table~\ref{tab:int_req} lists all request keywords.



\begin{table}
  \centering
  \caption[Control keywords for \sharc\ interfaces.]{Control keywords for \sharc\ interfaces. These keywords pass information from \sharc\ to the interface.}
  \label{tab:int_ctrl}
  \begin{tabular}{>{\ttfamily}lp{13cm}}
  \hline
  Keyword       &Description\\
  \hline
  init            &Specifies that this is the first calculation. The interface should create a save directory (if not existing already) to save all information necessary for a restart. \\
  samestep        &Specifies that this is an additional calculation at the same geometry/time step. Implies that, e.g., the converged orbitals from the savedir could be reused or that the wave function calculations could be skipped.\\
  restart         &Specifies that this is a calculation after a restart of \ttt{sharc.x}, possibly after a crash. Implies that in the savedir some files might be incomplete, the interface should therefore act as if a new step is requested, except that the savedir files should be managed accordingly.\\
  cleanup         &Specifies that all output files of the interface (except \ttt{QM.out}) should be deleted (including the save directory).\\
  backup          &Specifies that the content of the save directory should be stored in a persistent location (such that it is not overwritten in the next time step).\\
  unit            &Specifies in which unit the atomic coordinates are to be interpreted. Possible arguments are ``angstrom'' and ``bohr''.\\
  states          &Gives the number of excited states per multiplicity (singlets, doublet, triplets, ...).\\
  dt              &Gives the time between the last calculation and the current calculation in atomic units.\\
  savedir         &Gives a path to the directory where the interface should save files needed for restart and between time steps. If the interface-specific input files also have this keyword, \sharc\ assumes that the path in \ttt{QM.in} takes precedence.\\
  \hline
  \end{tabular}
\end{table}





\begin{table}
  \centering
  \caption[Request keywords for \sharc\ interfaces.]{Request keywords for \sharc\ interfaces. See Table~\ref{tab:interfaces} for which interfaces can fulfill these requests.}
  \label{tab:int_req}
  \begin{tabular}{>{\ttfamily}lp{13cm}}
  \hline
  Keyword       &Description\\
  \hline
  H               &Calculate the molecular Hamiltonian (diagonal matrix with the energies of the states of the model space). This request is always available.\\
  SOC             &Calculate the molecular Hamiltonian including the SOCs (not diagonal anymore within the model space).\\
  DM              &Calculate the state dipole moments and transition dipole moments between all states.\\
  GRAD            &Calculate gradients for all states. If followed by a list of states, calculate only gradients for the specified states.\\
  NACDT           &Calculate the time-derivatives $\left\langle\Psi_1|\partial/\partial t|\Psi_2\right\rangle$ by finite differences between the last time step and the current time step. This request is currently not supported by any interface.\\
  NACDR           &Calculate nonadiabatic coupling vectors $\left\langle\Psi_1|\partial/\partial \mathbf{R}|\Psi_2\right\rangle$ between all pairs of states. If followed by ``select'', read the list of pairs on the following lines until ``end'' and calculate nonadiabatic coupling vectors between the specified pairs of states.\\
  OVERLAP         &Calculate overlaps $\left\langle\Psi_1(t_0)|\Psi_2(t)\right\rangle$ between all pairs of states (between the last and current time step). If followed by ``select'', read the list of pairs on the following lines until ``end'' and calculate overlaps between the specified pairs of states.\\
  DMDR            &Calculate the Cartesian gradients of the dipole moments and transition dipole moments of all states.\\
%   SOCDR           &Calculate the Cartesian gradients of the full spin-orbit Hamiltonian.\\
  ION             &Calculate transition properties between neutral and ionic wave functions.\\

  THEODORE        &Run \textsc{TheoDORE} to compute electronic descriptors for all states.\\
  MOLDEN          &Generate \textsc{Molden} files of the relevant orbitals and copy them to the savedir.\\
  \hline
  \end{tabular}
\end{table}

\subsection{\ttt{QM.out} Specification}\label{intf:qmout}

The \ttt{QM.out} file communicates back the results of the quantum chemistry calculation to the dynamics code. After \sharc\ called \ttt{QM/runQM.sh}, it expects that the file \ttt{QM/QM.out} exists and contains the relevant data.

The following quantities are expected in the file (depending whether the corresponding keyword is in the \ttt{QM.in} file): Hamiltonian matrix, dipole matrices, gradients, nonadiabatic couplings (either NACDR or NACDT), overlaps, wave function phases, property matrices. The format of \ttt{QM.out} is described in the following. 

Each quantity is given as a data block, which has a fixed format. The order of the blocks is arbitrary, and between blocks arbitrary lines can be written. However, within a block no extraneous lines are allowed. Each data block starts with a exclamation mark \ttt{!}, followed by whitespace and an integer flag which specifies the type of data:

\begin{tabular}{ll}
1       &Hamiltonian matrix\\
2       &Dipole matrices\\
3       &Gradients\\
4       &Non-adiabatic couplings (NACDT)\\
5       &Non-adiabatic couplings (NACDR)\\
6       &Overlap matrix\\
7       &Wavefunction phases\\
8       &Wallclock time for QM calculation\\
11      &Property matrix (e.g., ionization probabilities) \quad \textbf{this flag is deprecated}\\
12      &Dipole moment gradients\\
% 13      &Spin-orbit matrix gradients\\
20      &Property matrices with number and labels (e.g., ionization probabilities)\\
21      &Property vectors with number and labels (e.g., \textsc{TheoDORE} output)
\end{tabular}

On the next line, two integers are expected giving the dimensions of the following matrix. Note, that all these matrices must be square matrices. On the following lines, the matrix or vector follows. Matrices are in general complex, and real and imaginary part of each element is given as a pair of floating point numbers.

The following shows an example of a $4\times 4$ Hamiltonian matrix. Note that the imaginary parts directly follow the real parts (in this example, the Hamiltonian is real).
\begin{example}
  \begin{verbatim}
  ! 1
  4 4
 -548.6488 0.0000    0.0000 0.0000    0.0003 0.0000    0.0003 0.0000
    0.0000 0.0000 -548.6170 0.0000    0.0003 0.0000    0.0003 0.0000
    0.0003 0.0000    0.0003 0.0000 -548.5986 0.0000    0.0000 0.0000
    0.0003 0.0000    0.0003 0.0000    0.0000 0.0000 -548.5912 0.0000
\end{verbatim}
\end{example}

The three dipole moment matrices ($x$, $y$ and $z$ polarization) must follow directly after each other, where the dimension specifier must be present for each matrix. The dipole matrices are also expected to be complex-valued.
\begin{example}
  \begin{verbatim}
  ! 2
  2 2
  0.1320 0.0000 -0.0020 0.0000
 -0.0020 0.0000 -1.1412 0.0000
  2 2
  0.0000 0.0000  0.0000 0.0000
  0.0000 0.0000  0.0000 0.0000
  2 2
  2.1828 0.0000  0.0000 0.0000
  0.0000 0.0000  0.6422 0.0000
\end{verbatim}
\end{example}

Gradient and nonadiabatic couplings vectors are written as $3\times n_\text{atom}$ matrices, with the $x$, $y$ and $z$ components of one atom per line. These vectors are expected to be real valued. Each vector is preceeded by its dimensions.
\begin{example}
  \begin{verbatim}
6 3 
 0.0000 -6.5429 -8.1187
 0.0000  5.8586  8.0160
 0.0000  6.8428  1.0265
 0.0000  6.5429  8.1187
 0.0000 -5.8586 -8.0160
 0.0000 -6.8428 -1.0265
\end{verbatim}
\end{example}
If gradients are requested, \sharc\ expects every gradient to be present, even if only some gradients are requested. The gradients are expected in the canonical ordering (see section~\ref{met:ordering}), which implies that for higher multiplets the same gradient has to be present several times. For example, with 3 singlets and 3 triplets, \sharc\ expects 12 gradients in the \ttt{QM.out} file (each triplet has three components with $M_s=$ -1, 0 or 1).

Similarly, for nonadiabatic coupling vectors, \sharc\ expects all pairs, even between states of different multiplicity. The vectors are also in canonical ordering, where the inner loop goes over the ket states. For example, with 3 singlets and 3 triplets (12 states), \sharc\ expects 144 ($12^2$) nonadiabatic coupling vectors in the \ttt{QM.out} file.
\begin{example}
  \begin{verbatim}
! 5 Non-adiabatic couplings (ddr) (2x2x1x3, real)
1 3 ! m1 1 s1 1 ms1 0   m2 1 s2 1 ms2 0
 0.0e+0  0.0e+0  0.0e+0 
1 3 ! m1 1 s1 1 ms1 0   m2 1 s2 2 ms2 0
+2.0e+0  0.0e+0  0.0e+0 
1 3 ! m1 1 s1 2 ms1 0   m2 1 s2 1 ms2 0
-2.0e+0  0.0e+0  0.0e+0 
1 3 ! m1 1 s1 2 ms1 0   m2 1 s2 2 ms2 0
 0.0e+0  0.0e+0  0.0e+0 
\end{verbatim}
\end{example}

The nonadiabatic coupling matrix (NACDT keyword), the overlap matrix and the property matrix are single $n\times n$ matrices ($n$ is the total number of states), respectively, like the Hamiltonian. 

The wave function phases are a vector of complex numbers.

The wallclock time is a single real number. 

The dipole moment gradients are a list of $3\times n_\text{atom}$ vectors, each specifying the gradient of one polarization of one dipole moment matrix element. In the outmost loop, the bra index is counted, then the ket index, then the polarization. Hence, the respective entry in \ttt{QM.out} would look like (for 2 states and 1 atom):
\begin{example}
  \begin{verbatim}
! 12 Dipole moment derivatives (2x2x3x1x3, real)
1 3 ! m1 1 s1 1 ms1 0   m2 1 s2 1 ms2 0   pol 0
 0.000000000000E+00  0.000000000000E+00  0.000000000000E+00 
1 3 ! m1 1 s1 1 ms10   m2 1 s2 1 ms2 0  pol 1
 0.000000000000E+00  0.000000000000E+00  0.000000000000E+00 
1 3 ! m1 1 s1 1 ms10   m2 1 s2 1 ms2 0  pol 2
 0.000000000000E+00  0.000000000000E+00  0.000000000000E+00 
1 3 ! m1 1 s1 1 ms10   m2 1 s2 2 ms2 0  pol 0
 1.000000000000E+00  0.000000000000E+00  0.000000000000E+00 
1 3 ! m1 1 s1 1 ms10   m2 1 s2 2 ms2 0  pol 1
 0.000000000000E+00  0.000000000000E+00  0.000000000000E+00 
1 3 ! m1 1 s1 1 ms10   m2 1 s2 2 ms2 0  pol 2
 0.000000000000E+00  0.000000000000E+00  0.000000000000E+00 
1 3 ! m1 1 s1 2 ms10   m2 1 s2 1 ms2 0  pol 0
 1.000000000000E+00  0.000000000000E+00  0.000000000000E+00 
1 3 ! m1 1 s1 2 ms10   m2 1 s2 1 ms2 0  pol 1
 0.000000000000E+00  0.000000000000E+00  0.000000000000E+00 
1 3 ! m1 1 s1 2 ms10   m2 1 s2 1 ms2 0  pol 2
 0.000000000000E+00  0.000000000000E+00  0.000000000000E+00 
1 3 ! m1 1 s1 2 ms10   m2 1 s2 2 ms2 0  pol 0
 0.000000000000E+00  0.000000000000E+00  0.000000000000E+00 
1 3 ! m1 1 s1 2 ms10   m2 1 s2 2 ms2 0  pol 1
 0.000000000000E+00  0.000000000000E+00  0.000000000000E+00 
1 3 ! m1 1 s1 2 ms10   m2 1 s2 2 ms2 0  pol 2
 0.000000000000E+00  0.000000000000E+00  0.000000000000E+00 
\end{verbatim}
\end{example}

The section containing the 2D property matrices consists of three subsequent parts: (i) the number of property matrices contained, (ii) a label for each property matrix (as the property matrices might contain arbitrary data, depending on the interface and the requests), and (iii) the matrices (full, complex-valued matrices like above):
\begin{example}
  \begin{verbatim}
! 20 Property Matrices
2    ! number of property matrices
! Property Matrix Labels (1 strings)
Dyson norms
Example matrix
! Property Matrices (1x4x4, complex)
4 4   ! Dyson norms
 0.000E+00  0.000E+00  0.000E+00  0.000E+00  9.663E-01  0.000E+00  9.663E-01  0.000E+00 
 0.000E+00  0.000E+00  0.000E+00  0.000E+00  4.822E-01  0.000E+00  4.822E-01  0.000E+00 
 9.663E-01  0.000E+00  4.822E-01  0.000E+00  0.000E+00  0.000E+00  0.000E+00  0.000E+00 
 9.663E-01  0.000E+00  4.822E-01  0.000E+00  0.000E+00  0.000E+00  0.000E+00  0.000E+00 
4 4   ! Example matrix
 1.000E+00  1.000E+00  0.000E+00  0.000E+00  0.000E+00  0.000E+00  0.000E+00  0.000E+00 
 0.000E+00  0.000E+00  2.000E+00  2.000E+00  0.000E+00  0.000E+00  0.000E+00  0.000E+00 
 0.000E+00  0.000E+00  0.000E+00  0.000E+00  3.000E+00  3.000E+00  0.000E+00  0.000E+00 
 0.000E+00  0.000E+00  0.000E+00  0.000E+00  0.000E+00  0.000E+00  4.000E+00  4.000E+00 
\end{verbatim}
\end{example}

The section containing the 1D property vectors also consists of three subsequent parts: (i) the number of property vectors contained, (ii) a label for each property vector (as the property vectors might contain arbitrary data, depending on the interface and the requests), and (iii) the vectors (real-valued):
\begin{example}
  \begin{verbatim}
! 21 Property Vectors
2    ! number of property vectors
! Property Vector Labels (2 strings)
Om
PRNTO
! Property Vectors (9x8, real)
! TheoDORE descriptor 1 (Om)
 0.000000000000E+000
 4.318700000000E-001
 2.688600000000E-001
 2.590000000000E-002
! TheoDORE descriptor 2 (PRNTO)
 0.000000000000E+000
 2.318700000000E-001
 1.688600000000E-001
 1.590000000000E-002
\end{verbatim}
\end{example}



\subsection{Further Specifications}

The interfaces may require additional input files beyond \ttt{QM.in}, which contain static information. This may include paths to the quantum chemistry executable, paths to scratch directories, or input templates for the quantum chemistry calculation (e.g.\ active space specifications, basis sets, etc.).
The dynamics code does not depend on these additional files, but they should all be stored in the \ttt{QM/} subdirectory.

The current conventions in the \sharc\ suite are that the quantum chemistry interfaces use two additional input files, one specifying the level of theory (template file, e.g., \ttt{MOLCAS.template}, \ttt{MOLPRO.template}, ...) and one specifying the computational resources like paths, memory, number of CPU cores, initial orbital source (resource file, e.g., \ttt{MOLCAS.resources}, \ttt{MOLPRO.resources}, ...).
Furthermore, the current interfaces allow to read in initial orbitals (e.g., \ttt{MOLCAS.*.RasOrb.init}, \ttt{mocoef.init}, ...).
For interfaces with QM/MM capabilities, additional files could be used to specify connection table, parameters, etc.



\subsection{Save Directory Specification}

The interfaces must be able to save all information necessary for restart to a given directory. The absolute path is written to \ttt{QM.in} by \sharc. Hence, for the trajectories the path to the save directory is always a subdirectory of the working directory of \sharc.



% ========================================================================================================= %
\clearpage
\section{Overview over Interfaces}\label{sec:int:overview}

The \sharc\ suite comes with a number of interfaces to different quantum chemistry programs.
Their capabilities and usage are explained in the following sections.
Table~\ref{tab:interfaces} gives an overview over the capabilities of the interfaces.
Table~\ref{tab:interface_files} shows the file names for interface-related input files of the different interfaces.



\begin{table}[htb]
  \newcommand{\chk}{\ensuremath{\surd}}
  \renewcommand{\tabcolsep}{4.5pt}
  \centering
  \caption[Overview over capabilities of \sharc\ interfaces.]{Overview over capabilities of \sharc\ interfaces. For each method and program, the table shows which multiplicities ($S^2$), which quantities, and whether QM/MM are available.}
  \label{tab:interfaces}
  \begin{tabular}{ll cccccccccc}
    \hline
    Method      &Program              &$S^2$   &SOC         &TDM$^a$     &Grad.       &NACDR       &OVL$^b$ &DMDR     &ION  &Theo.$^c$   &QM/MM\\
    \hline
    SA-CASSCF   &\textsc{Molpro}      &any     &\chk        &\chk        &\chk        &\chk        &\chk    &         &\chk &            &     \\
                &\textsc{Molcas}      &any     &\chk        &\chk        &\chk        &            &\chk    &\chk$^d$ &\chk &            &\chk \\
                &\textsc{Columbus}    &any     &\chk$^e$    &\chk        &\chk        &\chk$^e$    &\chk    &         &\chk &            &     \\
    MR-CISD     &\textsc{Columbus}    &any     &\chk$^e$    &\chk        &\chk        &\chk$^e$    &\chk    &         &\chk &            &     \\
    MS-CASPT2   &\textsc{Molcas}      &any     &\chk        &\chk        &\chk$^d$    &            &\chk    &\chk$^d$ &\chk &            &     \\
    TD-DFT      &\textsc{ADF}         &any     &\chk$^f$    &\chk$^g$    &\chk        &            &\chk    &         &\chk &\chk        &\chk \\
                &\textsc{Gaussian}    &any     &            &\chk$^g$    &\chk        &            &\chk    &         &\chk &\chk        &     \\
                &\textsc{Orca}        &any     &\chk$^f$    &\chk$^g$    &\chk        &            &\chk    &         &\chk &\chk        &\chk \\
    ADC2        &\textsc{Turbomole}   &S, T    &\chk$^f$    &\chk        &\chk        &            &\chk    &         &     &\chk        &     \\
    CC2         &\textsc{Turbomole}   &S, T    &            &\chk$^g$    &\chk        &            &\chk    &         &     &\chk        &     \\
    Analytical  &---                  &any     &\chk        &\chk        &\chk        &            &\chk    &\chk     &     &            &     \\
    LVC         &---                  &any     &\chk        &\chk        &\chk        &\chk        &\chk    &         &     &            &     \\
    \hline
  \end{tabular}

  $^a$ TDM: transition dipole moments;
  $^b$ OVL: wave function overlaps;
  $^c$ Theo.: \textsc{TheoDORE};
  $^d$ numerical gradients;
  $^e$ either NAC or SOC, but not both at the same time;
  $^f$ SOCs only between singlets and triplets;
  $^g$ TDMs only between $S_0$ and excited singlets.
\end{table}



\begin{table}[htb]
  \centering
  \caption{Overview over files of \sharc\ interfaces.}
  \renewcommand{\tabcolsep}{5pt}
  \label{tab:interface_files}
  \begin{tabular}{lllll}
    \hline
    Interface           &Template file             &Resource file             &Initial MOs                     &QM/MM\\
    \hline
    \textsc{Molpro}     &\ttt{MOLPRO.template}     &\ttt{MOLPRO.resources}    &\ttt{wf.init}                   &\\
                        &                          &                          &\ttt{wf.<job>.init}             &\\
    \textsc{Molcas}     &\ttt{MOLCAS.template}     &\ttt{MOLCAS.resources}    &\ttt{MOLCAS.<mult>.RasOrb.init} &\ttt{MOLCAS.qmmm.table}\\
                        &                          &                          &\ttt{MOLCAS.<mult>.JobIph.init} &\ttt{MOLCAS.qmmm.key}\\
    \textsc{Columbus}   &a directory               &\ttt{COLUMBUS.resources}  &\ttt{mocoef\_mc.init}           &\\
                        &                          &                          &\ttt{mocoef\_mc.init.<job>}     &\\
                        &                          &                          &\ttt{molcas.RasOrb.init}        &\\
                        &                          &                          &\ttt{molcas.RasOrb.init.<job>}  &\\
    \textsc{ADF}        &\ttt{ADF.template}        &\ttt{ADF.resources}       &\ttt{ADF.t21.<job>.init}        &\ttt{ADF.qmmm.table}\\
                        &                          &                          &                                &\ttt{ADF.qmmm.ff}\\
    \textsc{Gaussian}   &\ttt{GAUSSIAN.template}   &\ttt{GAUSSIAN.resources}  &\ttt{GAUSSIAN.chk.init}         &\\
                        &                          &                          &\ttt{GAUSSIAN.chk.<job>.init}   &\\
    \textsc{Orca}       &\ttt{ORCA.template}       &\ttt{ORCA.resources}      &\ttt{ORCA.gbw.init}             &\\
                        &                          &                          &\ttt{ORCA.gbw.<job>.init}       &\\
    \textsc{Turbomole}  &\ttt{RICC2.template}      &\ttt{RICC2.resources}     &\ttt{mos.init}                  &\\
    Analytical          &\ttt{Analytical.template} &N/A                       &N/A                             &\\
    LVC                 &\ttt{LVC.template}        &N/A                       &N/A                             &\\
    \hline
  \end{tabular}
\end{table}


\subsection{Example Directory}


The directory \ttt{\$SHARC/../examples/} contains for all quantum chemistry interfaces comprehensively commented examples of input files (template, resource files).
These example files should be regarded as supplementary files to the documentation of the interfaces.
For the interfaces without the possibility for automated template generation (e.g., \ttt{SHARC\_RICC2.py}), it is recommended that users copy the example template file and modify it to their needs.

However, note that it might not necessarily work to directly start the respective interface in the example directories.
In order to make the example calculations work,  some paths or variables in the resource files need to be adjusted.
If you need automatically working test calculations for \sharc, consider using \ttt{tests.py} instead.



% \subsection{Additional Programs: \textsc{WFoverlap} and \textsc{TheoDORE}}
% 
% Besides the quantum chemistry packages, many of the \sharc\ interfaces communicate also with additional programs, which carry out common tasks which are not implemented in the quantum chemistry packages.
% Currently, two such programs are \textsc{WFoverlap} and \textsc{TheoDORE}.
% 
% \paragraph{\text{WFoverlap}}
% 
% This program is part of the \sharc\ distribution, but can also be obtained as a \todo{stand-alone package}.
% It computes overlaps between many-electron wave functions expressed in terms of linear combinations of Slater determinant, which are based on molecular orbitals (from an LCAO ansatz).
% 
% \paragraph{\text{TheoDORE}}



% ========================================================================================================= %

% \clearpage
\section{MOLPRO Interface}\label{sec:int:molpro}

The \sharc-\textsc{Molpro} interface allows to run \sharc\ dynamics with \textsc{Molpro}'s CASSCF wave functions. RASSCF is not supported, since on RASSCF level state-averaging over different multiplicities is not possible. The interface uses \textsc{Molpro}'s CI program in order to calculate transition dipole moments and spin-orbit couplings. Gradients and nonadiabatic coupling vectors are calculated using \textsc{Molpro}'s CADPACK code (hence no generally contracted basis sets can be used). In the new version of the interface, overlaps are calculated using the \textsc{WFoverlap} code. 
Time-derivative couplings (NACDT) are not supported anymore in the \sharc-\textsc{Molpro} interface.
Wavefunction phases between the CASSCF and MRCI wave functions are automatically adjusted.
The interface can trivially parallelize the computation of gradients and coupling vectors over several processors.
Execution of parallel \textsc{Molpro} binaries is currently not supported.

The \sharc-\textsc{Molpro} interface needs two additional input files, which should be present in \ttt{QM/}. Those input files are \ttt{MOLPRO.resources}, which contains, e.g., the paths to \textsc{Molpro} and the scratch directory, and \ttt{MOLPRO.template}, which is a keyword-argument input file specifying the CASSCF level of theory. 
If \ttt{QM/wf.init} is present, it will be used as a \textsc{Molpro} wave function file containing the initial MOs.
For calculations with several ``jobs'' (see below), initial orbitals can also given as \ttt{QM/wf.<job>.init}.

\subsection{Template file: \ttt{MOLPRO.template}}

During the rework of the \textsc{Molpro} interface, the template file structure was completely changed.
In the new version, the template file is a keyword-argument list file, similar to the template files of most other interfaces.
A fully commented template file with all possible options is located in \ttt{\$SHARC/../examples/SHARC\_MOLPRO/}.

For simple cases, an example for the template file looks like this:
\begin{example}
  \begin{verbatim}
basis def2-svp
dkho 2                  # Douglas-Kroll second order
occ 14
closed 10
nelec           24
roots           4 0 3
rootpad         1 0 1
\end{verbatim}
\end{example}
This specifies a SA(4S+3T)-CASSCF(4,4)/def2-SVP calculation for 24 electrons.
Note the \ttt{rootpad} keyword, which adds one singlet and one triplet with zero weight to the state-averaging (so technically this is a SA(5S+4T) calculation, but the results are the same as SA(4S+3T)).
These zero-weight states are sometimes useful to improve convergence of CASSCF.

The new interface can also be used to perform several independent CASSCF calculations for different multiplicities (e.g., one CASSCF for the neutral states and another one for the ionic states).
In this case, each independent CASSCF calculation is called a ``job''.
In the template, most settings can be modified independently for each job.
An example is given here:
\begin{example}
  \begin{verbatim}
# In this way, users can employ custom basis sets
basis_external /path/to/basisset        # no spaces in path allowed
dkho 2

# job 1 for singlet+triplet; job 2 for doublets
jobs 1 2 1

occ 14 13       # for job 1 and 2
closed 11 10    # for job 1 and 2

nelec           24 23 24   # for job 1
nelec           24 23 24   # for job 2

roots           4 0 3   # for job 1
roots           0 2 0   # for job 2

rootpad         1 0 1   # for job 1
rootpad         0 2 0   # for job 2
\end{verbatim}
\end{example}
This template specifies two jobs, where job 1 should be used to compute singlet and triplet states, and job 2 used for doublet states.
Job 1 is a SA(4S+3T)-CASSCF(2,3) computation, with singlets and triplets each having 24 electrons.
Job 2 is a SA(2D)-CASSCF(3,3) computation, with 23 electrons.
Note how for different jobs it is possible to have different active spaces and state-averaging schemes.
However, keep in mind that all states of a given multiplicity are always calculated in the same job (e.g., it is not possible to have one job for $S_0$ and another job for $S_1$ and $S_2$).

It is also possible to do a mixed input, for example having two jobs, but only giving one number after \ttt{occ} or \ttt{closed}.
The interface provides comprehensive error messages during the template check.

Also note the \ttt{basis\_external} keyword. It provides a file, whose content is used in the basis set definition (it is inserted verbatim into \ttt{basis=\{...\}} in the \textsc{Molpro} input).
It is possible to use the generated input from the \link{https://bse.pnl.gov/bse/portal}{Basis Set Exchange Library}, but the \ttt{basis=\{} and \ttt{\}} need to be deleted from the file.

Remember that \ttt{molpro\_input.py} cannot create multi-job templates or templates with the \ttt{basis\_external} keyword.

\subsection{Resource file: \ttt{MOLPRO.resources}}

The interface requires some additional information beyond the content of \ttt{QM.in}. This information is given in the file \ttt{MOLPRO.resources}, which must reside in the directory where the interface is started. This file uses a simple ``\ttt{keyword argument}'' syntax. Comments using \# and blank lines are possible, the order of keywords is arbitrary. Lines with unknown keywords are ignored, since the interface just searches the file for certain keywords.

Table~\ref{tab:molpro_sh2} lists the existing keywords.
A fully commented resource file for this interface with all possible options is located in \ttt{\$SHARC/../examples/SHARC\_MOLPRO/}.

\begin{table}[t]
  \centering
  \caption{Keywords for the \ttt{MOLPRO.resources} input file.}
  \label{tab:molpro_sh2}
  \begin{tabular}{>{\ttfamily}lp{12cm}}
  \hline
  Keyword       &Description\\
  \hline
  molpro          &Is followed by a string giving the path to the \textsc{Molpro} executable. Relative and absolute paths, environment variables and \ttt{\textasciitilde} can be used. \\
  scratchdir      &Is a path to the temporary directory. Relative and absolute paths, environment variables and \ttt{\textasciitilde} can be used. If the directory does not exist, the interface will create it. In any case, the interface will delete this directory after the calculation.\\
  savedir         &Is a path to another temporary directory.  Relative and absolute paths, environment variables and \ttt{\textasciitilde} can be used. The interface will store files needed for restart there.\\
  memory        &(int) Memory for \textsc{Molpro} and \textsc{WFoverlap} in MB.\\
  ncpu          &(int) Number of CPU cores for parallel computation of gradients/NAC vectors.\\
  delay         &(float) Time in seconds between starting parallel \textsc{Molpro} runs. Useful to lessen the I/O burden when having many runs starting at the same time.\\
  gradaccudefault &(float) Default accuracy for CP-MCSCF.\\
  gradaccumax     &(float) Worst acceptable accuracy for CP-MCSCF (see below).\\
  always\_orb\_init &Do not use the orbital guesses from previous calculations/time steps, but always use the provided initial orbitals.\\
  always\_guess   &Always use the orbital guess from \textsc{Molpro}'s guess module.\\
  wfoverlap       &Is the path to the \textsc{WFoverlap} executable. Relative and absolute paths, environment variables and \ttt{\textasciitilde} can be used.\\
  numfrozcore           &Number of neglected core orbitals for overlap and Dyson calculations.\\
  numocc           &Number of doubly occupied orbitals for Dyson calculations.\\
  nooverlap       &Do not save determinant files for overlap computations.\\
  debug           &Increases the verbosity of the interface.\\
  no\_print       &Reduces interface standard output.\\
%   checknacs       &(deprecated)\\
%   correctnacs     &(deprecated)\\
%   checknacs\_mrcio &(deprecated)\\
%   checknacs\_ediff &(deprecated)\\
  \hline
  \end{tabular}
\end{table}

Mandatory keywords are the paths to \textsc{Molpro}, the scratch directory, and to the \textsc{WFoverlap} executable (the latter for overlap, Dyson, or NACDR calculations).

\paragraph{Parallel execution of MOLPRO}

In the new version of the \sharc-\textsc{Molpro} interface, calculations of multiple independent active spaces (``jobs''), of several gradients, and of several nonadiabatic coupling vectors are automatically parallelized over the given number of CPU cores.

Note that in the new version of the interface, \textsc{Molpro} calls itself cannot be run with multiple CPUs.


\subsection{Error checking}

The interface is written such that the output of \textsc{Molpro} is checked for commonly occuring errors, mostly bad convergence in the MCSCF or CP-MCSCF parts. In these cases, the input is adjusted and \textsc{Molpro} restarted. This will be done until all calculations are finished or an unrecoverable error is detected.
The interface will try to solve the following error messages:

\paragraph{EXCESSIVE GRADIENT IN CI} This error message can occur in the MCSCF part. The calculation is restarted with a P-space threshold (see \textsc{Molpro} manual) of 1. If the error remains, the threshold is quadrupled until the calculation converges or the threshold is above 100.

\paragraph{NO CONVERGENCE IN REFERENCE CI} The error occurs in the CI part. The calculation is restarted with a P-space threshold (see \textsc{Molpro} manual) of 1. If the error remains, the threshold is quadrupled until the calculation converges or the threshold is above 100.

\paragraph{NO CONVERGENCE OF CP-MCSCF} This error occurs when solving the linear equations needed for the calculation of MCSCF gradients or nonadiabatic coupling vectors. In this case, the interface finds in the output the value of closest convergence and restarts the calculation with the value found as the new convergence criterion. This ensures that the CP-MCSCF calculation converges, albeit with lower accuracy for this gradient for this time step.

This error check is controlled by two keywords in the \ttt{MOLPRO.resources} file. The interface first tries to converge the CP-MCSCF calculation to \ttt{gradaccudefault}. If this fails, it tries to converge to the best value possible within 900 iterations. \ttt{gradaccumax} defines the worst accuracy accepted by the interface. If a CP-MCSCF calculation cannot be converged below \ttt{gradaccumax} then the interface exits with an error, leading to the abortion of the trajectory.

\subsection{Things to keep in mind}

\paragraph{Initial orbital guess}

For CASSCF calculations it is always a good idea to start from converged MOs from a nearby geometry. For the first time step, if a file \ttt{QM/wf.init} is present, the \sharc-\textsc{Molpro} interface will take this file for the starting orbitals. In case of multi-job calculations, separate initial orbitals can be provided with files called \ttt{QM/wf.<job>.init}, where \ttt{<job>} is an integer.
In subsequent calculations, the MOs from the previous step will be used (unless the \ttt{always\_orb\_init} keyword is used).

\paragraph{Basis sets}

Note that the \textsc{Molpro} interface cannot calculation SA-CASSCF gradients for generally contracted basis sets (like Dunning's ``cc'' basis sets or Roos' ``ANO'' basis sets). Only segmented basis sets are allowed, like the Pople basis sets and the ``def'' family from \textsc{Turbomole}.

In order to employ user-defined basis sets, in the template the keyword \ttt{basis\_external}, followed by a filename, can be used.
The interface will then take the content of this file and insert it as basis set definition in the \textsc{Molpro} input files (i.e., it will add in the input \ttt{basis=\{ <content of the file> \}}.
Note that the filename must not contain spaces.
















\subsection{Molpro input generator: \ttt{molpro\_input.py}}\label{sec:molpro_input.py}

In order to quickly setup simple inputs for \textsc{Molpro}, the \sharc\ suite contains a small script called \ttt{molpro\_input.py}. It can be used to setup single point calculations, optimizations and frequency calculations on the HF, DFT, MP2 and CASSCF level of theory. Of course, \textsc{Molpro} has far more capabilities, but these are not covered by \ttt{molpro\_input.py}. However, \ttt{molpro\_input.py} can also prepare template files which are compatible with the \sharc-\textsc{Molpro} interface (\ttt{MOLPRO.template} file).

The script interactively asks the user to specify the calculation and afterwards writes an input file and optionally a run script.

\subsubsection{Input}

\paragraph{Type of calculation}

Choose to either perform a single-point calculation or a minimum optimization (including optionally frequency calculation), to generate a template file, or an optimization of a state crossing. For the template generation, no geometry file is needed, but the script looks for a \ttt{MOLPRO.input} in the same directory and allows to copy the settings. 

For single-point calculations, optimizations and frequency calculations, files in \textsc{Molden} format called \ttt{geom.molden}, \ttt{opt.molden} or \ttt{freq.molden}, respectively, are created (containing the orbitals, optimization steps and normal modes, respectively). The file \ttt{freq.molden} can be used to generate initial conditions with \ttt{wigner.py}.

\paragraph{Geometry}

Specify the geometry file in xyz format. Number of atoms and total nuclear charge is detected automatically. After the user inputs the total charge, the number of electrons is calculated automatically.

In the case of the generation of a template file, instead only the number of electrons is required.

\paragraph{Non-default atomic masses}

If a frequency calculation is requested, the user may modify the mass of specific atoms (e.g.\ to investigate isotopic effects). In the following menu, the user can add or remove atoms with their mass to a list containing all atoms with non-default masses. Each atom is referred to by its number as in the geometry file. Using the command \ttt{show} the user can display the list of atoms with non-default masses. Typing \ttt{end} confirms the list.

Note that when using the produced \textsc{Molden} file later with \ttt{wigner.py}, the user has to enter the same non-default masses again, since the \textsc{Molden} file does not contain the masses and \ttt{wigner.py} has no way to retrieve these numbers.

\paragraph{Level of theory}

Supported are Hartree-Fock (HF), density functional theory (DFT), M{\o}ller-Plesset perturbation theory (MP2), equation-of-motion coupled-cluster with singles and doubles (EOM-CCSD) and CASSCF (either single-state or state-averaged). All methods (except EOM-CCSD) are compatible with odd-electron wave functions (\ttt{molpro\_input.py} will use the corresponding UHF, UMP2 and UKS keywords in the input file, if necessary).

For template generation, state-average CASSCF is automatically chosen. All methods (except EOM-CCSD) can be combined with optimizations and frequency calculations, however, the frequency calculation is much more efficient with HF or SS-CASSCF. 

\paragraph{DFT functional}

For DFT calculations, enter a functional and choose whether dispersion correction should be applied. Note that the functional is just a string which is not checked by \ttt{molpro\_input.py}. 

\paragraph{Basis set}

The basis set is just a string which is not checked by \ttt{molpro\_input.py}. 

\paragraph{CASSCF settings}

For CASSCF calculations, enter the number of active electrons and orbitals. 

For SS-CASSCF, only the multiplicity needs to be specified. For SA-CASSCF, specify the number of states per multiplicity to be included. Note that \textsc{Molpro} allows to average over states with different numbers of electrons. This feature is not supported in \ttt{molpro\_input.py}. However, the user can generate a closely-matching input and simply add the missing states to the CASSCF block manually. 

For optimizations at SA-CASSCF level, the state to be optimized has to be given. For crossing point optimizations, two states need to be entered. The script automatically detects whether a conical intersection or a crossing between states of different multiplicity is requested and sets up the input accordingly.

\paragraph{EOM-CCSD settings}

EOM-CCSD calculations allow to calculate relatively accurate excited-state energies and oscillator strengths from a Hartree-Fock reference, but only for singlet excited states.

The user has to specify the number of states to be calculated. If only one state is requested, the script will setup a regular ground state CCSD calculation, while for more than one states, an EOM-CCSD calculation is setup.
Note that the calculation of transition properties takes twice as long as the energy calculation itself.

\paragraph{Memory}

Enter the amount of memory for \textsc{Molpro}. Note that values smaller than 50 MB are ignored, and 50 MB are used in this case.

\paragraph{Run script}

If requested, the script also generates a simple Bash script (\ttt{run\_molpro.sh}) to directly execute \textsc{Molpro}. The user has to enter the path to \textsc{Molpro} and the path to a suitable (fast) scratch directory. 

Note that the scratch directory will be deleted after the calculation, only the wave function file \ttt{wf} will be copied back to the main directory.







% ========================================================================================================= %

\section{MOLCAS Interface}\label{sec:int:molcas}

The \sharc-\textsc{Molcas} interface can be used to conduct excited-state dynamics based on \textsc{Molcas}' CASSCF wave functions, and with CASPT2 or MS-CASPT2 energies (and numerical gradients). 
RASSCF wave functions are not supported currently. 
It is important to note that currently, \sharc\ was only tested to work with \textsc{Openmolcas} 18, which is freely available.
Some versions of \textsc{Molcas} 8 might also work, but no guarantee is given.
Note that within this Manual, \textsc{Molcas} and \textsc{Openmolcas} are used synonymously.

The interface uses the modules GATEWAY, SEWARD (integrals), RASSCF (wave function, energies), RASSI (transition dipole moments, spin-orbit couplings, overlaps), MCLR, and ALASKA (gradients). 
For CASPT2 and MS-CASPT2, numerical gradients can be calculated, where the interface itself controls the calculations at the displaced geometries. 
The interface can also numerically differentiate dipole moments and spin-orbit couplings.
Since \textsc{Molcas} is not able to calculate the full nonadiabatic coupling vectors, only wave function overlaps are currently implemented in the interface (overlaps are calculated via RASSI). 
Using the \textsc{WFoverlap} code, it is possible to calculate Dyson norms between neutral and ionic states.
Note that (unlike \textsc{Molpro}) \textsc{Molcas} cannot average over states of different multiplicities; hence, the multiplicities are always computed in separate jobs which all share the same CAS settings.

The \sharc-\textsc{Molcas} interface furthermore allows to perform QM/MM dynamics (CASSCF plus force fields), through the \textsc{Molcas}-\textsc{Tinker} interface. 

The interface needs two additional input files, a template file for the quantum chemistry (file name is \ttt{MOLCAS.template}) and a resource file (\ttt{MOLCAS.resources}). If the interface finds files with the name \ttt{QM/MOLCAS.<i>.JobIph.init} or \ttt{QM/MOLCAS.<i>.RasOrb.init}, they are used as initial wave function files, where \ttt{<i>} is the multiplicity. In the case of QM/MM calculations, two more input files are needed: \ttt{MOLCAS.qmmm.key}, which contains the path to the force field parameters and the partitioning into QM and MM regions, and \ttt{MOLCAS.qmmm.table}, which contains the connectivity and force field IDs per atom.

\subsection{Template file: \ttt{MOLCAS.template}}

This file contains the specifications for the wave function. Note that this is not a valid \textsc{Molcas} input file. No sections like \ttt{\$GATEWAY}, etc., can be used. The file only contains a number of keywords, given in table~\ref{tab:molcas_temp}.
The actual input files are automatically generated.

A fully commented template file with all possible options is located in \ttt{\$SHARC/../examples/SHARC\_MOLCAS/}.

\begin{table}
  \centering
  \caption{Keywords for the \ttt{MOLACS.template} file.}
  \label{tab:molcas_temp}
  \begin{tabular}{>{\ttfamily}lp{12cm}}
  \hline
  Keyword       &Description\\
  \hline
basis           &The basis set used. Note that some basis sets (e.g., Pople basis sets) do not work, since the spin-orbit integrals cannot be calculated.\\
baslib          &Can be used to provide the path to a custom basis set library (analogous to the baslib keyword in \textsc{Molcas}).\\
nactel          &Number of active electrons for CASSCF.\\
ras2            &Number of active orbitals for CASSCF.\\
inactive        &Number of inactive orbitals.\\
roots           &Followed by a list of integers, giving the number of states per multiplicity in the state-averaging procedure.\\
rootpad         &Followed by a list of integers, giving the number of extra, zero-weight states in the state-averaging.\\
% spin            &The full line reads as \ttt{spin s roots r} and specifies the number of roots for the given multiplicity. This influences only the number of states in the state-averaging procedure. The number of states in the dynamics must not be larger than the number of states given here. This input style is still valid but deprecated, the \ttt{roots} keyword should be used instead.\\
method          &Followed by a string, which is either ``CASSCF'', ``CASPT2'' or ``MS-CASPT2'' (case insensitive), defining the level of theory. Default is ``CASSCF''.\\
no-douglas-kroll&Deactivates the use of the scalar-relativistic DK Hamiltonian and uses a non-relativistic Hamiltonian instead. Default is Douglas-Kroll-Hess 2nd order  (In \textsc{Molcas} standard parametrization).\\
ipea            &Followed by a float giving the IP-EA shift for CASPT2 (see \textsc{Molcas} manual for more information). The default is 0.25, as in \textsc{Molcas}.\\
imaginary       &Followed by a float giving the imaginary level shift for CASPT2 (see \textsc{Molcas} manual for more information). The default is 0.0, as in \textsc{Molcas}.\\
frozen          &Number of frozen orbitals for CASPT2. Default is -1, which lets \textsc{Molcas} choose the number of frozen orbitals.\\
cholesky        &Activates Cholesky decomposition in \textsc{Molcas}. Recommended for large basis sets. Will use numerical gradients even for CASSCF. Default is to use regular 4-electron integrals.\\
cholesky\_analytical    &Activates analytical SA-CASSCF gradients with Cholesky decomposition. This is only possible with \textsc{Molcas} 8.\\
gradaccudefault &(float) Default accuracy for CP-MCSCF.\\
gradaccumax     &(float) Worst acceptable accuracy for CP-MCSCF.\\
displ           &Cartesian displacement (in \AA) for numerical differentiation (default 0.005~\AA).\\
qmmm            &Activates the QM/MM mode. In this case, two more input files need to be present (see below).\\
pcmset          &Activates the PCM mode. Three arguments follow: the solvent (a string, default ``water''), the AARE value (a float, default 0.4, optional), the RMIN value (a float, default 1.0, optional). Check \textsc{Molcas} manual for details.\\
pcmstate        &Defines the state for which the PCM charges will be optimized. Followed by two numbers: multiplicity (1=singlet, ...) and state (1=lowest state of that multiplicity). Default is the first state according to the state request.\\
  \hline
  \end{tabular}
\end{table}

Simple template files can be setup with the tool \ttt{molcas\_input.py}.


\subsection{Resource file: \ttt{MOLCAS.resources}}

The file \ttt{MOLCAS.resources} contains mainly paths (to the \textsc{Molcas} executables, to the scratch directory, etc.). This file must reside in the same directory where the interface is started. It uses a simple ``\ttt{keyword argument}'' syntax. Comments using \# and blank lines are possible, the order of keywords is arbitrary. Lines with unknown keywords are ignored, since the interface just searches the file for certain keywords.

Table~\ref{tab:molcas_sh2} lists the existing keywords.
A fully commented resource file for this interface with all possible options is located in \ttt{\$SHARC/../examples/SHARC\_MOLCAS/}.

\begin{table}
  \centering
  \caption{Keywords for the \ttt{MOLCAS.resources} file.}
  \label{tab:molcas_sh2}
  \begin{tabular}{>{\ttfamily}lp{12cm}}
  \hline
  Keyword       &Description\\
  \hline
molcas          &Is the path to the \textsc{Openmolcas} installation. Relative and absolute paths, environment variables and \ttt{\textasciitilde} can be used. The interface will set \ttt{\$MOLCAS} to this path. If this keyword is not present in \ttt{MOLCAS.resources}, the interface will use the environment variable \ttt{\$MOLCAS}, if it is set.\\
tinker          &The path to the \textsc{Tinker} installation, usable with \textsc{Molcas}.\\
wfoverlap       &Is the path to the \textsc{WFoverlap} executable. Relative and absolute paths, environment variables and \ttt{\textasciitilde} can be used. Only needed for Dyson norm calculations.\\
scratchdir      &Is a path to the temporary directory. Relative and absolute paths, environment variables and \ttt{\textasciitilde} can be used. If it does not exist, the interface will create it. In any case, the interface will delete this directory after the calculation.\\
savedir         &Is a path to another temporary directory.  Relative and absolute paths, environment variables and \ttt{\textasciitilde} can be used. The interface will store files needed for restart there.\\
memory          &The memory usable by \textsc{Molcas}. The interface will set \ttt{\$MOLCASMEM} to this value. The default is 10~MB.\\
ncpu            &The number of CPUs used by the interface. If zero or negative, one CPU will be used and all subtasks (Hamiltonian, dipole moments, analytical gradients, overlaps) will be done in one \textsc{Molcas} call. If positive, analytical gradients will be calculated by separate calls to \textsc{Molcas}, parallelized over the given number of CPUs. \\
mpi\_parallel   &Uses MPI parallel MOLCAS runs. The number of cores is dynamically chosen based on the available cores and the number of tasks (energies, gradients, displacements).\\
schedule\_scaling &Gives the expected parallelizable fraction of the MOLCAS run time (Amdahl's law). With a value close to zero, the interface will try to run all jobs at the same time. With values close to one, jobs will be run sequentially with the maximum number of cores.\\
delay           &Followed by a float giving the delay in seconds between starting parallel jobs to avoid excessive disk I/O.\\
always\_orb\_init &Do not use the orbital guesses from previous calculations/time steps, but always use the provided initial orbitals.\\
always\_guess   &Always use the orbital guess from \textsc{Molcas}'s \ttt{guessorb} module.\\
  debug           &Increases the verbosity of the interface.\\
  no\_print       &Reduces interface standard output.\\
  \hline
  \end{tabular}
\end{table}

Note that the interface sets all environment variables necessary to run \textsc{Molcas} (e.g., \ttt{\$MOLCAS}, \ttt{\$MOLCASMEM}, \ttt{\$WorkDir} ,\ttt{\$Project}) automatically, based on the input from \ttt{MOLCAS.resources} and \ttt{QM.in}.

Some explanations on parallelization: There are two parallel modes in which the interface can act. In the first mode, the wave function and expectation values are calculated serially first, and then all analytical gradients are computed in parallel, over the given number of CPUs. If the number of CPUs is one, then still the interface prepares independent computations for each gradient, and runs those sequentially on one CPU. If the given number of CPUs is negative or zero, then the gradients are not prepared as separate \textsc{Molcas} calculations, but together with the other quantities. The latter variant has less overhead, but is only recommended if the user is sure that the gradient calculations (MCLR) will converge in virtually all cases. If MCLR does not converge occasionally, it is advisable to use NCPU=1 instead of NCPU$\leq$0.

In the second parallelization mode, used for numerical gradients, the central point is calculated first and then all displacements are computed in parallel. This mode will be used for gradients on CASPT2, MS-CASPT2 and Cholesky-CASSCF level as well as for spin-orbit/dipole moment derivatives on all levels. In this mode there is no distinction between NCPU=1 and NCPU$\leq$0. 


\subsection{Template file generator: \ttt{molcas\_input.py}}\label{sec:molcas_input.py}

This is a small interactive script to generate template files for the \sharc-\textsc{Molcas} interface. It simply queries the user for some input parameters and then writes the file \ttt{MOLCAS.template}, which can be used to run \sharc\ simulations with the \sharc-\textsc{Molcas} interface.
The input generator can also be used to write proper \textsc{Molcas} input files for single-point calculations and optimizations/frequency calculations on CASSCF and (MS-)CASPT2 level of theory.

\paragraph{Type of calculation}

Choose to either perform a single-point calculation or a minimum optimization (including optionally frequency calculation), or to generate a template file. For the template generation, no geometry file is needed, but the script looks for a \ttt{MOLCAS.input} in the same directory and allows to copy the settings. 

For single-point calculations, optimizations and frequency calculations, files in \textsc{Molden} format are created (containing the orbitals, optimization steps and normal modes, respectively). The file \ttt{MOLCAS.freq.molden} can be used to generate initial conditions with \ttt{wigner.py}.

\paragraph{Geometry file}

The geometry file is only used to calculate the nuclear charge.

\paragraph{Charge}

This is the overall charge of the molecule. This number is used with the nuclear charge to calculate the number of electrons.

\paragraph{Method}

Choose either CASSCF or CASPT2. Multi-state CASPT2 can be requested later.

\paragraph{Basis set}

This is simply a string, which is \textit{not} checked by the script to be a valid basis set of the \textsc{Molcas} library.

\paragraph{Number of active electrons and orbitals}

These settings are necessary for the definition of the CASSCF wave function. The number of inactive orbitals is automatically calculated from the total number of electrons and the number of active electrons.

\paragraph{States for state-averaging}

For each multiplicity, the number of states for the state-averaging procedure must be equal or larger than the number of states used in the dynamics.

\paragraph{Further settings}

Depending on the run type and method, the script might ask further questions regarding the root to optimize, CASPT2 settings (whether to do multi-state CASPT2, IPEA shift, imaginary level shift), or whether a spin-orbit RASSI should be performed (for input file generation only).



\subsection{QM/MM key file: \ttt{MOLCAS.qmmm.key}}

This file defines some aspects of the QM/MM calculation. An example is given below:

\begin{example}
\begin{verbatim}
parameters $MOLCAS/tinker/params/amber99sb.prm

QMMM 18
QM    -1 15
MM   -16 18

QMMM-ELECTROSTATICS ESPF
\end{verbatim}
\end{example}

The first line defines the force field parameters, which are read from \textsc{Tinker}'s library here. Note that in the file path, environment variables are expanded by the interface.
The next three lines define the QM and MM regions of the system, where in this case atoms 1--15 are in the QM region and atoms 16--18 are in the MM region (note the minus signs indicating the begin of an interval). The last line defines how to treat electrostatics in the calculation (currently, only ESPF is implemented for \textsc{Molcas}, so this line has to be present as shown, or omitted).

\subsection{QM/MM connection table file: \ttt{MOLCAS.qmmm.table}}

This is the template to generate the \textsc{Tinker} xyz file, which besides the geometry contains the force field atom type number and the connectivity information. A sample looks like:

\begin{example}
\begin{verbatim}
8
1 N*   1.6  3.3 -2.0  1132  2
2 CK   0.3  3.1 -2.6  1136  1  3  4
3 H5  -0.4  2.9 -1.9  1145  2
4 NB   0.3  3.2 -3.9  1135  2  5
5 CB   1.6  3.5 -4.2  1134  4  6
6 CA   2.2  3.8 -5.4  1140  5  7
7 N2   1.6  3.8 -6.6  1142  6  8
8 H    2.1  3.9 -7.5  1143  7
\end{verbatim}
\end{example}

The first line contains the number of atoms. In each following line, the atom index, the atomic symbol (or name), the coordinates of the atom, the force field atom type number and the index of atoms bonded to the current atom. The coordinates in this file are not used and are replaced by the \sharc-\textsc{Molcas} interface by the actual coordinates of the atoms. The atom type number can be looked up in the \ttt{.prm} files of the \textsc{Tinker} parameter directory. 






% ========================================================================================================= %

\section{COLUMBUS Interface}\label{sec:int:columbus}

The \sharc-\textsc{Columbus} interface allows to run \sharc\ dynamics based on \textsc{Columbus}' CASSCF, RASSCF and MRCI wave functions. 
The interface is compatible to \textsc{Columbus} calculations utilizing the \textsc{Columbus}-\textsc{Molcas} interface (\textsc{Seward} integrals and \textsc{Alaska} gradients), or using the \textsc{Dalton} integral code distributed with \textsc{Columbus}. 
Using \textsc{Seward} integrals, spin-orbit couplings can be calculated, but no nonadiabatic couplings (only overlaps can thus be used).
Using \textsc{Dalton} integrals, spin-orbit couplings are not possible, but nonadiabatic couplings can be calculated.
The CASSCF step can be done with either \textsc{Columbus}' \ttt{mcscf} code (all features available) or with \textsc{Molcas}' \ttt{rasscf} code (faster, but no gradients possible).
The interface utilizes the \textsc{WFoverlap} program to calculate the overlap matrices.
The interface can also calculate Dyson norms between neutral and ionic wave functions using the \textsc{WFoverlap} code.

The interface needs as additional input the file \ttt{QM/COLUMBUS.resources} and a template directory containing all input files needed for the \textsc{Columbus} calculations. Initial MOs can be given in the file \ttt{QM/mocoef\_mc.init}.
For multiple jobs, initial MOs can be given as \ttt{QM/mocoef\_mc.init.<job>}.
For runs with \ttt{rasscf}, initial MOs have to be given as \ttt{molcas.RasOrb.init} or \ttt{molcas.RasOrb.init.<job>}.

\subsection{Template input}

The interface does not generate the full \textsc{Columbus} input on-the-fly. Instead, the interface uses an existing set of input files and performs only necessary modifications (e.g., the number of states). The set of input files must be provided by the user. Please see the \link{http://www.univie.ac.at/columbus/docs_COL70/documentation_main.html}{\textsc{Columbus} online documentation} and, most importantly, the 
\link{http://www.univie.ac.at/columbus/docs_COL70/tutorial-SO.pdf}{\textsc{Columbus} SOCI tutorial} for a documentation of the necessary input.
The \sharc\ tutorial also has a section about generating the required \textsc{Columbus} input file collection.
An example template directory is located in \ttt{\$SHARC/../examples/SHARC\_COLUMBUS/}.

Generally, the input consists of a directory with one subdirectory with input for each multiplicity (singlets, doublets, triplets, \dots). However, even-electron wave functions of different multiplicities can be computed together in the same job if spin-orbit couplings are desired. Independent multiple-DRT inputs (ISC keyword) are also acceptable. Note that symmetry is not allowed when using the interface.

The path to the template directory must be given in \ttt{COLUMBUS.resources}, along with the other resources settings.

\paragraph{Integral input}

The interface is able to use input for calculations using \textsc{Seward} or \textsc{Dalton} integrals. 
If you want to calculate SOCs, you have to use \textsc{Seward} and have to include the AMFI keyword in the integral input.
If you use \textsc{Dalton}, SOCs are not available, but it is possible to compute nonadiabatic couplings.

It is important to make sure that \textbf{the order of atoms} in the template input files and in the \sharc\ input \textbf{is consistent}.

Furthermore, it is necessary to prepare all template subdirectories with the same integral code and the same AO basis set. 

\paragraph{MCSCF input}

The MCSCF section can use any desired state-averaging scheme, since the number of states in MCSCF is independent of the number of states in the MRCI module. However, frozen core orbitals in the MCSCF step are not possible (since otherwise gradients cannot be computed). Prepare the MCSCF input for CI gradients. It is advisable to use very tight MCSCF convergence criteria.

If gradients are not needed, you can also manually prepare a \textsc{Molcas} RASSCF input in \ttt{molcas.input}, in order to use \textsc{Molcas} RASSCF instead of \textsc{Columbus} MCSCF (see \ttt{molcas\_rasscf} keyword).

\paragraph{MRCI input}

Either prepare a single-DRT input without SOCI (to cover a single multiplicity), a single-DRT input with SOCI and a sufficient maximum multiplicity for spin-orbit couplings or an independent multiple-DRT input (as, e.g., for ISC optimizations). Make sure that all multiplicities are covered with all input directories.

In the MRCI input, make sure to use sequential \ttt{ciudg}. Also take care to setup gradient input on MRCI level.

\paragraph{Job control}

Setup a single-point calculation with the following steps:
\begin{itemize}
  \item SCF
  \item MCSCF
  \item MR-CISD (serial operation) \textbf{or} SO-CI coupled to non-rel CI (for SOCI DRT inputs)
  \item one-electron properties for all methods
  \item transition moments for MR-CISD
  \item nonadiabatic couplings (and/or gradients)
\end{itemize}

Request first transition moments and interstate couplings (or alternatively full nonadiabatic couplings if Dalton integrals are used) in the following dialogues. Analysis in internal coordinates and intersection slope analysis are not required.

\subsection{Resource file: \ttt{COLUMBUS.resources}}

Beyond the information from \ttt{QM.in} the interface needs additional input, which is read from the file \ttt{COLUMBUS.resources}. Table~\ref{tab:columbus_sh2} lists the keywords which can be given in the file.
A fully commented resource file with all possible options is located in \ttt{\$SHARC/../examples/SHARC\_COLUMBUS/}.



\begin{longtable}{>{\ttfamily}lp{12cm}}
  \caption{Keywords for the \ttt{COLUMBUS.resources} file.}
  \label{tab:columbus_sh2}\\

% ========================================

    \hline
    \rmfamily Keyword     &Description\\
    \hline
  \endfirsthead

% ========================================

\tthdump{
    \multicolumn{2}{c}{{\bfseries \tablename\ \thetable{} \mdseries-- Continued from previous page}} \\
    \hline
    \rmfamily Keyword     &Description\\
    \hline
  \endhead
}

% ========================================

\tthdump{
    \hline 
    \multicolumn{2}{r}{{Continued on next page}} \\ 
%     \hline
  \endfoot
}
  
% =======================================

\tthdump{
    \hline
  \endlastfoot
}

columbus        &Path to the \textsc{Columbus} main directory. Relative and absolute paths, environment variables and \ttt{\textasciitilde} can be used.\\
molcas          &Path to the \textsc{Molcas} main directory.  Relative and absolute paths, environment variables and \ttt{\textasciitilde} can be used. This path is only used for overlap/Dyson calculations (since in this case the interface calls \textsc{Molcas} explicitly). Otherwise \textsc{Columbus} will use the path to \textsc{Molcas} specified during the installation of \textsc{Columbus}.\\
wfoverlap       &Path to the wave function overlap and Dyson norm code. Relative and absolute paths, environment variables and \ttt{\textasciitilde} can be used. Only necessary if overlaps or Dyson norms are calculated. Needs a suitable code that computes overlaps of CI wave functions.\\
runc            &Path to the \ttt{runc} script for \textsc{Columbus} execution. Default is \ttt{\$COLUMBUS/runc}. This keyword is intended for users who like to modify \ttt{runc}.\\
scratchdir      &Path to the temporary directory, used to perform the \textsc{Columbus} calculations. Relative and absolute paths, environment variables and \ttt{\textasciitilde} can be used. If it does not exist, the interface will create it. The interface will delete this directory after the calculation.\\
savedir         &Path to another directory.  Relative and absolute paths, environment variables and \ttt{\textasciitilde} can be used. The interface will store files needed for restart in this directory. Default is a subdirectory of the directory where the interface is executed.\\
memory          &(integer, MB) Memory for \textsc{Columbus}. The maximum amount of memory used by the wfoverlap code is also controlled with this keyword.\\
ncpu            &(integer) Number of CPU cores to use for SMP-parallel execution of the wfoverlap code. Parallel \textsc{Columbus} is currently not supported.\\
wfthres         &(float) Gives the amount of wave function norm which will be kept in the truncation of the determinant files for Dyson and overlap calculations. Default is 0.97 (i.e., the wave functions in the determinant files will have a norm of 0.97)\\
nooverlap       &(no argument) Do not keep the files necessary to calculate wave function overlaps in the next time step. If Dyson norms are requested, \ttt{nooverlap} is ignored.\\
always\_orb\_init    &Use the initial MO guess (\ttt{mocoef\_mc.init}) for all time steps, not only for the first step.\\
always\_guess     &In all time steps obtain the MO guess from an SCF calculation, instead of using the MOs from the previous step.\\
integrals       &Followed by a string which is either \ttt{seward} or \ttt{dalton}. Chooses the integral program for \textsc{Columbus}. Note that with \textsc{Dalton} integrals SOC is not available. Default is \ttt{seward}.\\
molcas\_rasscf  &Use \textsc{Molcas}' RASSCF program instead of \textsc{Columbus}' MCSCF program. Needs a properly prepared \textsc{Columbus} input (\ttt{\&RASSCF} section in \ttt{molcas.input}). Note that gradients are not available in this mode.\\
template        &Is followed by the path to the directory containing the template subdirectories. Relative and absolute paths, environment variables and \ttt{\textasciitilde} can be used. See also \ref{int:col:template}.\\
DIR             &See \ref{int:col:template}.\\
MOCOEF          &See \ref{int:col:template}.\\
numfrozcore           &Can be used to override the number of frozen orbitals in Dyson calculations (Default is the value from the \ttt{cidrt} input).\\
numocc           &Can be used to declare orbitals above the frozen orbitals as doubly occupied in Dyson calculations. No ionization from these orbitals is calculated, but otherwise they are considered in the Dyson calculations. Default is zero.\\
  debug           &Increases the verbosity of the interface.\\
  no\_print       &Reduces interface standard output.\\
\end{longtable}



\subsection{Template setup}\label{int:col:template}

The template directory contains several subdirectories with input for different multiplicities. An example is given in figure~\ref{fig:dirs_COLtemp}. In \ttt{COLUMBUS.resources}, the user has to associate each multiplicity to a subdirectory. The line ``\ttt{DIR 1 Sing\_Trip}'' would make the interface use the input files from the subdirectory \ttt{Sing\_Trip} when calculating singlet states (the \ttt{1} refers to singlet calculations). All calculations using a particular input subdirectory are called a job.

Additionally, the user must specify which job(s) provide the MO coefficients (e.g., the calculation for doublet states could be based on the same MOs as the singlet and triplet calculation). The line ``\ttt{MOCOEF Doub\_Quar Sing\_Trip}'' would tell the interface to do a MCSCF calculation in the \ttt{Sing\_Trip} job, and reuse the MOs when doing the \ttt{Doub\_Quar} job without reoptimizing the MOs.

\begin{figure}[htb]
  \centering
  \includegraphics[scale=1]{img/dirs_COLtemp/dirs_COLtemp.pdf}
  \caption{Example directory structure of the \textsc{Columbus} template directory}
  \label{fig:dirs_COLtemp}
\end{figure}






% ========================================================================================================= %

\section{Analytical PESs Interface}\label{sec:int:analytical}

The \sharc\ suite also contains an interface which allows to run dynamics simulations on PESs expressed with analytical functions.
In order to allow dynamics simulations in the same way as it is done on-the-fly, the interface uses two kinds of potential couplings:
\begin{itemize}
  \item couplings that are pre-diagonalized in the interface (yielding the equivalent of the MCH basis),
  \item couplings given by the interface to \sharc\ as off-diagonal elements.
\end{itemize}

The interface needs one additional input file, called \ttt{Analytical.template}, which contains the definitions of all analytical expressions.

\subsection{Parametrization}

The interface has to be provided with analytical expressions for all matrix elements of the following matrices in the diabatic basis:
\begin{itemize}
  \item Hamiltonian: $\mathbf{H}$
  \item Derivative of the Hamiltonian with respect to each atomic coordinate: $\mathbf{H}_{x_i}$
  \item (Transition) dipole matrices for each polarization direction: $\mathbf{M}_i$
  \item Real and imaginary part of the SOC matrix: $\boldsymbol{\Sigma}$
  \item (Optionally) the derivatives of the transition dipole matrices: $\mathbf{D}_{i,x_j}$
\end{itemize}

The diabatic Hamiltonian is diagonalized:
\begin{equation}
  \mathbf{H}^{\text{d}}=\mathbf{W}^\dagger\mathbf{H}\mathbf{W}
\end{equation}


Then the following calculations lead to the MCH matrices which are passed to \sharc:
\begin{align}
  \mathbf{H}^{\text{MCH}}&=\mathbf{H}^{\text{d}}+\mathbf{W}^\dagger\boldsymbol{\Sigma}\mathbf{W}\\
  \left(\mathbf{g}^{\text{MCH}}_\alpha\right)_{x_i}&=\left(\mathbf{W}^\dagger\mathbf{H}_{x_i}\mathbf{W}\right)_{\alpha\alpha}\\
  \mathbf{M}^{\text{MCH}}_i&=\mathbf{W}^\dagger\mathbf{M}_i\mathbf{W}\\
  \mathbf{S}^{\text{MCH}}(t_0,t)&=\mathbf{W}^\dagger(t_0)\mathbf{W}(t)\\
  \mathbf{D}^{\text{MCH}}_{i,x_j}&=\mathbf{W}^\dagger\mathbf{D}_{i,x_j}\mathbf{W}
\end{align}
The MCH Hamiltonian is the diagonalized diabatic Hamiltonian plus the SO matrix transformed into the MCH basis. The gradients in the MCH basis are obtained by transforming the derivative matrices into the MCH basis. The dipole matrices are also simply transformed into the MCH basis. The overlap matrix is the overlap of old and new transformation matrix.

\subsection{Template file: \ttt{Analytical.template}}

The interface-specific input file is called \ttt{Analytical.template}. It contains the analytical expressions for all matrix elements mentioned above. All analytical expressions in this file are evaluated considering the atomic coordinates read from \ttt{QM.in}.

The file consists of a file header and the file body. The file body consists of variable definition blocks and matrix blocks.
A commented template file is located in \ttt{\$SHARC/../examples/SHARC\_Analytical/}.

\paragraph{Header} 

The header looks similar to an xyz file:
\begin{example}
  \begin{verbatim}
2
2
I       xI       0       0
Br      xBr      0       0
\end{verbatim}
\end{example}
Here, the first line gives the number of atoms and the second line the number of states. 

On the remaining lines, each Cartesian component of the atomic coordinates is associated to a variable name, which can be used in the analytical expressions. If a zero (\ttt{0}) is given instead of a variable name, then the corresponding Cartesian coordinate is neglected. In the above example, the variable name \ttt{xI} is associated with the $x$ coordinate of the first atom given in \ttt{QM.in}. The $y$ and $z$ coordinates of the first atom are neglected.

All variable names must be \link{https://docs.python.org/2/reference/lexical_analysis.html#identifiers}{valid Python identifiers} and must not start with an underscore. Hence, all strings starting with a letter, followed by an arbitrary number of letters, digits and underscores, are valid. It is not allowed to use a variable name twice.

Note that the file header also contains the atom labels, which are just used for cross-checking against the atom labels in \ttt{QM.in}.

The file header must not contain comments, neither at the end of a line nor separate lines. Also, blank lines are not allowed in the header. After the last line of the header (where the variables for the $n_{\text{atom}}$-th atom are defined), blank lines and comments can be used freely (except in matrix blocks).

\paragraph{Variable definition blocks}

Variable definition blocks can be used to store additional numerical values (beyond the atomic coordinates) in variables, which can then be used in the equations in the matrix blocks. The most obvious use for this is of course to define values which will appear several times in the equations.

A variable definition block looks like:
\begin{example}
  \begin{verbatim}
Variables
A1      0.067
g1      0.996   # Trailing comment
# Blank line with comment only
R1      4.666
End
\end{verbatim}
\end{example}
Each block starts with the keyword ``Variables'' and is terminated with ``End''. Inbetween, on each line a variable name and the corresponding numerical value (separated by blanks) can be given. Note that the naming conventions given above also apply to variables defined in these blocks. 

There can be any number of complete variable definitions blocks in the input file. All blocks are read first, before any matrix expressions are evaluated. Hence, the relative order of the variable blocks and the matrix blocks does not matter. Also, note that variable names must not appear twice, so variables cannot be redefined halfway through the file.

\paragraph{Matrix blocks}

The most important information in the input file are of course contained in the expressions in the matrix blocks. In general, a matrix block has the following format:
\begin{example}
  \begin{verbatim}
Matrix_Identifier
V11
V12,    V22
V13,    V23,    V33
...
\end{verbatim}
\end{example}
The first line identifies the type of matrix. Those are valid identifiers:

\begin{tabular}{p{5cm}p{9cm}}
\ttt{Hamiltonian}                               &Defines the Hamiltonian including the diabatic potential couplings.\\
\ttt{Derivatives} followed by a variable name   &Derivative of the Hamiltonian with respect to the given variable.\\
\ttt{Dipole} followed by 1, 2 or 3              &(Transition) dipole moment matrix for Cartesian direction $x$, $y$ or $z$, respectively.\\
\ttt{Dipolederivatives} followed by 1, 2 or 3 followed by a variable name       &Derivative of the respective dipole moment matrix.\\
\ttt{SpinOrbit} followed by \ttt{R} or \ttt{I}  &Real or Imaginary (respectively) part of the spin-orbit coupling matrix $\boldsymbol{\Sigma}$.\\
\end{tabular}

Since the interface searches the file for these identifiers starting from the top until it is found, for each matrix only the first block takes effect. Note that the Hamiltonian and all relevant derivatives must be present. If dipole matrix, dipole derivative matrix or SO matrix definitions are missing, they will be assumed zero.

In the lines after the identifier, the expressions for each matrix element are given. Note the lower triangular format (all matrices are assumed symmetric, except the imaginary part of the SO matrix, which is assumed antisymmetric). Matrix elements must be separated by commas (so that whitespace can be used inside the expressions). There must be at least as many lines as the number of states (additional lines are neglected). If any line or matrix element is missing, the interface will abort.

An examplary block looks like:
\begin{example}
  \begin{verbatim}
Hamiltonian
A1*( (1.-math.exp(g1*(R1-xI+xBr)))**2-1.),
0.0006,                                         3e-5*(xI-xBr)**2
\end{verbatim}
\end{example}
It is important to understand that the expressions are directly evaluated by the Python interpreter, hence all expressions must be valid Python expressions which evaluate to numeric (integer or float) values. Only the variables defined above can be used. 

Note that exponentiation in Python is \ttt{**}. In order to provide most usual mathematical functions, the \link{https://docs.python.org/2/library/math.html}{\ttt{math} module} is available. Among others, the \ttt{math} module provides the following functions:
\begin{itemize}
  \item \ttt{math.exp(x)}: Exponential function
  \item \ttt{math.log(x)}: Natural logarithm
  \item \ttt{math.pow(x,y)}: $x^y$
  \item \ttt{math.sqrt(x)}: $\sqrt{x}$
  \item \ttt{math.cos(x)}, \ttt{math.sin(x)}, \ttt{math.tan(x)}
  \item \ttt{math.acos(x)}, \ttt{math.asin(x)}, \ttt{math.atan(x)}
  \item \ttt{math.atan2(y,x)}: $\tan^{-1}\left(\frac{y}{x}\right)$, as in many programming languages (takes care of phases)
  \item \ttt{math.pi}, \ttt{math.e}: $\pi$ and Euler's number
  \item \ttt{math.cosh(x)}, \ttt{math.sinh(x)}: Hyperbolic functions (also tanh, acosh, asinh, atanh are available)
\end{itemize}



% ========================================================================================================= %

\section{ADF Interface}\label{sec:int:adf}

The \sharc-ADF interface allows to run \sharc\ simulations with ADF's TD-DFT functionality.
The interface is compatible with restricted and unrestricted ground states, but not with symmetry.
Spin-orbit couplings are obtained with the perturbative ZORA formalism, and wave function overlaps from the \textsc{WFoverlap} code are available (but no nonadiabatic couplings).
Dyson norms can also be computed through the \textsc{WFoverlap} code.
\textsc{TheoDORE} can be used to perform automatic wave function analysis.
QM/MM is possible through ADF's internal implementation, but only mechanical embedding is currently available in ADF.

The interface needs two additional input files, a template file for the quantum chemistry (file name is \ttt{ADF.template}) and a resource file (\ttt{ADF.resources}). 
If files \ttt{QM/ADF.t21.<job>.init} are are present, they are used to provide an initial orbital guess for the SCF calculation of the respective job.

Before running the ADF interface, it is necessary to source one of the \ttt{adfrc} files, so that Python can find the \ttt{kf} module of ADF.

Note that for full functionality of the interface, a very new ADF version is necessary. The computation of wave function overlaps requires at least ADF2017 (Dyson norms work with older ADF). Using the \ttt{dvd\_residu} or \ttt{qmmm\_coupling 2} options (see below) requires at least ADF2017.207 or ADF2018. Using \ttt{rihartreefock} (see below) requires at least ADF2016. 

\subsection{Template file: \ttt{ADF.template}}

This file contains the specifications for the quantum chemistry calculation. Note that this is not a valid \textsc{ADF} input file. The file only contains a number of keywords, given in table~\ref{tab:adf_temp}. The actual input for \textsc{ADF} will be generated automatically through the interface.
In order to enable many functionalities of ADF and to allow fine-tuning of the performance for large calculations, the template has a relatively large number of keywords.

A fully commented template file---with all possible options, a comprehensive descriptions, and some practical hints---is located in \ttt{\$SHARC/../examples/SHARC\_ADF/ADF.template}.
We recommend that users start from this template file and modify it appropriately for their calculations.

\begin{table}
  \centering
  \caption{Keywords for the \ttt{ADF.template} file.}
  \label{tab:adf_temp}
  \small
  \begin{tabular}{>{\ttfamily}lp{12cm}}
  \hline
  Keyword       &Description\\
  \hline
relativistic            &If not given, perform a nonrelativistic calculation. Otherwise, copy the line verbatim to the ADF input.
\\
basis                   &Gives the basis set for all atoms (default SZ).
\\
basis\_path             &gives the path to the basis library of ADF (\textasciitilde\ and \$ can be used).
\\
basis\_per\_element     &Followed by an elemental symbol (e.g., "Fe", "H.1") and then by a path to the desired ADF basis set file. Files with frozen core should not be used.
\\
define\_fragment        &Followed by an elemental symbol (e.g., "Fe", "H.1") and then by a list of atom numbers which should belong to this atom type.
\\
functional              &Followed by two strings. First argument gives the type of functional (LDA, GGA, hybrid), second argument gives the functional (VWN, BP86, B3LYP, ...).
\\
functional\_xcfun       &Enables functional evaluation with the XCFun library within ADF.
\\
dispersion              &If present, is written verbatim to ADF input with all arguments.
\\
charge                  &Sets the total charge of the (QM) system. Can be either followed by a single integer (then the interface will automatically assign the charges to the multiplicities) or by one charge per multiplicity. Automatic assignment might not work correctly for QM/MM computations.
\\
totalenergy             &Activates the computation of total energies (by default, ADF computes binding energies). Does not work for relativistic or QM/MM calculations.
\\
cosmo                   &Followed by a string giving a solvent. Activates COSMO (no gradients possible).
\\
cosmo\_neql             &Activates non-equilibrium solvation, which is needed for vertical excitation calculations. Is followed by a float giving the square of the refractive index of the solvent.
\\
grid                    &Followed by two strings (e.g., \ttt{beckegrid normal} or \ttt{integration 4.0}) defining which integration grid and accuracy to use. For details, see the example template file.
\\
grid\_qpnear            &Followed by a float giving the maximum distance (in Bohr) an MM point charge can have from a QM atom, such that integration grid points are generated around the MM charge.
\\
grid\_per\_atom         &Followed by a string (e.g., \ttt{basic}, \ttt{normal}, \ttt{good}) and a list of the atoms which should have the given integration accuracy. Can be used multiple times with different qualities.
\\
fit                     &Followed by two strings (e.g., \ttt{zlmfit normal} or \ttt{stofit}) defining which Coulomb integration method and accuracy to use. For details, see the example template file.
\\
fit\_per\_atom          &Works like \ttt{grid\_per\_atom}, but for the Coulomb method accuracy.
\\
exactdensity            &Enables the \ttt{exactdensity} keyword in the ADF input.
\\
rihartreefock           &Followed by a quality keyword (e.g., \ttt{basic}, \ttt{normal}, \ttt{good}). If not present, the old HF exchange routines in ADF are used.
\\
rihf\_per\_atom         &Works like \ttt{grid\_per\_atom}, but for the RI Hartree Fock method.
\\
occupations             &If present, the foll line is copied verbatim to the ADF input.
\\
scf\_iterations         &Followed by the maximum number of SCF iterations (default: 100)
\\
linearscaling           &Followed by an integer (between 0 and 99), which controls whether certain terms are neglected in ADF.
\\
cpks\_eps               &Followed by a float (default 0.0001) giving the convergence threshold in the CPKS equations for excited-state gradients.
\\
no\_tda                 &This keyword deactivates TDA, which the interface requests by default.
\\
paddingstates           &Followed by a list of integers, which give the number of extra states to compute by ADF, but which are neglected in the output. Should not be changed between time steps, as this will break ADF's restart routines.
\\
dvd\_vectors            &Number of Davidson vectors. Default: min(40,nstates+40).
\\
dvd\_tolerance          &Energy convergence criterion for the excited states (in Hartree).
\\
dvd\_residu             &Residual norm convergence criterion for the excited states. Only works for ADF version $\geq$2017.207 or ADF2018.
\\
dvd\_mblocksmall        &Activates the \ttt{mblocksmall} keyword in the ADF input. This undocumented keyword changes how the Davidson algorithm works. In reduces computation time, but might lead to incomplete convergence in certain cases.
\\
unrestricted\_triplets  &Requests that the triplets are calculated in a separate job from an unrestricted ground state. Default is to compute triplets as linear response of the restricted singlet ground state.
\\
modifyexcitations       &Followed by an integer, indicating that excitation should only be possible from the first $n$ MOs. Can be used to compute core-excitation states (for X-Ray spectra).
\\
qmmm                    &Activates QM/MM. In this case, the interface will look for two additional input files (see below).
\\
qmmm\_coupling          &Followed by an integer (1=mechanical embedding, 2=electrostatic embedding). Default is 1, option 2 only works for ADF version $\geq$2017.207 or ADF2018.\\
  \hline
  \end{tabular}
\end{table}

\subsection{Resource file: \ttt{ADF.resources}}

The file \ttt{ADF.resources} contains mainly paths (to the \textsc{ADF} executables, to the scratch directory, etc.) and other resources, plus settings for \ttt{wfoverlap.x} and \textsc{TheoDORE}. This file must reside in the same directory where the interface is started. It uses a simple ``\ttt{keyword argument}'' syntax. Comments using \# and blank lines are possible, the order of keywords is arbitrary. Lines with unknown keywords are ignored, since the interface just searches the file for certain keywords.

Table~\ref{tab:adf_sh2} lists the existing keywords.
A fully commented resource file with all possible options and comprehensive descriptions is located in \ttt{\$SHARC/../examples/SHARC\_ADF/}.

\begin{table}
  \centering
  \caption{Keywords for the \ttt{ADF.resources} file.}
  \label{tab:adf_sh2}
  \begin{tabular}{>{\ttfamily}lp{12cm}}
  \hline
  Keyword       &Description\\
  \hline
adfhome                 &Is the path to the \textsc{ADF} installation. Relative and absolute paths, environment variables and \ttt{\textasciitilde} can be used. The interface will set \ttt{\$ADFHOME} to this path, and will set the \ttt{\$ADFBIN}.
\\
scmlicense              &Is the path to the \textsc{ADF} license file. Relative and absolute paths, environment variables and \ttt{\textasciitilde} can be used. 
\\
scm\_tmpdir             &Path to the ADF-internal scratch directory. Usually, this is set at installation, but can be overridden here. Should not exist and must not be identical to scratchdir.
\\
scratchdir              &Is a path to the temporary directory. Relative and absolute paths, environment variables and \ttt{\textasciitilde} can be used. If it does not exist, the interface will create it. In any case, the interface will delete this directory after the calculation.
\\
savedir                 &Is a path to another temporary directory.  Relative and absolute paths, environment variables and \ttt{\textasciitilde} can be used. The interface will store files needed for restart there.
\\
memory                  &The memory usable by \textsc{WFoverlap}. The default is 100~MB. The ADF memory usage cannot be controlled.
\\
ncpu                    &The number of CPUs used by the interface. Is overridden by environment variables from queuing engines (e.g., \ttt{\$NSLOTS} or \ttt{\$SLURM\_NTASKS\_PER\_NODE}). Will either be used to run ADF in parallel or to run several independent ADF runs at the same time.
\\
schedule\_scaling       &Gives the expected parallelizable fraction of the ADF run time (Amdahl's law). With a value close to zero, the interface will try to run all jobs at the same time. With values close to one, jobs will be run sequentially with the maximum number of cores.
\\
delay                   &Followed by a float giving the delay in seconds between starting parallel jobs to avoid excessive disk I/O (usually not necessary).
\\
qmmm\_table             &Followed by the path to the connection table file, in ADF format.
\\
qmmm\_ff\_file          &Followed by the path to the force field file, in Amber95-like format for ADF.
\\
wfoverlap               &Path to the \textsc{WFoverlap} code. Needed for overlap and Dyson norm calculations.
\\
wfthres                 &(float) Gives the amount of wave function norm which will be kept in the truncation of the determinant files. Default is 0.99 (i.e., the wave functions in the determinant files will have a norm of 0.99). Note that if hybrid functionals and no TDA are used, the response vector can have a norm larger than one, and wfthres should be increased.
\\
numfrozcore             &Number of frozen core orbitals for overlap and Dyson norm calculations. A value of -1 enables automatic frozen core.
\\
numocc                  &Number of ignored occupied orbitals in Dyson calculations.
\\
nooverlap               &Do not save determinant files for overlap computations.
\\
theodir                 &Path to the \textsc{TheoDORE} installation. Relative and absolute paths, environment variables and \ttt{\textasciitilde} can be used. The interface will set \ttt{\$PYTHONPATH} automatically.
\\
theodore\_prop          &Followed by a list with the descriptors which \textsc{TheoDORE} should compute. Note that descriptors will only be computed for restricted singlets (and triplets). Instead of a simple list, a Python literal can also be used, as in the \textsc{TheoDORE} input files.
\\
theodore\_fragment      &Followed by a list of atom numbers which should constitute a fragment in \textsc{TheoDORE}. For multiple fragments, the keyword can be used multiple times. Instead, the keyword can be followed by a Python literal, as in the \textsc{TheoDORE} input files.
\\
always\_orb\_init       &Do not use the orbital guesses from previous calculations/time steps, but always use the provided initial orbitals.
\\
always\_guess           &Always use the orbital guess from ADF.
\\
  debug                 &Increases the verbosity of the interface (standard out). Does not clean up the scratch directory. Copies all ADF outputs to the save directory.
\\
  no\_print             &Reduces interface standard output.
\\
  \hline
  \end{tabular}
\end{table}

\paragraph{Parallelization}

ADF usually shows very good parallel scaling for most calculations.
However, it is more efficient to carry out multiple ADF calculations (different multiplicities, multiple gradients) in parallel, each one using a smaller number of CPUs.

In the \sharc-ADF interface, parallelization is controlled by the keywords \ttt{ncpu} and \ttt{schedule\_scaling}.
The first keyword controls the maximum number of CPUs which the interface is allowed to use for all ADF runs simultaneously.
The second keyword is the parallel fraction from Amdahl's Law, see Section~\ref{met:amdahl}.
With a value close to zero, the interface will try to run all jobs at the same time. With values close to one, jobs will be run sequentially with the maximum number of cores.
Typical values for \ttt{schedule\_scaling} are 0.95 for GGA functionals, 0.75 for hybrid functionals, and 0.90 for hybrid functions in combination with the \ttt{rihartreefock} option.

Note that it is very advantageous to set \ttt{schedule\_scaling} appropriately if the ADF installation is older than version 2017.207, because in this case all gradients need to be done in separate ADF runs, leading to a relatively large number of ADF runs.
If one uses ADF2018 (or ADF$\geq$2017.207), then multiple gradients can be computed in one ADF run, so that in many cases only one ADF run is carried out and \ttt{schedule\_scaling} becomes irrelevant.





\subsection{QM/MM force field file: \ttt{ADF.qmmm.ff}}

The force field file for ADF contains all force field parameters for the MM part of the QM/MM calculation.
The interface uses this file without any modification, except that it will copy the file into the relevant scratch directory.

The force field file needs to be in the format specified in the \link{https://www.scm.com/doc/QMMM/ADF_QMMM/The_Force_Field_File.html}{ADF manual}, which is similar in format to an \textsc{Amber} parameter file.
\ttt{\$SHARC/../examples/SHARC\_ADF\_qmmm/ADF.qmmm.ff} contains a functioning example of the file format.
Here, we only show briefly the general format:
\begin{example}
  \begin{verbatim}
FORCE_FIELD_SETTINGS
=================================
 ELSTAT_1-4_SCALE       0.8333
 ELSTAT_NB_CUTOFF       none
 VDW_1-4_SCALE          0.5
 VDW_DEFAULT_POTENTIAL  1         (1:6-12   2:exp-6  3:exp purely repulsive)
 VDW_NB_CUTOFF          none
 DIELECTRIC_CONSTANT    1.000 
=================================


MASSES & ATOM LABELS
--------------------------------
force_field  atomic
 atom_type   symbol    mass        NOTES
================================
    BR         Br     79.9000      bromine
================================


BONDS   Ebond = 0.5*K(r-ro)**2    
--------------------------------
 Atoms    pot   
i  -   j  type     K      R       NOTES
==============================
C    CT    1     634.00  1.522    JCC,7,(1986),230; AA
==============================


BENDS    Ebend = 0.5*k(a-ao)^2   (K in kcal/mol*rad^2)
--------------------------------
    Atoms     pot 
i  -  j - k  type     K     theta   NOTES
===================================
H1   CT   HC   1    70.00   109.50  M. Swart added
===================================


TORSIONS     Etors=K(1+cos(nt-to))
--------------------------------
    Atoms           pot            per.   shift     
i   - j  - k -  l  type     k       n      to      NOTES
=================================================
*    C    CT   *     1     0.0000    2       0.0   JCC,7,(1986),230  
=================================================


OUT-OF-PLANE       Etors=K(1-cos(2t-to))
--------------------------------
      Atoms        pot      
 i - j  - k  - l   type   K     to     NOTES
=====================================
*    *    CC   *     1   1.00  180.0   HEME improper
=====================================

VAN DER WAALS
--------------------------------
  atom(s)     Emin    Rmin   gamma     NOTES
====================================
    H       0.0157   1.2000  12.00     Ferguson base pair geom.
====================================

CHARGES
--------------------------------
 type      charge(e)    NOTES
=====================
  OW       -0.8340      Amber95 water charges (TIP3P had -0.82)
=====================
\end{verbatim}
\end{example}


\subsection{QM/MM connection table file: \ttt{ADF.qmmm.table}}

The connection table file defines the topology of the QM/MM system, by defining which atoms belong to QM or MM, and which atoms have MM bonds between them.
The file can also be used to define link atoms or to override the MM charges of specific atoms.

In the ADF input file, the interface will automatically add the line \ttt{MM\_CONNECTION\_TABLE} (so this line should not be present in \ttt{ADF.qmmm.table}), then copy the content of \ttt{ADF.qmmm.table} verbatim to the ADF input, and then add a line \ttt{END}.
Hence, \ttt{ADF.qmmm.table} needs to follow the format required by ADF.
% One should keep in mind that the content of the file is copied verbatim to the ADF input, inside the \ttt{MM\_CONNECTION\_TABLE} subblock of the QM/MM input section.
See the \link{https://www.scm.com/doc/QMMM/ADF_QMMM/QMMM_keyblock_options.html}{ADF manual} for details on this input section.

An example of the file is given in \ttt{\$SHARC/../examples/SHARC\_ADF\_qmmm/ADF.qmmm.table}.
In the simplest form, the content of the file could look like:
\begin{example}
  \begin{verbatim}
1       SO      QM      2
2       C       QM      1       3       4
3       H1      QM      2
4       H1      QM      2
5       OW      MM      6      7
6       HW      MM      5
7       HW      MM      5
\end{verbatim}
\end{example}
Here, the first column is the atom index, the second is the MM atom type (as defined in the force field file), the third column is the atom designation (either \ttt{QM}, \ttt{MM}, or \ttt{LI} for a link atom), and the remaining entries are the atom indices to which the current atom is bonded in MM.
Note that in the connection table, all \ttt{MM} atoms must come at the very end.

As the file content is copied verbatim, it is possible to add further sublocks to the QM/MM input section.
This is necessary if any of the atoms is designated as link atom (label \ttt{LI}).
In this case, the \ttt{LINK\_BONDS} section needs to be added:
\begin{example}
  \begin{verbatim}
1       SO      QM      2
2       C       QM      1       3       4
3       HP      QM      2
4       CT      QM      2       5       6       7
5       H1      QM      4
6       H1      QM      4
7       CT      LI      4       8       9       10
8       HC      MM      7
9       HC      MM      7
10      HC      MM      7
  subend
  link_bonds
7 - 4  1.4  H.1  HC
\end{verbatim}
\end{example}
Note how the \ttt{MM\_CONNECTION\_TABLE} block is terminated by \ttt{subend} and then the \ttt{LINK\_BONDS} section starts; the \ttt{LINK\_BONDS} section is not terminated by \ttt{subend} because the interface adds a \ttt{subend} after the file content.
Within the \ttt{LINK\_BONDS} section, each line has the following structure: (i) atom index of the \ttt{LI} atom; (ii) a \ttt{-} separated by spaces; (iii) atom index of a \ttt{QM} atom bonded to the link atom; (iv) a floating point number giving the parameter $f$; (v) the atomic fragment type of the atom which replaces the \ttt{LI} atom in the QM calculation; (vi) the MM atom type of the replacing atom.
Given these parameters, internally ADF will replace the \ttt{LI} atom by an atom of the specified atomic fragment type, which will be placed on the \ttt{QM}--\ttt{LI} bond where the new bond length is the old bond length divided by $f$.

The file can also be used to override the MM charges for specific atoms:
\begin{example}
  \begin{verbatim}
1       SO      QM      2
2       C       QM      1       3       4
3       H1      QM      2
4       H1      QM      2
5       OW      MM      6      7
6       HW      MM      5
7       HW      MM      5
 subend
 charges
5  -0.96
6  +0.48
7  +0.48
\end{verbatim}
\end{example}
The same rules regarding the \ttt{subend} as above apply.


\subsection{Input file generator: \ttt{ADF\_input.py}}\label{sec:ADF_input.py}

In order to quickly setup simple calculations using ADF, the \sharc\ suite contains a small script called \ttt{ADF\_input.py}. It can be used to setup single point calculations, optimizations, and frequency calculations on the DFT and TD-DFT level of theory. Of course, ADF has far more capabilities, but these are not covered by \ttt{ADF\_input.py}. For more complicated inputs, users should manually adjust the generated input or employ \ttt{adfinput}.

The script interactively asks the user to specify the calculation and writes an input file and optionally a run script.

\subsubsection{Input}

\paragraph{Type of calculation}

Choose to either perform a single-point calculation or a minimum optimization (including optionally frequency calculation).

The standard output or \ttt{TAPE21} files from a frequency calculation can be converted (see section~\ref{sec:ADF_freq.py}) to a file in \textsc{Molden} format, which can then be used to generate initial conditions with \ttt{wigner.py}.

\paragraph{Geometry, Charge, Multiplicity}

Specify the geometry file in xyz format. Number of atoms and total nuclear charge is detected automatically. After the user inputs the total charge, the number of electrons is calculated automatically. The number of unpaired electrons can be specified to define the multiplicity.

\paragraph{Basis set}

With \ttt{ADF\_input.py} it is only possible to enter a single basis set for all atoms. More complicated basis setups can be achieved by manually adjusting the generated input file.

\paragraph{Level of theory}

One can setup calculations with any ADF-supported functional, and for ground state or excited state. The user is also queried whether relativistic effects need to be included, and for the most important accuracy settings.

% \paragraph{Run script}
% 
% If requested, the script also generates a simple Bash script (\ttt{run\_molpro.sh}) to directly execute ADF. The user has to enter the path to ADF and the path to a suitable (fast) scratch directory. 
% 
% Note that the scratch directory will be deleted after the calculation, only the wave function file \ttt{wf} will be copied back to the main directory.



\subsection{Frequencies converter: \ttt{ADF\_freq.py}}\label{sec:ADF_freq.py}

The small script \ttt{ADF\_freq.py} can be used to convert the standard output or the \ttt{TAPE21} file created by an ADF frequency calculation.
The usage is very simple:
\begin{example}
  \begin{verbatim}
$SHARC/ADF_freq.py ADF.out
\end{verbatim}
\end{example}
or:
\begin{example}
  \begin{verbatim}
$SHARC/ADF_freq.py TAPE21
\end{verbatim}
\end{example}
The script detects automatically the file format.
Note that in ADF, the infrared intensities are only accessible from the standard output, so use this for IR spectrum generation.
However, the data in \ttt{TAPE21} has a higher numeric precision, so it is recommended to convert the \ttt{TAPE21} file if no intensities are needed.
In any case, a file called \ttt{<filename>.molden} is written, containing the frequencies and normal modes.
This file can then be used with \ttt{wigner.py}.


% ========================================================================================================= %

\section{RICC2 Interface}\label{sec:int:ricc2}

The \sharc-\textsc{Ricc2} interface can be used to conduct excited-state dynamics based on \textsc{Turbomole}'s CC2 and ADC(2) methods, for singlet and triplet states.
The interface uses the programs \ttt{define}, \ttt{dscf} and \ttt{ricc2}.
For spin-orbit couplings, \textsc{Orca} needs to be installed in addition to \textsc{Turbomole}.
Only ADC(2) can be used to calculate spin-orbit couplings, but not CC2 (hence, it is not recommended to perform CC2 calculations with both singlet and triplet states).
Even with ADC(2), only singlet-triplet SOCs are obtained, but no triplet-triplet SOCs; $S_0$-triplet SOCs are also missing currently.
Wavefunction overlaps are calculated using the \textsc{WFoverlap} code of the Gonz\'alez group.\cite{Plasser2016JCTC}

The interface needs two additional input files, a template file for the quantum chemistry (file name is \ttt{RICC2.template}) and a general input file (\ttt{RICC2.resources}). 
If a file \ttt{QM/mos.init} is are present, it is used to provide an initial orbital guess for the SCF calculation.


\subsection{Template file: \ttt{RICC2.template}}

This file contains the specifications for the quantum chemistry calculation. Note that this is not a valid \textsc{Turbomole} input file. The file only contains a number of keywords, given in table~\ref{tab:ricc2_temp}. The actual input for \textsc{Turbomole} will be generated automatically through \ttt{define}.
A fully commented template file with all possible options is located in \ttt{\$SHARC/../examples/SHARC\_RICC2/}.

\begin{table}
  \centering
  \caption{Keywords for the \ttt{RICC2.template} file.}
  \label{tab:ricc2_temp}
  \begin{tabular}{>{\ttfamily}lp{12cm}}
  \hline
  Keyword       &Description\\
  \hline
basis           &The basis set used. The interface will convert this string to the correct case for \textsc{Turbomole}.\\
auxbasis        &The auxiliary basis set used in \ttt{ricc2}. If no auxbasis is given, the interface will let \ttt{define} decide on a suitable auxbasis.\\
basislib        &Path to external basis set library. Can be used to employ custom basis sets.\\
charge          &The total molecular charge. The interface will calculate the number of electrons automatically during setup.\\
method          &Followed by a string, which is either ``CC2'' or ``ADC(2)'' (case insensitive), defining the level of theory. Default is ``ADC(2)''.\\
douglas-kroll   &Activates the use of the scalar-relativistic DK Hamiltonian of 2nd order. Default (if no keyword is given) is to use the non-relativistic Hamiltonian.\\
spin-scaling    &Followed by a string, which is either ``none'', ``scs'' or ``sos''. Using these options, spin-component scaling can be activated. Under certain restrictions (no SOC, no transition dipole moments, no SMP), ``lt-sos'' can be used to perform cheaper ``sos'' calculations.\\
scf             &Followed by a string which is either ``dscf'' or ``ridft''. Using this option, the SCF program can be chosen. Note that currently, there is no advantage of using ``ridft''.\\
frozen          &Followed by an integer giving the number of frozen core orbitals in the \ttt{ricc2} calculations. Default is to use frozen core orbitals and let \ttt{define} decide on the number. If frozen core is not wanted, use \ttt{frozen 0} in the template.\\
  \hline
  \end{tabular}
\end{table}

\subsubsection{External basis set libary}

If users want to employ their own basis sets, they can create a basis set library directory with the required files, and use the \ttt{basislib} keyword to tell the interface to use this directory.
The \ttt{basislib} keyword cannot be used together with the \ttt{auxbasis} keyword.

The specified directory must contain \ttt{basen/} and \ttt{cbasen/} subdirectories. These must contain one file per element, containing the desired basis set parameters.
The files in \ttt{cbasen/} must auxiliary basis sets of the same name as the basis sets in \ttt{basen/}.
See the \textsc{Turbomole} directory structure to see how the directories and files need to be prepared.

% \todo{Explain basislib keyword. Cannot be used together with auxbasis keyword. External library must have basen and cbasen subdirectries. These must contain one file per element with the desired basis set. cbasen must contain one file per element with the custom auxbasis, which must have the same name as the basis.}



\subsection{Resource file: \ttt{RICC2.resources}}

The file \ttt{RICC2.resources} contains mainly paths (to the \textsc{Turbomole} and \textsc{Orca} executables, to the scratch directory, etc.). This file must reside in the same directory where the interface is started. It uses a simple ``\ttt{keyword argument}'' syntax. Comments using \# and blank lines are possible, the order of keywords is arbitrary. Lines with unknown keywords are ignored, since the interface just searches the file for certain keywords.

Table~\ref{tab:ricc2_sh2} lists the existing keywords.
A fully commented resource file with all possible options is located in \ttt{\$SHARC/../examples/SHARC\_RICC2/}.

\begin{table}
  \centering
  \caption{Keywords for the \ttt{RICC2.resources} file.}
  \label{tab:ricc2_sh2}
  \begin{tabular}{>{\ttfamily}lp{12cm}}
  \hline
  Keyword       &Description\\
  \hline
turbodir        &Is the path to the \textsc{Turbomole} installation. Relative and absolute paths, environment variables and \ttt{\textasciitilde} can be used. The interface will set \ttt{\$TURBODIR} to this path, and will set the \ttt{\$PATH} correctly (using \textsc{Turbomole}'s \ttt{sysname} tool. If this keyword is not present in \ttt{RICC2.resources}, the interface will use the environment variable \ttt{\$TURBODIR}, if it is set.\\
orcadir         &Is the path to the \textsc{Orca} installation, which is necessary when spin-orbit couplings are calculated. Relative and absolute paths, environment variables and \ttt{\textasciitilde} can be used. If this keyword is not present in \ttt{RICC2.resources}, the interface will use the environment variable \ttt{\$ORCADIR}, if it is set.\\
scratchdir      &Is a path to the temporary directory. Relative and absolute paths, environment variables and \ttt{\textasciitilde} can be used. If it does not exist, the interface will create it. In any case, the interface will delete this directory after the calculation.\\
savedir         &Is a path to another temporary directory.  Relative and absolute paths, environment variables and \ttt{\textasciitilde} can be used. The interface will store files needed for restart there.\\
memory          &The memory usable by \textsc{Turbomole} and \textsc{WFoverlap}. The default is 100~MB.\\
ncpu            & The number of CPUs used by the interface. If larger than one, the interface will use the SMP executables of \textsc{Turbomole} with the given number of CPUs.\\
always\_orb\_init &Do not use the orbital guesses from previous calculations/time steps, but always use the provided initial orbitals.\\
always\_guess   &Always use the orbital guess from the EHT calculation of \ttt{define}.\\
dipolelevel     &Followed by an integer which is either 0, 1, or 2. Controls which dipole moment calculations are skipped by the interface.\\
wfoverlap       &Is the path to the \textsc{WFoverlap} executable. Relative and absolute paths, environment variables and \ttt{\textasciitilde} can be used. This keyword is only necessary if overlaps need to be calculated.\\
wfthres         &Threshold for the truncation of the response vectors which are used in the overlap calculations. A threshold of $x$ implies that only the minimal number of determinants needed to give a norm of $1-x$ are included.\\
numfrozcore     &Overrides the number of frozen core orbitals for the overlap calculation. Default is to use the frozen orbitals from the RICC2 steps.\\
nooverlap       &Do not save files necessary to do overlaps.\\
debug           &Increases the verbosity of the interface.\\
  no\_print       &Reduces interface standard output.\\
  \hline
  \end{tabular}
\end{table}

Note that the interface sets all environment variables necessary to run \textsc{Turbomole} (e.g., \ttt{\$PATH}) automatically, based on the input from \ttt{RICC2.resources} and \ttt{QM.in}.

For parallel calculations, the interface will call the SMP executables of \textsc{Turbomole} and \textsc{WFoverlap}.

Note that the \ttt{dipolelevel} keyword can have significant impact on the calculation time.
Generally, in response methods like CC2 and ADC(2), extra computational effort is required for the calculation of state and transition dipole moments .
However, dipole moments have only influence in the dynamics simulations if a laser field is present.
Using the \ttt{dipolelevel} keyword, it is possible to deactivate dipole moment calculations if they are not required.
There are three different settings for \ttt{dipolelevel}: 
\begin{itemize}
  \item \ttt{dipolelevel}=0: The interface will return only dipole moments which can be calculated at no cost (state dipole moments of states where a gradient is calculated; excited-excited transition dipole moments if SOCs are calculated)
  \item \ttt{dipolelevel}=1: In addition, the interface will calculate transition dipole moments between $S_0$ and excited singlet states. Use at least this level for the initial condition setup (\ttt{setup\_init.py} takes care of this).
  \item \ttt{dipolelevel}=2: The interface will calculate all state and transition dipole moments
\end{itemize}
If only energies and dipole moments are calculated, \ttt{dipolelevel}=1 is only slightly more expensive than \ttt{dipolelevel}=0, while \ttt{dipolelevel}=2 increases computation time more strongly.
However, the computation time also depends on whether or not spin-orbit couplings and gradients are calculated. 




% ========================================================================================================= %

\section{LVC Interface}\label{sec:int:lvc}

The purpose of the LVC interface is to allow performing computationally efficient dynamics using a linear vibronic coupling (LVC) model \cite{Koeppel84ACP}.
In the vibronic coupling model the diabatic energy and coupling matrix $\mathbf{V}$ is constructed as
\begin{equation}
\label{eq:V}
\mathbf{V}=
V_0\mathbf{1}+\mathbf{W}
\end{equation}

To construct these matrices, the Cartesian coordinate displacements $r_{\alpha}$ (relative to a reference geometry) are first converted to dimensionless mass-frequency scaled normal coordinates, which are given as
%
\begin{equation}
\label{eq:Qi}
Q_i=\sqrt{\omega_i}\sum_\alpha K_{\alpha i}\sqrt{M_\alpha}r_{\alpha}
\end{equation}
%
where $\omega_i$ is the frequency of normal mode $i$, $M_\alpha$ is an atomic mass, and $K_{\alpha i}$ denotes the conversion matrix between mass-weighted Cartesian and normal coordinates.
Using these coordinates, the harmonic ground state potential is given as
%
\begin{equation}
\label{eq:V0Qi}
V_0= \sum_i \dfrac{\omega_i}{2}Q_i^2
.
\end{equation}
%
In the case of the linear vibronic coupling model, one additionally considers the following state-specific terms that constitute the $\mathbf{W}$ matrix.
%
\begin{align}
\label{eq:Wnn}
W_{nn}&=\epsilon_n + \sum_i\kappa_i^{(n)}Q_i\\
\label{eq:Wmn}
W_{mn}&=\eta_{mn}+\sum_j\lambda_j^{(m,n)}Q_j
\end{align}
%
Here the $\epsilon_n$ are the vertical excitation energies, the $\eta_{mn}$ are the SOC constants, and the $\kappa_i^{(n)}$ and $\lambda_j^{(m,n)}$ are termed intrastate and interstate vibronic coupling constant \cite{Koeppel84ACP}.

\subsection{Input files}

Two input files are needed:

\paragraph{\ttt{V0.txt}} Contains all information to describe $V_0$: the equilibrium geometry, the frequencies $\omega_i$, and an orthogonal matrix containing the normal coordinates $K_{\alpha i}$.
\begin{example}
\begin{verbatim}
Geometry
 S     16.    0.00000000    0.00000000   -0.00039079   31.97207180
 O      8.    0.00000000   -2.38362453    1.36159121   15.99491464
 O      8.    0.00000000    2.38362453    1.36159120   15.99491464
Frequencies
 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.00234  0.0053  0.0064
Mass-weighted normal modes
  0.0000  1.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000
  0.6564  0.0000  0.0000  0.3108  0.0494  0.2004  0.0000  0.0000 -0.6557
...
\end{verbatim}
\end{example}
\ttt{V0.txt} can be created from a \textsc{Molden} file using \ttt{wigner.py}.

\paragraph{\ttt{LVC.template}} Contains the state-specific information: $\epsilon_i$, $\kappa_i^{(n)}$, $\lambda_j^{(m,n)}$, $\eta_{mn}$, as well as (transition) dipole moments.
Here, for multiplets $\epsilon_i$, $\kappa_i^{(n)}$, and $\lambda_j^{(m,n)}$ are shared between the multiplet components, whereas in the SOC and dipole moment matrices the multiplet components need to be considered explicitly.
\begin{example}
\begin{verbatim}
V0.txt
4 0 3 
epsilon
7
  1   1  0.0000000000
  1   2  0.1640045037
  1   3  0.1781507393
  1   4  0.2500917150
  3   1  0.1341247878
  3   2  0.1645714941
  3   3  0.1700512159
kappa
12
  1   2     7  1.19647e-03
  1   2     8  1.21848e-02
...
lambda
3
  1   1   4     9 -1.83185e-02
  1   2   3     9  7.32022e-03
  3   1   3     9  5.71746e-03
SOC R
  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00 ...
  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00 -1.086e-04 ...
...
SOC I
  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00 ...
  0.000e+00  0.000e+00  0.000e+00  1.000e+00  0.000e+00  1.000e-04 ...
...
DMX R
 -7.400e-07 -1.639e-01  0.000e+00  3.000e-06  0.000e+00  0.000e+00  ...
 -1.639e-01  3.930e-06  0.000e+00  2.400e-05  0.000e+00  0.000e+00  ...
...
DMX I
...
DMY R
...
DMY I
...
DMZ R
...
DMZ I
...
\end{verbatim}
\end{example}
Only the two first lines are mandatory, but then all states will have the same potentials (the ground state potential).
This file can be prepared with \ttt{QMout2LVC.py}.


\subsection{Template File Setup: \ttt{wigner.py}, \ttt{setup\_LVCparam.py}, and \ttt{create\_LVCparam}}\label{sec:setup_LVCparam.py}

\paragraph{Reference potential}

\ttt{V0.txt} is created using the \ttt{wigner.py} script, which is also used for initial condition generation. Simply call, e.g.
\begin{example}
\ttt{\$SHARC/wigner.py -l <filename.molden>}
\end{example}
Note that this only works if all $3N$ normal modes are present in the file (unfortunately, some programs omit translations and rotations in the output).

\paragraph{Setup for single point calculations}

To obtain the LVC parameters, two steps are necessary: (i) running quantum chemistry calculations, and (ii) converting the quantum chemistry output to the LVC parameters.

The first of these steps is carried out with \ttt{setup\_LVCparam.py}.
It is an interactive script that works very similarly to the other setup scripts (e.g., \ttt{setup\_init.py}, \ttt{setup\_traj.py}).
The script will ask for the following:
\begin{itemize}
  \item Path to the \ttt{V0.txt} file,
  \item Number of states,
  \item Which interface to use (in principle, all \sharc-interfaces can be used, but only the ab initio interfaces are useful here),
  \item Whether spin-orbit couplings should be calculated (only if applicable),
  \item Whether $\kappa$ parameters should be obtained from analytical gradients or numerical differentiation (depends on availability of analytical gradients),
  \item Whether $\lambda$ parameters should be obtained from analytical nonadiabatic coupling vectors or numerical differentiation (depends on availability of analytical nonadiabatic coupling vectors),
  \item Which normal modes to include,
  \item Which displacement value to use for numerical differentiation (default 0.05 dimensionless mass-frequency scaled units),
  \item Whether intruder states should be ignored or not,
  \item Whether one- or two-sided differentiation should be done,
  \item Interface-specific input for the chosen quantum chemistry interface (see section~\ref{sec:setup_init.py:several}).
\end{itemize}
The script will set up a directory (\ttt{DSPL\_RESULTS}) with one subdirectory for each single point calculation.
If both $\kappa$'s and $\lambda$'s are computed analytically, then only the \ttt{DSPL\_000\_eq} subdirectory will be present.
Otherwise, for each chosen normal mode 1--2 subdirectories (for one-sided or two-sided differentiation) will be present.
Additionally, \ttt{displacements.log} presents the most important settings.
In all cases, \ttt{displacements.json} is also present; this file is crucial to communicate all settings to the read-out script after the single point jobs are finished.

\paragraph{Extracting LVC parameters}

After the single point jobs are finished, run \ttt{create\_LVCparam.py} inside the \ttt{DSPL\_RESULTS} directory.
This is a fully automatic script that reads \ttt{displacements.json} and the \ttt{QM.out} files in the subdirectories.
After everything is successfully read, it creates the \ttt{LVC.template} file.
This file can be used to run SHARC-LVC trajectories.




% \subsection{Preparing Template Files: \ttt{wigner.py} and \ttt{QMout2LVC.py}}\label{sec:QMout2LVC.py}
% 
% Both input files can be conveniently created using the \sharc\ tools. \ttt{V0.txt} is created using the \ttt{wigner.py} script, which is also used for initial condition generation. Simply call, e.g.
% \begin{example}
% \ttt{\$SHARC/wigner.py -l <filename.molden>}
% \end{example}
% Note that this only works if all $3N$ normal modes are present in the file (unfortunately, some programs omit translations and rotations in the output).
% 
% To obtain the LVC parameters, it is first necessary to perform a single-point computation at the ground state equilibrium geometry.
% Using the \ttt{QM.in} file, you can specify, which types of properties to compute. For the full LVC model, \ttt{SOC}, \ttt{DM}, \ttt{Grad}, and \ttt{NACDR} are needed, so use
% \begin{example}
% \begin{verbatim}
% 3
% 
%  S    0.00000000    0.00000000   -0.00039079 
%  O    0.00000000   -2.38362453    1.36159121 
%  O    0.00000000    2.38362453    1.36159120 
% Unit Bohr
% States           4           0           3
%  INIT
%  SOC
%  DM
%  Grad
%  NACDR
% \end{verbatim}
% \end{example}
% 
% Copy the \ttt{V0.txt} to the same directory containing the \ttt{QM.out} file of this computation and run
% \begin{example}
% \ttt{\$SHARC/QMout2LVC.py}
% \end{example}
% to create the file \ttt{LVC.template}.
% Note that currently, the script only works for singlets and triplets.

% ========================================================================================================= %

\section{\textsc{Gaussian} Interface}\label{sec:int:gaussian}

The \sharc-\textsc{Gaussian} interface allows to run \sharc\ simulations with \textsc{Gaussian}'s TD-DFT functionality.
The interface is compatible with restricted and unrestricted ground states, but not with symmetry.
Spin-orbit couplings cannot be computed, but wave function overlaps from the \textsc{WFoverlap} code are available (no nonadiabatic couplings).
Dyson norms can also be computed through the \textsc{WFoverlap} code.
\textsc{TheoDORE} can be used to perform automatic wave function analysis.

The interface needs two additional input files, a template file for the quantum chemistry (file name is \ttt{GAUSSIAN.template}) and a resource file (\ttt{GAUSSIAN.resources}). 
If files \ttt{QM/GAUSSIAN.chk.init} or \ttt{QM/GAUSSIAN.chk.<job>.init} are present, they are used to provide an initial orbital guess for the SCF calculation of the respective job.

\subsection{Template file: \ttt{GAUSSIAN.template}}

This file contains the specifications for the quantum chemistry calculation. Note that this is not a valid \textsc{Gaussian} input file. The file only contains a number of keywords, given in table~\ref{tab:gauss_temp}. The actual input for \textsc{Gaussian} will be generated automatically through the interface.

A fully commented template file---with all possible options, a comprehensive descriptions, and some practical hints---is located in \ttt{\$SHARC/../examples/SHARC\_GAUSSIAN/GAUSSIAN.template}.
We recommend that users start from this template file and modify it appropriately for their calculations.

\begin{table}
  \centering
  \caption{Keywords for the \ttt{GAUSSIAN.template} file.}
  \label{tab:gauss_temp}
  \small
  \begin{tabular}{>{\ttfamily}lp{12cm}}
  \hline
  Keyword       &Description\\
  \hline
basis                   &Gives the basis set for all atoms (default 6-31G).
\\
functional              &followed by one string giving the exchange-correlation functional. Default is \ttt{PBEPBE}.
\\
dispersion              &Activates dispersion correction. Arguments are written verbatim to \textsc{Gaussian} input (in \ttt{EmpiricalDispersion=()}). Default is no dispersion. An example argument would be \ttt{GD3}.
\\
charge                  &Sets the total charge of the system. Can be either followed by a single integer (then the interface will automatically assign the charges to the multiplicities) or by one charge per multiplicity. 
\\
scrf                    &Activates solvation. All arguments (e.g., \ttt{iefpcm solvent=water}) are copied to \textsc{Gaussian} input (in \ttt{scrf=()}).
\\
grid                    &Followed by a string (e.g., \ttt{grid finegrid}) defining which integration grid and accuracy to use. For details, see the example template file.
\\
denfit                  &Activates density fitting, which might speed up the computation.
\\
scf                     &Arguments are written verbatim to \textsc{Gaussian} input (in \ttt{scf=()}).
\\
no\_tda                 &This keyword deactivates TDA, which the interface requests by default.
\\
paddingstates           &Followed by a list of integers, which give the number of extra states to compute by \textsc{Gaussian}, but which are neglected in the output. Should not be changed between time steps, as this will break \textsc{Gaussian}'s restart routines.
\\
unrestricted\_triplets  &Requests that the triplets are calculated in a separate job from an unrestricted ground state. Default is to compute triplets as linear response of the restricted singlet ground state.
\\
iop                     &Arguments are written verbatim to \textsc{Gaussian} input (in \ttt{iop=()}). Expert option.
\\
keys                    &Arguments are written verbatim to \textsc{Gaussian} input as separate keywords. Expert option.
\\
  \hline
  \end{tabular}
\end{table}

\subsection{Resource file: \ttt{GAUSSIAN.resources}}

The file \ttt{GAUSSIAN.resources} contains mainly paths (to the \textsc{Gaussian} executables, to the scratch directory, etc.) and other resources, plus settings for \ttt{wfoverlap.x} and \textsc{TheoDORE}. This file must reside in the same directory where the interface is started. It uses a simple ``\ttt{keyword argument}'' syntax. Comments using \# and blank lines are possible, the order of keywords is arbitrary. Lines with unknown keywords are ignored, since the interface just searches the file for certain keywords.

Table~\ref{tab:gauss_sh2} lists the existing keywords.
A fully commented resource file with all possible options and comprehensive descriptions is located in \ttt{\$SHARC/../examples/SHARC\_GAUSSIAN/}.

\begin{table}
  \centering
  \caption{Keywords for the \ttt{GAUSSIAN.resources} file.}
  \label{tab:gauss_sh2}
  \begin{tabular}{>{\ttfamily}lp{12cm}}
  \hline
  Keyword       &Description\\
  \hline
groot                   &Is the path to the \textsc{Gaussian} installation. Relative and absolute paths, environment variables and \ttt{\textasciitilde} can be used. The interface will set \ttt{\$GAUSS\_EXEDIR} to this path. 
Note that this needs to be the path to the directory containing the \textsc{Gaussian} executables, e.g., \ttt{g09}/\ttt{g16}, \ttt{l9999.exe}, etc.
\\
scratchdir              &Is a path to the temporary directory. Relative and absolute paths, environment variables and \ttt{\textasciitilde} can be used. If it does not exist, the interface will create it. In any case, the interface will delete this directory after the calculation. The interface will set \ttt{\$GAUSS\_SCRDIR} to this path.
\\
savedir                 &Is a path to another temporary directory.  Relative and absolute paths, environment variables and \ttt{\textasciitilde} can be used. The interface will store files needed for restart there.
\\
memory                  &The memory usable by \textsc{Gaussian} and \textsc{WFoverlap}. The default is 100~MB. 
\\
ncpu                    &The number of CPUs used by the interface. Is overridden by environment variables from queuing engines (e.g., \ttt{\$NSLOTS} or \ttt{\$SLURM\_NTASKS\_PER\_NODE}). Will either be used to run \textsc{Gaussian} in parallel or to run several independent \textsc{Gaussian} runs at the same time.
\\
schedule\_scaling       &Gives the expected parallelizable fraction of the \textsc{Gaussian} run time (Amdahl's law). With a value close to zero, the interface will try to run all jobs at the same time. With values close to one, jobs will be run sequentially with the maximum number of cores.
\\
delay                   &Followed by a float giving the delay in seconds between starting parallel jobs to avoid excessive disk I/O (usually not necessary).
\\
wfoverlap               &Path to the \textsc{WFoverlap} code. Needed for overlap and Dyson norm calculations.
\\
wfthres                 &(float) Gives the amount of wave function norm which will be kept in the truncation of the determinant files. Default is 0.99 (i.e., the wave functions in the determinant files will have a norm of 0.99). Note that if hybrid functionals and no TDA are used, the response vector can have a norm larger than one, and wfthres should be increased.
\\
numfrozcore             &Number of frozen core orbitals for overlap and Dyson norm calculations. A value of -1 enables automatic frozen core.
\\
numocc                  &Number of ignored occupied orbitals in Dyson calculations.
\\
nooverlap               &Do not save determinant files for overlap computations.
\\
theodir                 &Path to the \textsc{TheoDORE} installation. Relative and absolute paths, environment variables and \ttt{\textasciitilde} can be used. The interface will set \ttt{\$PYTHONPATH} automatically.
\\
theodore\_prop          &Followed by a list with the descriptors which \textsc{TheoDORE} should compute. Note that descriptors will only be computed for restricted singlets (and triplets). Instead of a simple list, a Python literal can also be used, as in the \textsc{TheoDORE} input files.
\\
theodore\_fragment      &Followed by a list of atom numbers which should constitute a fragment in \textsc{TheoDORE}. For multiple fragments, the keyword can be used multiple times. Instead, the keyword can be followed by a Python literal, as in the \textsc{TheoDORE} input files.
\\
always\_orb\_init       &Do not use the orbital guesses from previous calculations/time steps, but always use the provided initial orbitals.
\\
always\_guess           &Always use the orbital guess from \textsc{Gaussian}.
\\
  debug                 &Increases the verbosity of the interface (standard out). Does not clean up the scratch directory. Copies all \textsc{Gaussian} outputs to the save directory.
\\
  no\_print             &Reduces interface standard output.
\\
  \hline
  \end{tabular}
\end{table}

\paragraph{Parallelization}

\textsc{Gaussian} usually shows very good parallel scaling for most TD-DFT calculations.
However, it is more efficient to carry out multiple \textsc{Gaussian} calculations (different multiplicities, multiple gradients) in parallel, each one using a smaller number of CPUs.

In the \sharc-\textsc{Gaussian} interface, parallelization is controlled by the keywords \ttt{ncpu} and \ttt{schedule\_scaling}.
The first keyword controls the maximum number of CPUs which the interface is allowed to use for all \textsc{Gaussian} runs simultaneously.
The second keyword is the parallel fraction from Amdahl's Law, see Section~\ref{met:amdahl}.
With a value close to zero, the interface will try to run all jobs at the same time. With values close to one, jobs will be run sequentially with the maximum number of cores.
Typical values for \ttt{schedule\_scaling} are around 0.90 for both GGA functionals and hybrid functionals,  possibly less for very small computations.









% ========================================================================================================= %

\section{\textsc{Orca} Interface}\label{sec:int:orca}

The \sharc-\textsc{Orca} interface allows to run \sharc\ simulations with \textsc{Orca}'s TD-DFT functionality.
The interface is compatible with restricted and unrestricted ground states, but not with symmetry.
Spin-orbit couplings can be computed and wave function overlaps from the \textsc{WFoverlap} code are available (no nonadiabatic couplings).
Dyson norms can also be computed through the \textsc{WFoverlap} code.
\textsc{TheoDORE} can be used to perform automatic wave function analysis.
Note that the interface only works with \textsc{Orca} 4.1 and newer (you can check by looking whether the program \ttt{orca\_fragovl} is present in the \textsc{Orca} installation).

The \sharc-\textsc{Orca} interface furthermore allows to perform QM/MM dynamics (TDDFT plus force fields), using \textsc{Tinker} for the MM part.

The interface needs two additional input files, a template file for the quantum chemistry (file name is \ttt{ORCA.template}) and a resource file (\ttt{ORCA.resources}). 
If files \ttt{QM/ORCA.gbw.init} or \ttt{QM/ORCA.gbw.<job>.init} are present, they are used to provide an initial orbital guess for the SCF calculation of the respective job.
In the case of QM/MM calculations, two more input files are needed: an \textsc{Amber}95 force field file, and \ttt{ORCA.qmmm.table}, which contains the connectivity and force field IDs per atom.

\subsection{Template file: \ttt{ORCA.template}}

This file contains the specifications for the quantum chemistry calculation. Note that this is not a valid \textsc{Orca} input file. The file only contains a number of keywords, given in table~\ref{tab:orca_temp}. The actual input for \textsc{Orca} will be generated automatically through the interface.

A fully commented template file---with all possible options, a comprehensive descriptions, and some practical hints---is located at \ttt{\$SHARC/../examples/SHARC\_ORCA/ORCA.template}.
For QM/MM calculations, a second example template (along with the QM/MM-specific files) is located at \ttt{\$SHARC/../examples/SHARC\_ORCA\_Tinker/ORCA.template}.
We recommend that users start from this template file and modify it appropriately for their calculations.

\begin{table}
  \centering
  \caption{Keywords for the \ttt{ORCA.template} file.}
  \label{tab:orca_temp}
  \small
  \begin{tabular}{>{\ttfamily}lp{12cm}}
  \hline
  Keyword       &Description\\
  \hline
basis                   &Gives the basis set for all atoms (default 6-31G).
\\
auxbasis                &Gives the auxiliary basis set (default: Orca chooses).
\\
basis\_per\_element     &Overrides the basis set for the specified element (argument 1: element symbol, argument 2: basis set). Can be given multiple times.
\\
basis\_per\_atom        &Overrides the basis set for the specified atom (argument 1: atom number starting at 1 and only counting QM atoms, argument 2: basis set). Can be given multiple times.
\\
functional              &followed by one string giving the exchange-correlation functional. Default is \ttt{PBE}.
\\
hfexchange              &Modifies the amount of HF exchange in the functional (give in fraction of 1). Default is whatever the chosen functional uses.
\\
dispersion              &Activates dispersion correction. Arguments are written verbatim to \textsc{Orca} input (in \ttt{EmpiricalDispersion=()}). Default is no dispersion. An example argument would be \ttt{GD3}.
\\
range\_sep\_settings    &Controls several functional parameters. Is followed by 5 arguments: \ttt{RangeSepMu}, \ttt{RangeSepScal}, \ttt{ACM-A}, \ttt{ACM-B}, \ttt{ACM-C}. Each is a floating point number. See the \textsc{Orca} manual for details on these settings. Note that \ttt{ACM-A} should not be used together with \ttt{hfexchange}.
\\
charge                  &Sets the total charge of the system. Can be either followed by a single integer (then the interface will automatically assign the charges to the multiplicities) or by one charge per multiplicity. 
\\
grid                    &Followed by a number (e.g., \ttt{grid 4}) defining which integration grid and accuracy to use. For details, see the example template file.
\\
gridx                   &To control the exchange grid (\ttt{rijcosx}).
\\
gridxc                  &To control the exchange-correlation grid (TDDFT).
\\
ri                      &Controls the density fitting scheme. Arguments can be, e.g., \ttt{rijcosx}. If not given, RI is deactivated.
\\
keys                    &Arguments are written verbatim to \textsc{Orca} input as separate keywords. Expert option. Note that this can be used to do PCM calculations (but requesting gradients will lead to a crash).
\\
no\_tda                 &This keyword deactivates TDA, which the interface requests by default.
\\
paddingstates           &Followed by a list of integers, which give the number of extra states to compute by \textsc{Gaussian}, but which are neglected in the output. Should not be changed between time steps, as this will break \textsc{Gaussian}'s restart routines.
\\
unrestricted\_triplets  &Requests that the triplets are calculated in a separate job from an unrestricted ground state. Default is to compute triplets as linear response of the restricted singlet ground state.
\\
qmmm                    &Activates QM/MM. In this case, the interface will look for two additional input files (see below).\\
  \hline
  \end{tabular}
\end{table}

\subsection{Resource file: \ttt{ORCA.resources}}

The file \ttt{ORCA.resources} contains mainly paths (to the \textsc{Orca} executables, to the scratch directory, etc.) and other resources, plus settings for \ttt{wfoverlap.x} and \textsc{TheoDORE}. This file must reside in the same directory where the interface is started. It uses a simple ``\ttt{keyword argument}'' syntax. Comments using \# and blank lines are possible, the order of keywords is arbitrary. Lines with unknown keywords are ignored, since the interface just searches the file for certain keywords.

Table~\ref{tab:orca_sh2} lists the existing keywords.
A fully commented resource file with all possible options and comprehensive descriptions is located in \ttt{\$SHARC/../examples/SHARC\_ORCA/}.

\begin{table}
  \centering
  \caption{Keywords for the \ttt{ORCA.resources} file.}
  \label{tab:orca_sh2}
  \begin{tabular}{>{\ttfamily}lp{12cm}}
  \hline
  Keyword       &Description\\
  \hline
orcadir                 &Is the path to the \textsc{Orca} installation. Relative and absolute paths, environment variables and \ttt{\textasciitilde} can be used. The interface will automatically update the \ttt{\$LD\_LIBRARY\_PATH}. 
\\
scratchdir              &Is a path to the temporary directory. Relative and absolute paths, environment variables and \ttt{\textasciitilde} can be used. If it does not exist, the interface will create it. In any case, the interface will delete this directory after the calculation. 
\\
savedir                 &Is a path to another temporary directory.  Relative and absolute paths, environment variables and \ttt{\textasciitilde} can be used. The interface will store files needed for restart there.
\\
memory                  &The memory usable by \textsc{Orca} and \textsc{WFoverlap}. The default is 100~MB. 
\\
ncpu                    &The number of CPUs used by the interface. Is overridden by environment variables from queuing engines (e.g., \ttt{\$NSLOTS} or \ttt{\$SLURM\_NTASKS\_PER\_NODE}). Will either be used to run \textsc{Orca} in parallel or to run several independent \textsc{Orca} runs at the same time.
\\
schedule\_scaling       &Gives the expected parallelizable fraction of the \textsc{Orca} run time (Amdahl's law). With a value close to zero, the interface will try to run all jobs at the same time. With values close to one, jobs will be run sequentially with the maximum number of cores.
\\
delay                   &Followed by a float giving the delay in seconds between starting parallel jobs to avoid excessive disk I/O (usually not necessary).
\\
wfoverlap               &Path to the \textsc{WFoverlap} code. Needed for overlap and Dyson norm calculations.
\\
wfthres                 &(float) Gives the amount of wave function norm which will be kept in the truncation of the determinant files. Default is 0.99 (i.e., the wave functions in the determinant files will have a norm of 0.99). Note that if hybrid functionals and no TDA are used, the response vector can have a norm larger than one, and wfthres should be increased.
\\
numfrozcore             &Number of frozen core orbitals for overlap and Dyson norm calculations. A value of -1 enables automatic frozen core.
\\
numocc                  &Number of ignored occupied orbitals in Dyson calculations.
\\
nooverlap               &Do not save determinant files for overlap computations.
\\
theodir                 &Path to the \textsc{TheoDORE} installation. Relative and absolute paths, environment variables and \ttt{\textasciitilde} can be used. The interface will set \ttt{\$PYTHONPATH} automatically.
\\
theodore\_prop          &Followed by a list with the descriptors which \textsc{TheoDORE} should compute. Note that descriptors will only be computed for restricted singlets (and triplets). Instead of a simple list, a Python literal can also be used, as in the \textsc{TheoDORE} input files.
\\
theodore\_fragment      &Followed by a list of atom numbers which should constitute a fragment in \textsc{TheoDORE}. For multiple fragments, the keyword can be used multiple times. Instead, the keyword can be followed by a Python literal, as in the \textsc{TheoDORE} input files.
\\
always\_orb\_init       &Do not use the orbital guesses from previous calculations/time steps, but always use the provided initial orbitals.
\\
always\_guess           &Always use the orbital guess from \textsc{Orca}.
\\
  debug                 &Increases the verbosity of the interface (standard out). Does not clean up the scratch directory. Copies all \textsc{Orca} outputs to the save directory.
\\
  no\_print             &Reduces interface standard output.
\\
qmmm\_table             &Followed by the path to the connection table file, in SHARC-QM/MM format.
\\
qmmm\_ff\_file          &Followed by the path to the force field file, in \textsc{Amber}95 format for \textsc{Tinker}.
\\
  \hline
  \end{tabular}
\end{table}

\paragraph{Parallelization}

\textsc{Orca} usually shows very good parallel scaling for most TD-DFT calculations.
However, it is more efficient to carry out multiple \textsc{Orca} calculations (different multiplicities, multiple gradients) in parallel, each one using a smaller number of CPUs.

In the \sharc-\textsc{Orca} interface, parallelization is controlled by the keywords \ttt{ncpu} and \ttt{schedule\_scaling}.
The first keyword controls the maximum number of CPUs which the interface is allowed to use for all \textsc{Orca} runs simultaneously.
The second keyword is the parallel fraction from Amdahl's Law, see Section~\ref{met:amdahl}.
With a value close to zero, the interface will try to run all jobs at the same time. With values close to one, jobs will be run sequentially with the maximum number of cores.
Typical values for \ttt{schedule\_scaling} are around 0.90 for both GGA functionals and hybrid functionals,  possibly less for very small computations.




\subsection{QM/MM force field file}

These force field files should have the format that can be found in the \textsc{Tinker} directory.



\subsection{QM/MM connection table file: \ttt{ORCA.qmmm.table}}

This file defines which atoms are in the QM or MM region, the atom types (for \textsc{Tinker}), and the connectivity.
Note that the SHARC-\textsc{Orca} interface uses newly developed routines to setup QM/MM calculations and communicate with \textsc{Tinker}, so this file is not identical to the table files of the ADF or \textsc{Molcas} interfaces.

A sample looks like:

\begin{example}
\begin{verbatim}
QM    223        
QM    222        1  3  4
QM    136        
QM     80        
MM     63        6 7
MM     64       
MM     64       
\end{verbatim}
\end{example}

Each line corresponds to one atom.
The example has 7 atoms, the first four in the QM region and the three last ones in the MM region.
The second column defines the atom types according to the numbering in the used force field file.
The integers to the right define the connectivity. 
Note that the interface automatically adds any necessary redundancy to the connectivity table, so it is enough in the table file to define a bond once.














% ========================================================================================================= %

\section{The \textsc{WFoverlap} Program}\label{sec:int:wfoverlap}

This section does not describe an interface to \sharc, but rather the \textsc{WFoverlap} program.
This program is part of the \sharc\ distribution, but can also be obtained as a \link{https://sharc-md.org/?page_id=309}{stand-alone package} (including a more detailed manual and a set of auxiliary scripts).
It computes overlaps between many-electron wave functions expressed in terms of linear combinations of Slater determinant, which are based on molecular orbitals (from an LCAO ansatz).
It can also compute Dyson orbitals and Dyson norms between wave functions differing by one $\alpha$ or one $\beta$ electron.
The program is based on the efficient and general algorithm published in Ref.~\cite{Plasser2016JCTC}.
It is possible to vary the geometry, the basis set, the molecular orbitals, and the wavefunction expansion between the calculations.

The resulting wave function overlaps or Dyson norms can be used for example for:
\begin{itemize}
  \item Propagation in local diabatization, the main application inside \sharc,
  \item Computation of photoionization spectra~\cite{Ruckenbauer2016SR,Ruckenbauer2016JCP},
  \item Comparison of wave functions at different levels of theory~\cite{Plasser2016JCP}.
\end{itemize}
If you employ the \ttt{wfoverlap.x} code inside the \sharc\ suite for these purposes, please cite these references!

The documentation here only gives a brief overview over the input options of \textsc{wfoverlap.x}, because within the \sharc\ suite the \ttt{wfoverlap.x} program is always called automatically by the interfaces. For the full manual (and for access to the auxiliary scripts of \textsc{wfoverlap.x}), please download the separate \link{https://sharc-md.org/?page_id=309}{\textsc{WFoverlap} package}.


\subsection{Installation}

\paragraph{Using precompiled binaries}

After unpacking, the directory \ttt{\$SHARC} should contain a binary \ttt{wfoverlap\_ascii.x} and a link called \ttt{wfoverlap.x} pointing to the binary.
With this setup, most interfaces should work without problems.

\paragraph{Manual installation}

The only exceptions are the following: \textsc{Columbus} (overlaps and Dyson norms) and \textsc{Molcas} (only Dyson norms).
These features are only available if \ttt{wfoverlap.x} is recompiled with proper support for these programs.
Alternatively, you may want to link \ttt{wfoverlap.x} against your favorite libraries.
In these cases, a manual installation is necessary.

For the manual installation you need a working Fortran90 compatible compiler (Intel's ifort is recommended),
some reasonably fast BLAS/LAPACK libraries (Intel's MKL is recommended, although atlas is also fine).

Optionally, with a working \textsc{Columbus} Installation you can install the \textsc{Columbus} bindings, which will
allow direct reading of SIFS integral files generated by \textsc{Dalton}. To use this option, it is
necessary to use the \verb|read_dalton.o| object file.
\textsc{Molcas/Seward} integral files can be read by linking with the \textsc{Columbus/Molcas} interface. Link against
\verb|read_molcas.o| for this purpose.

To compile the source code, switch to the \texttt{source} directory and edit the Makefile to adjust it to your Fortran compiler and BLAS/LAPACK installation. The location of your \textsc{Columbus} installation has to be set via the enviroment variable
\verb|$COLUMBUS|.

Issuing the command:

\begin{example}
\verb|cd $SHARC/../wfoverlap/source/|

\verb|make|
\end{example}

will compile the source and create the binaries.


If you are unable to link against \textsc{Columbus} and/or \textsc{Molcas}, simply call

\begin{example}
\verb|make  wfoverlap_ascii.x|
\end{example}

to compile a minimal version of the CI Overlap program that only reads ASCII files.
In this case, overlap and Dyson calculations with \ttt{SHARC\_COLUMBUS.py} and Dyson calculations with \ttt{SHARC\_MOLCAS.py} will not be possible.

\paragraph{Testing}

The command 

\begin{example}
\verb|make test|
\end{example}

will run a couple of tests to check if the program is working correctly (alternatively you can call \verb|ovl_test.bash $OVDIR|, but \ttt{\$OVDIR} needs to be set before).


\subsection{Workflow}

The workflow of the overlap program is shown in Figure~\ref{fig:work}.
Four pieces of input, as shown on top, have to be given:
\begin{itemize}
\item Overlaps between the two sets of AOs used to construct the bra and ket wavefunctions,
\item MO coefficients of the bra and ket wavefunctions,
\item information about the Slater determinants,
\item the corresponding CI coefficients.
\end{itemize}

Two main intermediates are computed, the MO overlaps and the unique factors $\mathcal{S}_{kl}, \bar{\mathcal{S}}_{kl}$ where the latter may require significant amounts of memory to be stored.
The reuse of these intermediates is one of the main reasons for the decent performance of the \ttt{wfoverlap.} program.

\begin{figure}[htb]
  \centering
  \includegraphics[scale=1]{img/wfoverlap/wfoverlap.pdf}
  \caption{Workflow of the wavefunction overlap program.}
  \label{fig:work}
\end{figure}



\subsection{Calling the program}
The main program is called in the following form
%
\begin{example}
\ttt{wfoverlap.x [-m <mem=1000>] [-f <input\_file=cioverlap.input>]}
\end{example}
%
with the command line options
%
\begin{itemize}
\item \ttt{-m} : amount of memory in MB
\item \ttt{-f} : input file
\end{itemize}

Example:
\begin{example}
\ttt{wfoverlap.x -m 2000 -f wfov.in}
\end{example}

\paragraph{Mode}

The program automatically detects whether overlaps or Dyson orbitals should be calculated.
If the number of electrons in the bra and ket wavefunctions is the same, wavefunction overlaps are computed.
If the number of $\alpha$ electrons or the number of $\beta$ electrons differ by exactly 1, Dyson orbitals are computed.
If the wave functions differ by more than one electron, the program will stop with an error message.

\paragraph{Memory}

The amount of \textbf{memory} given is a decisive factor for the performance of the code.
Depending on the amount of memory, one of three different modes is chosen:

(i) All $\mathcal{S}_{kl}$ and $\bar{\mathcal{S}}_{kl}$ terms are kept in core (using arrays called \ttt{P\_ovl}and \ttt{Q\_ovl}).

(ii) Only the $\mathcal{S}_{kl}$ factors (\ttt{P\_ovl}) are kept in core. This is indicated by
\begin{example}
\ttt{Allocation of Q block overlap matrix failed.\\
  - Using on-the-fly algorithm for Q determinants.}
\end{example}
This mode is generally as efficient as 1. but shows somewhat worse parallel scaling.

(iii) Not even all $\mathcal{S}_{kl}$ factors can be stored
\begin{example}
\ttt{Only 437 out of 886 columns of the P\_ovl matrix are stored in memory (3 MB)!\\
 Increase the amount of memory to improve the efficiency.}
\end{example}
This mode is significantly slower than (i) and (ii) and should be avoided by increasing the amount of memory.

\paragraph{Input file}

\begin{table}[tb]
  \caption[List of keywords given in the input file.]{List of keywords given in the input file. The \ttt{a\_mo, b\_mo, a\_det, b\_det} keywords are mandatory, all others are optional.}
  \label{tab:key_wfo}
  \begin{tabular}{llp{9cm}}
    \hline
    Keyword & Default & Description \\
    \hline
    \ttt{a\_mo}                 & ---                           & MO coefficient file (bra)\\
    \ttt{b\_mo}                 & ---                           & MO coefficient file (ket)\\
    \ttt{a\_mo\_read}           & 0                             & Format for the MO coefficients (bra):\\
                                                                && 0: \textsc{Columbus}, 1: \textsc{Molcas}, 2: \textsc{Turbomole}\\
    \ttt{b\_mo\_read}           & 0                             & Format for the MO coefficients (ket)\\
    \ttt{a\_det}                & ---                           & Determinant file (bra)\\
    \ttt{b\_det}                & ---                           & Determinant file (ket)\\
    \ttt{ncore}                 & 0                             & Number of discarded core orbitals\\
    \ttt{ndocc}                 & 0                             & Number of doubly occupied orbitals that are not used for 
                                                                annihilation in Dyson orbital calculations (only has effect if larger than \ttt{ncore})\\
    \ttt{ao\_read}              & 0                             & Format for overlap integrals:\\
                                                                &&0: ASCII, 1: \textsc{Molcas}, 2: \textsc{Columbus/SIFS},\\
                                                                &&-1: Compute by inversion of MO coefficient matrx\\
    \ttt{mix\_aoovl}            & \ttt{S\_mix/ONEINT/aoints}    & AO overlap file\\
                                & for \ttt{ao\_read=0/1/2}      &\\
    \ttt{same\_aos}             & .false.                       & If both calculations were performed with the same set of AOs (specify only for \ttt{ao\_read=1/2})\\
    \ttt{nao\_a}                & \textit{automatic}            & Number of bra AOs for \ttt{ao\_read=1/2} (specify only if different from ket AOs) \\
    \ttt{nao\_b}                & \textit{automatic}            & Number of ket AOs (see above)  \\
    \ttt{moprint}               & 0                             & Print Dyson orbitals: 1: coefficients to std. out,\\
                                                                & 2: as Jmol script\\
    \ttt{force\_direct\_dets}   & .false.                       & Compute $\mathcal{S}_{kl}$ terms directly (turn off "superblocks").
                                                                Recommended if the number of CPU-cores is large (on the same order as the number of "superblocks").\\
    \ttt{force\_noprecalc}      & .false.                       & Do not precalculate the $\bar{\mathcal{S}}_{kl}$ factors.\\
    \ttt{mixing\_angles}        & .false.                       & Compute mixing angles as a matrix logarithm.\\
    \hline
  \end{tabular}
\end{table}

An example input file is shown below:
%
\begin{example}
\begin{verbatim}
a_mo=mocoef_a
b_mo=mocoef_b
a_det=dets_a
b_det=dets_b
ao_read=2
same_aos
\end{verbatim}
\end{example}
The full list of keywords is given in Table~\ref{tab:key_wfo}.



\subsection{Input data}
Typically, three types of input need to be provided: AO overlaps, MO coefficients, and a combined file with determinant information and CI coefficients (cf.~Figure~\ref{fig:work}).
The file formats are explained here.
Within \sharc, these files are automatically extracted or converted by the interfaces, so the user does not need to create them.

\paragraph{AO overlaps}
The mixed AO overlaps $\left<\chi_{\mu}|\chi_{\nu}'\right>$ between the AOs used to expand the bra and ket wavefunctions are required.
They are in general created by a "double molecule" calculation, i.e. an AO integral calculation where every atom is found twice in the input file.

The native format (\ttt{ao\_read=0}) is a simple ASCII file containing the relevant off-diagonal block of the mixed AO overlap matrix, e.g.

\begin{example}
\begin{verbatim}
   7 7
     9.97108464676133E-001    2.36720699813181E-001   ...
     2.36720699813181E-001    9.99919192433940E-001   ...
     1.00147985713321E-002    6.52340422397770E-003   ...
         ...
\end{verbatim}
\end{example}

In addition, \textsc{Molcas} (\ttt{ao\_read=1}) and \textsc{Columbus/SIFS} (\ttt{ao\_read=2}) files can be read in binary form.

If the same AOs are used for the bra and ket wavefunctions and the MO coefficient matrix is square, it is possible to reconstruct the overlaps by inversion of the MO coefficient matrix (\ttt{ao\_read=-1}).
In this case it is not necessary to supply a \ttt{mix\_aoovl} file.

\paragraph{MO coefficients}

MO coefficients of the bra and ket wavefunctions can usually be read in directly in the form written by the quantum chemistry program.
The supported options for \ttt{a\_mo\_read} and \ttt{b\_mo\_read} are \ttt{0} for \textsc{Columbus} format, \ttt{1} for \textsc{Molcas} lumorb format, and \ttt{2} for \textsc{Turbomole} format.

Because the number of electrons strongly affects the run time of \ttt{wfoverlap.x}, it is generally beneficial to apply a frozen core approximation, even if the actual wave function calculation did not do so.
Most interfaces which use \ttt{wfoverlap.x} have a keyword \ttt{numfrozcore} in the resource file, which only affects the number of frozen core orbitals for the overlap calculation (If the interface support frozen core for the quantum chemistry itself, there will be a keyword in the template file).


\paragraph{Slater determinants and CI coefficients}

Slater determinants and CI coefficients are currently supplied by an ASCII file of the form
%
\begin{example}
\begin{verbatim}
3 7 168
dddddee   0.979083342437    0.979083342437   -0.122637656388
ddddabe  -0.094807515471   -0.094807515471   -0.663224542162
ddddbae   0.094807515471    0.094807515471    0.663224542162
...
\end{verbatim}
\end{example}

The first line specifies
%
\begin{itemize}
\item the number of states (columns in the file),
\item the number of MOs (length of the determinant strings), and
\item the number of determinants in the file (length of the file).
\end{itemize}

Every subsequent line gives the determinant string and the corresponding CI coefficients for the different states.
The following symbols are used in the determinant string:

\begin{itemize}
\item[d] - doubly occupied
\item[a] - singly occupied ($\alpha$)
\item[b] - singly occupied ($\beta$)
\item[e] - empty
\end{itemize}

Most relevant for \sharc\ users, the \ttt{wfoverlap.x} program \emph{fully considers all determinants inside these files}, without applying any form of truncation. Hence, truncation of long wave functions is done during the creation of the determinant files.
Most interfaces which write these files have a keyword \ttt{wfthres} in their resource file. This threshold is a number between 0.0 and 1.0, and is the minimum wave function norm to which the wave functions should be truncated. During truncation, the interfaces generally retain the largest-amplitude CI coefficients, and remove determinants with small coefficients, i.e., they find the truncated expansion with the fewest determinants which has a norm above the \ttt{wfthres}.
Choosing this threshold properly can very strongly affect the computational time spent in the overlap calculation. Generally, for CASSCF wave functions the threshold can be set to 1 (\ttt{SHARC\_MOLPRO.py} and \ttt{SHARC\_MOLCAS.py} always use all determinants and do not have the \ttt{wfthres} option), for TDA-DFT/ADC(2) it should usually be above 0.99, for and for MRCI wave functions it might be necessary to go as low as 0.95, depending on the accuracy and performance needed.
For TD-DFT calculations without the Tamm-Damcoff approximation, the response vectors are usually normalized to $|\mathbf{X}|^2-|\mathbf{Y}|=1$, but only $\mathbf{X}$ is used in the overlap calculation; since the norm of $\mathbf{X}$ can thus exceed 1, the \ttt{wfthres} should be increased above 1, too.
As a rule of thumb, the threshold should always be chosen such that each state is represented by at least a few 100 determinants in the file, in order to obtain smoothly varying overlaps.
If unsure, the user should perform a test calculation, varying the \ttt{wfthres} until a suitable one is found.



\subsection{Output}

Usually, the output of \ttt{wfoverlap.x} is automatically extracted by the interfaces, and reported in \ttt{QM.out} in the overlap or 2D-property sections.

\paragraph{Wavefunction overlaps}

The output first lists some information about the wavefunction structure and about the computational time taken for the individual steps (cf. Figure~\ref{fig:work}).

A typical result of a wavefunction overlap computation is shown here:
\begin{example}
\begin{verbatim}
 Overlap matrix <PsiA_i|PsiB_j>
                |PsiB  1>     |PsiB  2>
<PsiA  1|    0.5162656622 -0.2040109070
<PsiA  2|   -0.2167057391 -0.5266552021

 Renormalized overlap matrix <PsiA_i|PsiB_j>
                |PsiB  1>     |PsiB  2>
<PsiA  1|    0.5162656622 -0.2040109070
<PsiA  2|   -0.2167057391 -0.5266552021

 Performing Lowdin orthonormalization by SVD...

 Orthonormalized overlap matrix <PsiA_i|PsiB_j>
                |PsiB  1>     |PsiB  2>
<PsiA  1|    0.9273847015 -0.3741090956
<PsiA  2|   -0.3741090956 -0.9273847015
\end{verbatim}
\end{example}

\ttt{Overlap matrix} gives the raw overlap values
\begin{equation}
  \braket{\Psi_I|\Psi_J'}
\end{equation}
of the wavefunctions supplied.

\ttt{Renormalized overlap matrix} gives the renormalized overlap values
\begin{equation}
  \frac{  \braket{\Psi_I|\Psi_J'}  }{  ||\Psi_I||^2  ||\Psi_J'||^2}
\end{equation}
relevant in the case of wavefunction truncation.

The \ttt{Orthonormalized overlap matrix} is constructed according to a procedure described in more detail in Ref.~\cite{Plasser2016JCTC}.



\paragraph{Dyson orbitals}
The matrix of Dyson norms is printed at the end of the file

\begin{example}
\begin{verbatim}
 ALPHA ionization
 Dyson norm matrix |<PsiA_i|PsiB_j>|^2                                          
                |PsiB  1>     |PsiB  2>     |PsiB  3>                           
<PsiA  1|    0.8817323437  0.4716319904  0.0680618001                           
<PsiA  2|    0.0615587916  0.4657174978  0.8772909828                           
<PsiA  3|    0.0000000000  0.0363130811  0.0000000000                           
<PsiA  4|    0.9634885049  0.0000000000  0.0017379586                           
<PsiA  5|    0.0000000000  0.9261839484  0.0000000000 
\end{verbatim}
\end{example}

In the case of \ttt{moprint=1} the orbitals are printed, as well.
The expansion is given with respect to the MOs of the neutral system.
%
\begin{example}
\begin{verbatim}
  Dyson orbitals in reference |ket> MO basis:                                   
<PsiA  1|                                                                       
         |PsiB   1>      |PsiB   2>      |PsiB   3>                             
MO     1 -1.24032037E-03  0.00000000E+00  9.98160731E-04                        
MO     2 -5.90277699E-02  0.00000000E+00  6.14517859E-02                        
MO     3 -1.23295110E-09  0.00000000E+00  3.08416849E-10                        
MO     4  0.00000000E+00 -6.86713110E-01  0.00000000E+00                        
MO     5  9.31351013E-01  0.00000000E+00 -2.49427166E-01                        
MO     6 -3.50106110E-02  0.00000000E+00  4.19935829E-02
MO     7 -1.83303166E-10  0.00000000E+00 -3.67409953E-12                        
MO     8 -1.91183349E-10  0.00000000E+00  9.40768334E-11 
...
\end{verbatim}
\end{example}
%













% ========================================================================================================= %
% ========================================================================================================= %
% ========================================================================================================= %

\chapter{Auxilliary Scripts}\label{chap:aux}

In this chapter, all auxiliary scripts and programs are documented. Input generators (like \ttt{molpro\_input.py} and \ttt{molcas\_input.py}) are documented in the relevant interface sections.

All auxiliary scripts are either interactive---prompting user input from stdin in order to setup a certain task---or non-interactive, meaning they are controlled by command-line arguments and options, in the same way as many standard command-line tools work.

All interactive scripts sequentially ask a number of questions to the user. In many cases, a default value is presented, which is either preset or detected by the scripts based on the availability of certain files. Furthermore, the scripts feature auto-completion of paths and filenames (use TAB), which is active only in questions where auto-completion is relevant. For certain questions where lists of integers needs to be entered, ranges can be indicated with the tilde symbol (\textasciitilde), e.g., \ttt{-8\textasciitilde-2} (note that no spaces are allowed between the tilde and the two numbers) to indicate the list \ttt{-8 -7 -6 -5 -4 -3 -2}. 

All interactive scripts write a file called \ttt{KEYSTROKES.<script\_name>} which contains the user input from the last completed session. These files can be piped to the interactive scripts to perform the same task again, for example:
\begin{verbatim}
user@host> cat KEYSTROKES.excite - | $SHARC/excite.py
\end{verbatim}
Note the \ttt{-}, which tells \ttt{cat} to switch to stdin after the file ends, so that the user can proceed if the script asks for more input than contained in the \ttt{KEYSTROKES} file.

All non-interactive scripts can be called with the \ttt{-h} option to obtain a short description, usage information and a list of the command line options.
Non-interactive scripts also write a \ttt{KEYSTROKES.<script\_name>} file, which will contain the last command entered to execute the script (including all options and arguments).

All scripts can be safely killed during a run by using \ttt{Ctrl-C}. In the case of interactive scripts, a \ttt{KEYSTROKES.tmp} file remains, containing the user input made so far. Note that the \ttt{KEYSTROKES.tmp} file cannot be directly piped to the scripts, because \ttt{KEYSTROKES.tmp} will be overwritten when the script starts.



% ========================================================================================================= %

\section{Wigner Distribution Sampling: \ttt{wigner.py}}\label{sec:wigner.py}

The first step in preparing the dynamics calculation is to obtain a set of physically reasonable initial conditions. Each initial condition is a set of initial atomic coordinates, initial atomic velocities and initial electronic state. The initial geometry and velocities can be obtained in different ways. With \sharc, often sampling of a quantum-harmonic Wigner distribution is performed. 

The sampling is carried out with the non-interactive Python script \ttt{wigner.py}. The theoretical background is summarized in section~\ref{met:wigner}.

\subsection{Usage}

The general usage is 
\begin{verbatim}
user@host> $SHARC/wigner.py [options] filename.molden
\end{verbatim}
\ttt{wigner.py} takes exactly one command-line argument (the input file with the frequencies and normal modes), plus some options. Usually, the \ttt{-n} option is necessary, since the default is to only create 3 initial conditions.

The argument is the filename of the file containing the information about the vibrational frequencies and normal modes. The file is by default assumed to be in the \link{http://www.cmbi.ru.nl/molden/molden_format.html}{\textsc{Molden} format}. For usage with \ttt{wigner.py}, only the following blocks have to be present:

\begin{minipage}{0.9\textwidth}
  \begin{itemize}
    \item $[$FREQ$]$
    \item $[$FR-COORD$]$
    \item $[$FR-NORM-COORD$]$
  \end{itemize}
\end{minipage}

The script accepts a number of command-line options, specified in table~\ref{tab:wigner_opts}.
\begin{table}
  \centering
  \caption{Command-line options for script \ttt{wigner.py}.}
  \label{tab:wigner_opts}
  \begin{tabular}{>{\ttfamily}lp{9cm}l}
    \hline
    \rmfamily Option        &Description      &Default\\
    \hline
    -h                  &Display help message and quit              &---                            \\
    -n  INTEGER         &Number of initial conditions to generate   &3                              \\
    -m                  &Modify atom masses (starts interactive dialog)  &Most common isotopes      \\
    -s  FLOAT           &Scaling factor for the frequencies         &1.0                            \\
    -t  FLOAT           &Use Boltzmann-weighted distribution at the given temperature   &0.0 K\\
    -T                  &Discard very high vibrational states at high temperatures      &Don't discard, but warn\\
    -L  FLOAT           &Discard frequencies below the given one (in cm$^{-1}$)         &10.0\\
    -o  FILENAME        &Output filename                            &\ttt{initconds}                \\
    -x                  &Creates an xyz file with the sampled geometries &\ttt{initconds.xyz}       \\
    -l                  &Instead of generating \ttt{initconds}, create input for \ttt{SHARC\_LVC.py}    &Create \ttt{initconds}\\
    -r  INTEGER         &Seed for random number generator           &16661                          \\
    -f F                &Type of normal modes read (0=detect automatically, 1--4=see below)     &0\\
    --keep\_trans\_rot  &Do not remove translations and rotations from velocity vector &\\
    --use\_eq\_geom     &Sample only velocities, but keep equilibrium geometry  &Sample normally\\
    --use\_zero\_veloc  &Sample only geometries, but set velocities to zero     &Sample normally\\
    \hline
  \end{tabular}
\end{table}


\subsection{Normal mode types}

The normal mode vectors contained in a \textsc{Molden} file can follow different conventions, e.g., unscaled Cartesian displacements or different kinds of mass-weighted displacements.
By default, \ttt{wigner.py} attempts to identify which convention is followed by the file (by performing different renormalizations and checking if the so-obtained matrix is orthogonal).
In order to use this automatic detection, use \ttt{-f 0}, which is the default.
Otherwise, there are four possible options: \ttt{-f 1} to assume normal modes in the \textsc{Gaussian} convention (used by \textsc{Gaussian}, \textsc{Turbomole}, \textsc{Q-Chem}, ADF, and \textsc{Orca}); \ttt{-f 2} to assume Cartesian normal modes (used by \textsc{Molcas} and \textsc{Molpro}); \ttt{-f 3} to assume the \textsc{Columbus} convention; or \ttt{-f 4} for mass-weighted, orthogonal normal modes.

\subsection{Non-default masses}

When the \ttt{-m} option is used, the script will ask the user to interactively modify the atom masses. For each atom (referred to by the atom index as in the \textsc{Molden} file), a mass can be given (relative atomic weights). Note that the frequency calculation which produces the \textsc{Molden} should be done with the same atomic masses.

\subsection{Sampling at finite temperatures}

When the \ttt{-t} option is used, the script assumes a finite, non-zero temperature.
The sampling will then consist of two steps, where first randomly a vibrational state is picked from the Boltzmann distribution, and then the Wigner distribution of that state is employed.
For more details, see Section~\ref{met:wigner}.

At high temperatures and for low-frequency modes it is possible that very large vibrational quantum numbers will be selected.
Because of the occurrence of factorials in the Laguerre polynomials in the excited Wigner distributions, this leads to variable overflow for $\nu_\text{vib}>150$. Hence, the highest vibrational quantum number considered is 150, and higher ones are set to 150.
Since this can lead to an overrepresentation of $\nu_\text{vib}=150$, with the \ttt{-T} option one can instead discard all samplings where $\nu_\text{vib}>150$.
No matter whether \ttt{-T} is used or not, keep in mind that usually such high vibrational states might invalidate the assumption of an harmonic oscillator, and other sampling methods (e.g., molecular dynamics) should be considered.

\subsection{Output}

The script \ttt{wigner.py} generates a single output file, by default called \ttt{initconds}. All information about the initial conditions is stored in this file. Later steps in the preparation of the initial conditions add information about the excited states to this file. The file is formatted in a human-readable form.

The \ttt{initconds} file format is specified in section~\ref{sec:initcondsfile}.

When the \ttt{-x} option is given, additionally the script produces a file called \ttt{initconds.xyz}, which contains the sampled geometries in standard xyz format. This can be useful to inspect the distribution of geometry parameters (e.g., bond lengths) or to perform single point calculations at the sampled geometries.

When the \ttt{-l} option is given, the script only produces a file called \ttt{V0.txt}, which is a necessary input file for the LVC interface (see section~\ref{sec:int:lvc}). If this option is activated, no \ttt{initconds} or \ttt{initconds.xyz} files are produced.



% ========================================================================================================= %

\section{\textsc{Amber} Trajectory Sampling: \ttt{amber\_to\_initconds.py}}\label{sec:amber_to_initconds.py}

The first step in preparing the dynamics calculation is to obtain a set of physically reasonable initial conditions. Each initial condition is a set of initial atomic coordinates, initial atomic velocities and initial electronic state. The initial geometry and velocities can be obtained in different ways. 
Besides sampling from a quantum mechanical Wigner distribution (with \ttt{wigner.py}), it is a widespread approach to sample geometries and velocities from a ground state molecular dynamics simulation.

Using \ttt{amber\_to\_initconds.py}, one can convert the results of an \textsc{Amber} simulation to a \sharc\ \ttt{initconds} file.

\subsection{Usage}

In order to use \ttt{amber\_to\_initconds.py}, it is necessary to first carry out an \textsc{Amber} simulation.
You need to add the following options to the \textsc{Amber} MD input file: (i) \ttt{ntxo=1} to tell \textsc{Amber} to write ASCII restart files, (ii) \ttt{ntwr=-5000} to create a restart file every 5000 steps (other values are possible, but use the minus to not overwrite the restart files).
This will create a set of \textsc{Amber} restart files called, e.g., \ttt{md.rst\_5000}, \ttt{md.rst\_10000}, ...

Note that it is also necessary to reimage the \textsc{Amber} restart files, because \sharc\ does not work with periodic boundary conditions, but the \textsc{Amber} trajectories might use them.
The reimaging can be performed with \textsc{Amber}'s tool \ttt{cpptraj}, using the following input:
\begin{verbatim}
parm <filename>.prmtop
trajin <filename>.rst7
autoimage
trajout <filename2>.rst7
run
\end{verbatim}
This command has to be repeated for each restart file which needs to be reimaged.
Note that \ttt{amber\_to\_initconds.py} only works with the \ttt{rst7} ASCII file format, not with the \ttt{rst} format (even if it is ASCII-formatted).

If you saved the restart files in \textsc{Amber}'s newer NetCDF format, \ttt{cpptraj} can also be used to convert them to ASCII \ttt{rst7} restart files.
If you did not save restart files, \ttt{cpptraj} can even be used to generate restart files from the trajectory file (\ttt{.mdcrd} and \ttt{.mdvel}), but for this way it is necessary to save the velocities (\ttt{.mdvel} file).

With the restart files prepared, call \ttt{amber\_to\_initconds.py} like this:
\begin{verbatim}
user@host> $SHARC/amber_to_initconds.py [options] md.prmtop md.rst_0 md.rst_5000 md.rst_10000 ...
\end{verbatim}
The possible options are shown in Table~\ref{tab:amber_opts}.

\begin{table}
  \centering
  \caption{Command-line options for script \ttt{amber\_to\_initconds.py}.}
  \label{tab:amber_opts}
  \begin{tabular}{>{\ttfamily}lp{9cm}l}
    \hline
    \rmfamily Option        &Description      &Default\\
    \hline
    -h                  &Display help message and quit              &---                            \\
    -t  FLOAT           &Time step (in femtoseconds) used in the \textsc{Amber} simulation      &---\\
    -o  FILENAME        &Output filename                            &\ttt{initconds}                \\
    -x                  &Creates an xyz file with the sampled geometries &\ttt{initconds.xyz}       \\
    -m                  &Modify atom masses (starts interactive dialog)  &As in \ttt{prmtop} file   \\
    --keep\_trans\_rot  &Do not remove translations and rotations from velocity vector &\\
    --use\_zero\_veloc  &Sample only geometries, but set velocities to zero     &Sample normally\\
    \hline
  \end{tabular}
\end{table}

\subsection{Time Step}

Note that the option \ttt{-t} (giving the time step used in \textsc{Amber} in femtoseconds) is mandatory; if not given, an error message is produced.
This is because \textsc{Amber} uses a Leapfrog algorithm and thus stores in the restart file $R(t)$ and $v(t-\Delta t/2)$, whereas \sharc\ uses the velocity-Verlet algorithm and requires geometry and velocity at the same time, e.g., $R(t-\Delta t/2)$ and $v(t-\Delta t/2)$.
To compensate this, \ttt{amber\_to\_initconds.py} computes $R(t-\Delta t/2)$ from $R(t)-v(t-\Delta t/2)\Delta t/2$.
Hence, $\Delta t$ of the \textsc{Amber} trajectory needs to be known.

\subsection{Atom Types and Masses}

By default, atom types and masses are read from the \ttt{prmtop} file (from flags \ttt{ATOMIC\_NUMBER} and \ttt{MASS}). 
If the atomic number is not sensible (e.g., -1 for a transition metal) then \ttt{amber\_to\_initconds.py} prompts the user to define the element.
The masses in the \ttt{prmtop} file can be overridden if the \ttt{-m} option is given; then the user can adjust the mass of each atom individually.

\subsection{Output}

\ttt{amber\_to\_initconds.py} produces the same output as \ttt{wigner.py} (section ~\ref{sec:wigner.py}).
By default, a file called \ttt{initconds} is generated for the converted initial conditions. 
It is important to note that the first restart file given (the second command line argument) is treated as the ``equilibrium'' geometry for the purpose of generating the \ttt{initconds} file.
The second given restart file is then converted to the initial condition with index 1, and so on.
Note that it is possible to give the same restart file multiple times as an argument (so that the same geometry can be used as ``equilibrium'' geometry and as proper initial condition.




% ========================================================================================================= %

\section{\sharc\ Trajectory Sampling: \ttt{sharctraj\_to\_initconds.py}}\label{sec:sharctraj_to_initconds.py}

The first step in preparing the dynamics calculation is to obtain a set of physically reasonable initial conditions. Each initial condition is a set of initial atomic coordinates, initial atomic velocities and initial electronic state. The initial geometry and velocities can be obtained in different ways. 
Besides sampling from a quantum mechanical Wigner distribution, it is often appropriate to sample geometries and velocities from a ground state molecular dynamics simulation.

Using \ttt{sharctraj\_to\_initconds.py}, one can convert the results of a \sharc\ simulation to a new \sharc\ \ttt{initconds} file.

\subsection{Usage}

In order to use \ttt{sharctraj\_to\_initconds.py}, it is necessary to first run a number of \sharc\ trajectories (the initial conditions for those need to be obtained with \ttt{wigner.py} or \ttt{amber\_to\_initconds.py}).
The trajectories can be run with any number of states and in any state, and with any desirable options; only geometries and velocities are converted to the new \ttt{initconds} file.

With the trajectories prepared, call \ttt{sharctraj\_to\_initconds.py} like this:
\begin{verbatim}
user@host> $SHARC/sharctraj_to_initconds.py [options] Singlet_0 ...
\end{verbatim}
Alternatively, with the \ttt{--give\_TRAJ\_paths} option, one can also do:
\begin{verbatim}
user@host> $SHARC/sharctraj_to_initconds.py --give_TRAJ_paths [options] TRAJ_00001 TRAJ_00002 ...
\end{verbatim}
The possible options are shown in Table~\ref{tab:sharctraj_opts}.

\begin{table}
  \centering
  \caption{Command-line options for script \ttt{sharctraj\_to\_initconds.py}.}
  \label{tab:sharctraj_opts}
  \begin{tabular}{>{\ttfamily}lp{8.5cm}l}
    \hline
    \rmfamily Option        &Description      &Default\\
    \hline
    -h                  &Display help message and quit              &---                            \\
    -r  INTEGER         &Seed for the random number generator       &16661                          \\
    -S INTEGER INTEGER  &Range of time steps from which a step is randomly chosen       &last step  \\
    -o  FILENAME        &Output filename                            &\ttt{initconds}                \\
    -x                  &Creates an xyz file with the sampled geometries &\ttt{initconds.xyz}       \\
    --keep\_trans\_rot  &Do not remove translations and rotations from velocity &\\
    --use\_zero\_veloc  &Sample only geometries, but set velocities to zero     &Sample normally\\
    --debug             &Show timings                                           &\\
    --give\_TRAJ\_paths &Allows specifying individual trajectories &Specify parent directories\\
    \hline
  \end{tabular}
\end{table}

\subsection{Random Picking of Time Step}

For each directory specified as command line argument, \ttt{sharctraj\_to\_initconds.py} picks exactly one time step and extracts geometries and velocities of that time step.
Note that a directory can be given several times as argument, so that multiple time steps can be selected.

The time steps are generally picked randomly (uniform probabilities) from an interval specified with the \ttt{-S} option.
This option takes two integers, e.g., \ttt{-S 50 -50}, which can be positive or negative.
The meaning of positive/negative/zero is the same as in \textsc{Python}: positive numbers simply denote a time step (start counting with zero for the step zero of the trajectory); for negative numbers, start counting at the end, i.e., -1 is the last time step of the trajectory.
In this way, it is possible to select the snapshot from the last $n$ steps of all trajectories, even if they have different length.
The example above, \ttt{-S 50 -50}, means picking a time step between the 50th and the 50th-last step. Note that if a trajectory is shorter than 100 steps, in this example it is skipped because there are no steps between the 50th and the 50th-last step.

\subsection{Output}

\ttt{sharctraj\_to\_initconds.py} produces the same output as \ttt{wigner.py} (section ~\ref{sec:wigner.py}).
By default, a file called \ttt{initconds} is generated for the converted initial conditions. 
It is important to note that the first directory given (the first command line argument) is treated as the ``equilibrium'' geometry for the purpose of generating the \ttt{initconds} file.
The second given directory is then converted to the initial condition with index 1, and so on.
Note that it is possible to give the same directory multiple times as an argument (so that the same geometry can be used as ``equilibrium'' geometry and as proper initial condition.


% ========================================================================================================= %

\section{Setup of Initial Calculations: \ttt{setup\_init.py}}\label{sec:setup_init.py}

The interactive script \ttt{setup\_init.py} creates input for single point calculations at the initial geometries given in an \ttt{initconds} file. These calculations might be necessary for some schemes to select the initial electronic state of the trajectory, e.g., based on the excitation energies and oscillator strength of the transitions from ground state to the excited state, or based on overlaps with a reference wave function. 

There are other choices of the initial state possible, which do not require single point calculations at all initial geometries. See the description of \ttt{excite.py} (section~\ref{sec:excite.py}). In this case, \ttt{setup\_init.py} can be used to set up only the calculation at the equilibrium geometry (see below at ``Range of Initial Conditions'').

\subsection{Usage}

The script is interactive, and can be started by simply typing 
\begin{verbatim}
user@host> $SHARC/setup_init.py
\end{verbatim}
Please be aware that the script will setup the calculations in the directory where it was started, so the user should \ttt{cd} to the desired directory before executing the script.

Please note that the script does not expand \ttt{\textasciitilde} or shell variables, except where noted otherwise.

\subsection{Input}

The script will prompt the user for the input. In the following, all input parameters are documented:

\paragraph{Initial Conditions File}

Enter the filename of the initial conditions file, which was generated beforehand with \ttt{wigner.py}. If the script finds a file called \ttt{initconds}, the user is asked whether to use this file, otherwise the user has to enter an appropriate filename. The script detects the number of initial conditions and number of atoms automatically from the initial conditions file.

\paragraph{Range of Initial Conditions}

The initial conditions in \ttt{initconds} are indexed, starting with the index 1. In order to prepare ab initio calculations for a subset of all initial conditions, enter a range of indices, e.g.\ $a$ and $b$. This will prepare all initial conditions with indices in the interval $[a,b]$. In any case, the script will additionally prepare a calculation for the equilibrium geometry (except if a finished calculation for the equilibrium geometry was found).

If the interval $[0,0]$ is given, the script will only setup the calculation at the equilibrium geometry.

\paragraph{Number of states}

Here the user can specify the number of excited states to be calculated. Note that the ground state has to be counted as well, e.g., if 4 singlet states are specified, the calculation will involve the $S_0$, $S_1$, $S_2$ and $S_3$. Also states of higher multiplicity can be given, e.g.\ triplet or quintet states. 
For even-electron molecules, including odd-electron states (e.g.\ doublets) is only useful if transition properties for ionization can be computed (e.g.\ Dyson norms with some of the interfaces). These transition properties can be used to calculate ionization spectra or to obtain initial conditions for dynamics after ionization.

\paragraph{Interface}

In this point, choose any of the displayed interfaces to carry out the ab initio calculations. Enter the corresponding number. 

\paragraph{Spin-orbit calculation}

Usually, it is sufficient to calculate the spin-free excitation energies and oscillator strengths in order to decide for the initial state. However, using this option, the effects of spin-orbit coupling on the excitation energies and oscillator strengths can be included. Note that the script will never calculate spin-orbit couplings if only singlet states are included in the calculation, or if the chosen interface does not support calculation of spin-orbit couplings.

\paragraph{Reference overlaps}

The calculations can be setup in such a way that the wave function overlaps between states at the equilibrium geometry and the displaced geometries is computed.
This allows correlating the states at the displaced geometries with the reference states, such that one can know the state characters of all states without inspection.
This is useful for a crude ``diabatization'' of the states, e.g., if one wants to start all trajectories in the $n\pi^*$ state of the molecule although this state can be $S_1$, $S_2$, or $S_3$ (use \ttt{excite.py} to setup initial conditions in such a way, see section~\ref{sec:excite.py}).

When activating this option, keep in mind that the calculation in \ttt{ICOND\_00000} must be successfully finished before any of the other \ttt{ICOND\_XXXXX} calculations can be started.

\paragraph{TheoDORE analysis}

If the chosen interface supports wave function analysis with TheoDORE, then this option can be activated here.
\ttt{setup\_init.py} will then include the relevant keywords in the computations, and the results of the TheoDORE analysis will be written to the \ttt{QM.out} files.

The remaining settings for TheoDORE (fragment and descriptor input) will be asked later in \ttt{setup\_init.py}.

\subsection{Interface-specific input}

The following input in \ttt{setup\_init.py} depends on the chosen interface.
However, some input is similar between several interfaces and is discussed (out of input order) in section~\ref{sec:setup_init.py:several}.
Note that most of the questions asked in the input dialogue have some counterpart in the resource file of the chosen interface, so you might find additional information in chapter~\ref{chap:interfaces}.

% -----------------------------------------------------------------------------------------

\subsubsection{Input which is similar between interfaces}\label{sec:setup_init.py:several}

\paragraph{Path to quantum chemistry programs and licenses}

Here the user is prompted to provide the path to the relevant executables or installation directories. 

Note that the setup script will not expand the user (\ttt{\textasciitilde}) and shell variables (since the calculations might be running on a different machine than the one used for setup, so the meaning of shell variables could be different). \ttt{\textasciitilde} and shell variables will only be expanded by the interfaces during the actual calculation.

For some interfaces, also the path to a license file might need to be specified.

\paragraph{Scratch directory}

The script takes the string without any checking. Each individual initial condition uses a subdirectory of the given path, so that there are no clashes between the initial conditions. \ttt{\textasciitilde} and shell variables will only be expanded by the interface during the actual calculation.

Note that you can use, e.g., \ttt{./temp/} as scratch directory, which is a subdirectory of the working directory of the interface. Using \ttt{./} as scratch directory is not recommended, since the interface will delete the scratch directory.

\paragraph{Template file}

For almost all interfaces, \ttt{setup\_init.py} will ask for a template file containing the quantum chemistry specifics of the calculations. The format of the template file depends on the interface; formats are described in the interface chapter~\ref{chap:interfaces}.

For most interfaces, \ttt{setup\_init.py} will perform some basic checks, but ultimately the template file will be checked by the interface once the calculation is submitted.

\paragraph{Initial orbital guess}

For some interfaces, \ttt{setup\_init.py} will ask for files containing initial orbital guesses. providing initial is especially important for multi-reference calculations, because otherwise it is very likely that the calculations converge to an undesirable active space.

\paragraph{Resource usage}

For most interfaces, \ttt{setup\_init.py} will ask for the amount of memory and the number of CPU cores to use. For some interfaces, additionally a delay between the start of parallel calculation can be provided (this might be helpful if many calculations starting at the same time is problematic for I/O).
For the \textsc{Molcas}, ADF, \ttt{Gaussian}, and \textsc{Orca} interfaces, furthermore the \ttt{schedule\_scaling} is queried for (see section~\ref{met:amdahl} for details).

\paragraph{Wavefunction overlap program}

The scripts asks for the path to the wavefunction overlap program. In a complete \sharc\ installation, the overlap program should be located at \ttt{\$/SHARC/wfoverlap.x}.
Depending on the interface, the script might also ask for the threshold to truncate the wave function files, or for the memory for \ttt{wfoverlap.x} (if the memory of the quantum chemistry program cannot be set).
For some interfaces, it is also possible to control the number of frozen core orbitals for the overlap calculation (and the number of inactive orbitals for Dyson orbital calculations).

\paragraph{\textsc{TheoDORE} setup}

If \textsc{TheoDORE} is enabled, \ttt{setup\_init.py} will query for the path to the \textsc{TheoDORE} installation directory. Furthermore, the user needs to enter a list of the descriptors to be calculated by \textsc{TheoDORE}, as well as the atom indices for each fragment for the charge transfer number computation.

\paragraph{QM/MM setup}

For some interfaces, \ttt{setup\_init.py} will ask for QM/MM-related files if the template file specifies a QM/MM calculation.
Currently, the QM/MM setup consists simply of providing the relevant interface-specific files to \ttt{setup\_init.py}, which it simply copies accordingly.

% -----------------------------------------------------------------------------------------

\subsubsection{Input for \textsc{Molpro}}\label{sec:setup_init.py:molpro}

\paragraph{Path to \textsc{Molpro}}

Here the user is prompted to provide the path to the \textsc{Molpro} executable. 
For more details, see section~\ref{sec:setup_init.py:several}.

\paragraph{Scratch directory}

See section~\ref{sec:setup_init.py:several}.

\paragraph{Template file}

Enter the filename for the \textsc{Molpro} template input. This file should contain at least the basis set, active orbitals and electrons, number of electrons and number of states for state-averaging. For details, see the section about the \sharc-\textsc{Molpro} interface (\ref{sec:int:molpro}). The setup script will check whether the template file contains the necessary entries. 

\paragraph{Initial wave function file}

You can provide an initial wave function (with an appropriate MO guess) to \textsc{Molpro}. This usually provides a drastic speedup for the CASSCF calculations and helps in ensuring that all calculations are based on the same active space. In simple cases it might be acceptable to not provide an initial wave function.

\paragraph{Resource usage}

The script asks for the memory and number of CPU cores to be used. Note that both settings apply to \textsc{Molpro} and to \ttt{wfoverlap.x}, if the latter is required (but note that the two programs are never run simultaneously).
If more than one CPU core is used, the interface will parallelize different \textsc{Molpro} runs (but will not run \textsc{Molpro} in parallel mode); \ttt{wfoverlap.x} will be run in SMP-parallel mode.

\paragraph{Wavefunction overlap program}

See section~\ref{sec:setup_init.py:several}. Note that for the \textsc{Molpro} interface, no truncation threshold can be given, since \ttt{wfoverlap.x} is very efficient for CASSCF wavefunctions.
% -----------------------------------------------------------------------------------------

\subsubsection{Input for \textsc{Molcas}}\label{sec:setup_init.py:molcas}

\paragraph{Path to \textsc{Molcas}}

Here the user is prompted to provide the path to the \textsc{Molcas} directory. 
For more details, see section~\ref{sec:setup_init.py:several}.

\paragraph{Scratch directory}

See section~\ref{sec:setup_init.py:several}.

\paragraph{Template file}

Enter the filename for the \textsc{Molcas} template input. This file should contain at least the basis set, active orbitals and electrons, number of inactive orbitals, and number of states for state-averaging. For details, see the section about the \sharc-\textsc{Molcas} interface (\ref{sec:int:molcas}). The setup script will check whether the template file contains the necessary entries. 

\paragraph{QM/MM}

If the keyword \ttt{qmmm} is contained in the template file, the user will be queried for the path to \textsc{Tinker}'s \ttt{bin/} directory. Note that only \textsc{Tinker} installations with the necessary modifications for \textsc{Molcas} can be used.

The script will also ask for the key file (containing the path to the force field file and the QM/MM partition) and the connection table file. See section~\ref{sec:int:molcas} for details.

\paragraph{Initial wave function file}

You can provide initial MO coefficients to \textsc{Molcas}. This usually provides a drastic speedup for the CASSCF calculations and helps in ensuring that all calculations are based on the same active space. In simple cases it might be acceptable to not provide an initial wave function. For \textsc{Molcas}, you have to specify one file for each multiplicity (but you can use the same file as guess for several multiplicities).
Initial orbital files can either be in \ttt{JobIph} or \ttt{RasOrb} format.

\paragraph{Resource usage}

The script asks for the memory and number of CPU cores to be used. Note that both settings apply to \textsc{Molcas} and to \ttt{wfoverlap.x}, if the latter is required (but note that the two programs are never run simultaneously).
If more than one CPU core is used, the interface will parallelize different \textsc{Molcas} runs (but will not run \textsc{Molcas} in parallel mode); \ttt{wfoverlap.x} will be run in SMP-parallel mode.

\paragraph{Wavefunction overlap program}

See section~\ref{sec:setup_init.py:several}. The \ttt{wfoverlap.x} program is only required if Dyson norms need to be calculated. Note that for the \textsc{Molcas} interface, no truncation threshold can be given, since \ttt{wfoverlap.x} is very efficient for CASSCF wavefunctions.

% -----------------------------------------------------------------------------------------

\subsubsection{Input for \textsc{Columbus}}\label{sec:setup_init.py:columbus}

\paragraph{Path to \textsc{Columbus}}

Here the user is prompted to provide the path to the \textsc{Columbus} directory. 
For more details, see section~\ref{sec:setup_init.py:several}.

\paragraph{Scratch directory}

See section~\ref{sec:setup_init.py:several}.

\paragraph{Template directory}

Enter the path to a directory containing subdirectories with \textsc{Columbus} input files necessary for the calculations. The setup script will expand \ttt{\textasciitilde} and shell variables and will pass the absolute path to the calculations.

The script will auto-detect (based on the \ttt{cidrtin} files) which subdirectory contains the input for each multiplicity. The user has to check in the following dialog whether the association of multiplicities with job directories is correct. Additionally, the association of MO coefficient files to the jobs has to be checked. See the section on the \sharc-\textsc{Columbus} interface (\ref{sec:int:columbus}) for further details. 

Note that the setup script does not check any content of the input files beyond the multiplicity. Note that \ttt{transmomin} and \ttt{control.run} do not need to be present (and that their content has no effect on the calculation), since these files are written on-the-fly by the interface. 

The template directory can either be linked or copied to the initial condition calculations.

\paragraph{Initial orbital coefficient file}

You can provide initial MO coefficients for the \textsc{Columbus} calculation. This usually provides a drastic speedup for the CASSCF calculations and helps to ensure that all calculations are based on the same active space. In simple cases it might be acceptable to not provide an initial wave function.

\paragraph{Memory}

For \textsc{Columbus}, the available memory must be given here. The \textsc{Columbus} interface does not allow for parallelization currently.

\paragraph{Wavefunction overlap program}

See section~\ref{sec:setup_init.py:several}. For CASSCF calculations with \textsc{Columbus}, use a truncation threshold of 1.0, for MRCIS/MRCISD calculations, 0.97-0.99 should provide sufficient performance and accuracy.

% -----------------------------------------------------------------------------------------

\subsubsection{Input for analytical potentials}\label{sec:setup_init.py:analytical}

\paragraph{Template file}

Enter the filename for the template input specifying the analytical potentials. For details, see the section about the \sharc-Analytical interface (\ref{sec:int:analytical}). The setup script will check whether the template file is valid. 

% -----------------------------------------------------------------------------------------

\subsubsection{Input for \textsc{ADF}}\label{sec:setup_init.py:adf}

\paragraph{Path to \textsc{ADF}}

Here the user is first asked if setup should be made from an \ttt{adfrc.sh} file. If yes, only the path to this file is needed.

Otherwise, the path to the ADF installation directory and the path to the license file need to be provided.
For more details, see section~\ref{sec:setup_init.py:several}.

\paragraph{Scratch directory}

See section~\ref{sec:setup_init.py:several}.

\paragraph{Template file}

Enter the filename for the \textsc{ADF} template input. This file should contain at least the basis set, functional, and charge keywords. For details, see the section about the \sharc-\textsc{ADF} interface (\ref{sec:int:adf}). The setup script will check whether the template file contains the necessary entries. 

\paragraph{QM/MM}

The script will ask for the two QM/MM-specific input files for the interface, which are called \ttt{ADF.qmmm.ff} and \ttt{ADF.qmmm.table}.
See section~\ref{sec:int:adf} for details on these files.

\paragraph{Initial orbital coefficient file}

You can provide initial MO coefficients for the \textsc{ADF} calculation. This can provide a small speedup for the calculations and helps to ensure that all calculations are based on the same orbitals.
In many cases, no initial orbitals are required.

\paragraph{Resource usage}

For \textsc{ADF}, the amount of memory to be used cannot be controlled. However, the number of CPU cores needs to be given. If more than one core is used, also the parallel scaling needs to be specified (see section~\ref{met:amdahl}).

\paragraph{Wavefunction overlap program}

See section~\ref{sec:setup_init.py:several}. \ttt{setup\_init.py} will ask for the memory usage of \ttt{wfoverlap.x}. The truncation threshold for TD-DFT can usually be chosen as 0.99--1.00 (depending on the system and functional, but it is recommended that it is high enough that each state is described by around 100 determinants in the generated files). See section~\ref{sec:int:adf} for details.

\paragraph{\textsc{TheoDORE} setup}

See section~\ref{sec:setup_init.py:several}. 

% -----------------------------------------------------------------------------------------

\subsubsection{Input for \textsc{Ricc2}}\label{sec:setup_init.py:ricc2}

\paragraph{Path to \textsc{Turbomole}}

The path to the \textsc{Turbomole} installation directory needs to be provided.
If spin-orbit couplings are requested, also the path to the \textsc{Orca} installation directory is needed.
For more details, see section~\ref{sec:setup_init.py:several}.

\paragraph{Scratch directory}

See section~\ref{sec:setup_init.py:several}.

\paragraph{Template file}

Enter the filename for the \textsc{Ricc2} template input. This file should contain at least the basis set and charge keywords. For details, see the section about the \sharc-\textsc{Ricc2} interface (\ref{sec:int:ricc2}). The setup script will check whether the template file contains the necessary entries. 

\paragraph{Initial orbital coefficient file}

You can provide initial MO coefficients for the \textsc{Turbomole} calculation. This can provide a small speedup for the calculations and helps to ensure that all calculations are based on the same orbitals.
In many cases, no initial orbitals are required.

\paragraph{Resource usage}

The amount of memory and the number of CPU cores are required in this section.
Note that the \sharc-\textsc{Ricc2} interface always runs only one \textsc{Turbomole} instance at the same time (if the number of CPU cores is larger than one, it will use the SMP/OMP binaries of \textsc{Turbomole}).

\paragraph{Wavefunction overlap program}

See section~\ref{sec:setup_init.py:several}. \ttt{setup\_init.py} will ask for the memory usage of \ttt{wfoverlap.x}. The truncation threshold can usually be chosen as 0.99--1.00 (depending on the system, but it is recommended that it is high enough that each state is described by around 100 determinants in the generated files). See section~\ref{sec:int:ricc2} for details.

\paragraph{\textsc{TheoDORE} setup}

See section~\ref{sec:setup_init.py:several}.

% -----------------------------------------------------------------------------------------

\subsubsection{Input for LVC interface}\label{sec:setup_init.py:lvc}

\paragraph{Template file}

Enter the filename for the template input specifying the LVC parameters. For details, see the section about the \sharc-LVC interface (\ref{sec:int:lvc}). The setup script will not check whether the template file is valid. 

% -----------------------------------------------------------------------------------------

\subsubsection{Input for \textsc{Gaussian} interface}\label{sec:setup_init.py:gaussian}

\paragraph{Path to \textsc{Gaussian}}

The path to the \textsc{Gaussian} installation directory needs to be provided.
Note that this needs to be the path to the directory containing the \textsc{Gaussian} executables, e.g., \ttt{g09}/\ttt{g16}, \ttt{l9999.exe}, etc.
The interface will automatically detect which \textsc{Gaussian} version is used.
For more details, see section~\ref{sec:setup_init.py:several}.

\paragraph{Scratch directory}

See section~\ref{sec:setup_init.py:several}.

\paragraph{Template file}

Enter the filename for the \textsc{Gaussian} template input. This file should contain at least the basis set, functional, and charge keywords. For details, see the section about the \sharc-\textsc{Gaussian} interface (\ref{sec:int:gaussian}). The setup script will check whether the template file contains the necessary entries. 

\paragraph{Initial orbital coefficient file}

You can provide initial MO coefficients for the \textsc{Gaussian} calculation. This can provide a small speedup for the calculations and helps to ensure that all calculations are based on the same orbitals.
In many cases, no initial orbitals are required.

\paragraph{Resource usage}

The amount of memory and the number of CPU cores are required in this section. If more than one core is used, also the parallel scaling needs to be specified (see section~\ref{met:amdahl}).

\paragraph{Wavefunction overlap program}

See section~\ref{sec:setup_init.py:several}. \ttt{setup\_init.py} will ask for the memory usage of \ttt{wfoverlap.x}. The truncation threshold for TD-DFT can usually be chosen as 0.99--1.00 (depending on the system and functional, but it is recommended that it is high enough that each state is described by around 100 determinants in the generated files). See section~\ref{sec:int:gaussian} for details.

\paragraph{\textsc{TheoDORE} setup}

See section~\ref{sec:setup_init.py:several}. 

% -----------------------------------------------------------------------------------------

\subsection{Input for Run Scripts}\label{sec:setup_init.py:run}

\paragraph{Run script mode}

The script \ttt{setup\_init.py} generates a run script (Bash) for each initial condition calculation. Due to the large variety of cluster architectures, these run scripts might not work in every case. It is the user's responsibility to adapt the generated run scripts to his needs.

\ttt{setup\_init.py} can generate run scripts for two different schemes how to execute the calculations. With the first scheme, the ab initio calculations are performed in the directory where they were setup (subdirectories of the directory where \ttt{setup\_init.py} was started). Note that the interfaces will still use their scratch directories to perform the actual quantum chemistry calculations. Currently, this is the default option.

With the second option, the run scripts will transfer the input files for each ab initio calculation to a temporary directory, where the interface is started. After the interface finishes all calculations, the results files are transferred back to the primary directory and the temporary directory is deleted. Note that \ttt{setup\_init.py} in any case creates the directory structure in the directory where it was started. The name of the temporary directory can contain shell variables, which will be expanded when the script is running (on the compute host).

\paragraph{Submission script}

The setup script can also create a Bash script for the submission of all ab initio calculations to a queueing system. The user has to provide a submission command for that, including any options which might be necessary. This submission script might not work with all queueing systems.

\paragraph{Project name}

The user can enter a project name. This is used currently only for the job names of submitted jobs (\ttt{-N} option for queueing system).

\subsection{Output}

\ttt{setup\_init.py} will create for each initial condition in the given range a directory whose names follow the format \ttt{ICOND\_\%05i/}, where \ttt{\%05i} is the index of the initial condition padded with zeroes to 5 digits. Additionally, the directory \ttt{ICOND\_00000/} is created for the calculation of the excitation energies at the equilibrium geometry.

To each directory, the following files will be added:
\begin{itemize}
  \item \ttt{QM.in}: Main input file for the interface, contains the geometry and the control keywords (to specify which quantities need to be calculated). 
  \item \ttt{run.sh}: Run script, which can be started interactively in order to perform the ab initio calculation in this directory. Can also be adapted to a batch script for submission to a queue 
  \item Interface-specific files: Usually a template file, a resource file, and an initial wave function.
\end{itemize}

The calculations in each directory can be simply executed by starting \ttt{run.sh} in each directory. In order to perform this task consecutively on a single machine, the script \ttt{all\_run.sh} can be executed. The file \ttt{DONE} contains the progress of this calculation.
Alternatively, each run script can be sent to a queueing system (you might need to adapt this script to you cluster system).
Note that if reference overlaps were requested, the calculation in \ttt{ICOND\_00000/} must be finished before starting any of the other calculations.

In figure~\ref{fig:dirs_init}, the directory tree structure setup by \ttt{setup\_init.py} is given.

\begin{figure}
  \centering
  \includegraphics[scale=1]{img/dirs_init/dirs_init.pdf}
  \caption[Directory structure created by \ttt{setup\_init.py}.]{Directory structure created by \ttt{setup\_init.py}. Directories are in blue, executable scripts in green and regular files in black and white. Interface files usually include initial MO coefficients, template files and interface input files.}
  \label{fig:dirs_init}
\end{figure}

After all calculations are finished, \ttt{excite.py} can be used to collect the results.




% ========================================================================================================= %

\section{Excitation Selection: \ttt{excite.py}}\label{sec:excite.py}

\ttt{excite.py} has two tasks: adding excited-state information to the \ttt{initconds} file, and deciding which excited state for which initial condition is a valid initial state for the dynamics.


\subsection{Usage}

The script is interactive, and can be started by simply typing 
\begin{verbatim}
user@host> $SHARC/excite.py
\end{verbatim}


\subsection{Input}

\paragraph{Initial condition file}

Enter the path to the initial conditions file, to which \ttt{excite.py} will add excited-state information. This file can already contain excited-state information (in this case this information can be reused).

\paragraph{Generate excited state list}

There are three possibilities to add excited-state information to the \ttt{initconds} file:
\begin{enumerate}
  \item generate a list of dummy excited states,
  \item read excited-state information from the output of the initial ab initio calculations (prepare the calculations with \ttt{setup\_init.py}),
  \item keep the existing excited-state information in the \ttt{initconds} file.
\end{enumerate}
The first option is mainly used if no initial ab initio calculations need to be performed (e.g., the initial state is known). 

In order to use the second option, one should first setup initial excited-state calculations using \ttt{setup\_init.py} (see~\ref{sec:setup_init.py}) and run the calculations. \ttt{excite.py} can then read the output of the initial calculations and calculate excitation energies and oscillator strengths.

The third option can be used to reuse the information in the \ttt{initconds} file, e.g., to apply a different selection scheme to the states or to just read the number of states.

\paragraph{Path to ab initio results}

If \ttt{excite.py} will read the excited-state information from the ab initio calculation results, here the user has to provide the path to the directory containing the \ttt{ICOND\_\%05i} subdirectories.

\paragraph{Number of states}

If a dummy list of states will be generated, the user has to provide the number of states per multiplicity. Note that a singlet ground state has to be counted as well, e.g.\ if 4 singlet states are specified, the calculation will involve the $S_0$, $S_1$, $S_2$ and $S_3$. Also states of higher multiplicity can be given, e.g.\ doublet or triplet states (e.g., \ttt{2 2 1} for two singlets, two doublets and one triplet).

If the ab initio results are read the number of states will be automatically determined from the results.

\paragraph{Excited-state representation}

When generating new lists of excited states (either dummy states or from ab initio results), the user has to specify the representation of the excited states (either MCH or diagonal representation). The MCH representation is spin-free, meaning that transition dipole moments are only allowed between states of the same multiplicity. For molecules without heavy atoms, this option is sufficient. For heavier atoms, the diagonal representation can be used, which includes the effects of spin-orbit coupling on the excitation energies and oscillator strengths. 
Note, however, that excited-state selection with delta pulse excitation (option 3 under ``Initial state selection'') should be carried out in the MCH representation if the ground state is not significantly spin-orbit-mixed.

When reading ab initio results, \ttt{excite.py} will diagonalize the Hamiltonian and transform the transition dipole matrices for each initial condition to obtain the diagonal representation. 

When a dummy state list is generated, the representation will only be written to \ttt{initconds.excited} (but has no actual numeric effect for \ttt{excite.py}). Note that the representation which is declared in the \ttt{initconds.excited} file influences how \sharc\ determines the initial coefficients (see the paragraph on initial coefficients in~~\ref{ssec:input:keywords}).

Note that the representation cannot be changed if existing excited-state information is kept. 

Hint: If the \ttt{ICOND\_\%05i} directories need to be deleted (e.g., due to disk space restrictions), making one read-out with \ttt{excite.py} for each representation and saving the results to two different files will preserve most necessary information.

\paragraph{Ionization probabilities}

If \ttt{excite.py} detects that the ab initio results contain ionization probabilities, then those can be used instead of the transition dipole moments. Note that in this case the transition dipole moments are not written to the \ttt{initconds.excited} file. 

\paragraph{Reference energy}

\ttt{excite.py} can read the reference energy (ground state equilibrium energy) directly from the ab initio results. If the ab initio data is read anyways, \ttt{excite.py} already knows the relevant path. If a dummy list of states is generated, the user can provide just the path to the \ttt{QM.out} file of the ab initio calculation for the equilibrium geometry. Otherwise, \ttt{excite.py} will prompt the user to enter a reference energy manually (in hartree).

\paragraph{Initial state selection}

Every excited state of each initial condition has a flag specifying it either as a valid initial state or not. \ttt{excite.py} has four modes how to flag the excited states:
\begin{enumerate}
  \item Unselect all excited states,
  \item User provides a list of initial states,
  \item States are selected stochastically based on excitation energies and oscillator strengths,
  \item Keep all existing flags.
\end{enumerate}
The first option can be used if \ttt{excite.py} is only used to read the ab initio results for the generation of an absorption spectrum (using \ttt{spectrum.py}). 

The second option can be used to directly specify a list of initial states, if the initial state is known (e.g., starting in the ground state and exciting with an explicit laser field). In this case, the given states of \textit{all} initial conditions are flagged as initial states. This option is also useful if reference overlaps were computed (see~\ref{sec:setup_init.py}.

The third option is only available if excited-state information exists (i.e., if no dummy list is generated). For details on the stochastic selection procedure, see section~\ref{met:exc_selection}. 

The fourth option can only be used if the existing state information is kept. In this case \ttt{excite.py} does nothing except counting the number of flagged initial states.

\paragraph{Excitation window}

This option allows to exclude excited states from the selection procedure if they are outside a given energy window. This option is only available if excited state information exists, but not if a dummy list of states is generated (because the dummy states have no defined excitation energy).

For the stochastic selection procedure, states outside the excitation window do not count for the determination of $p_\text{max}$ (see equation~\eqref{eq:exc_prob}). This allows to excite, e.g., to a dark $n\pi^*$ state despite the presence of a much brighter $\pi\pi^*$ state.

For the keep-flags option, this option can be used to count the number of excited states in the energy window.

\paragraph{Considered states}

Here the user can specify the list of desired initial states. 
If reference overlaps are present in the excitation calculations, then the user can choose to specify the initial state in terms of diabatized states (as defined by the overlap with the reference, where the diabatized states are identical to the computed states).
See section~\ref{met:exc_diabatic} for how the diabatization is carried out.

For the stochastic selection procedure, the user can instead exclude certain states from the procedure. Excluded states do not count for the determination of $p_\text{max}$ (see equation~\eqref{eq:exc_prob}).

If the number of states per multiplicity is known, \ttt{excite.py} will print a table giving for each state index the multiplicity, quantum number and $M_s$ value.


\paragraph{Random number generator seed}

The random number generator in \ttt{excite.py} is used in the stochastic selection procedure. Instead of typing an integer, typing ``\ttt{!}'' will initialize the RNG from the system time. Note that this will not be reproducible, i.e.\ repeating the \ttt{excite.py} run with ``\ttt{!}'' as random seed will give a different selection in each run.



\subsection{Matrix diagonalization}

When using the diagonal representation, \ttt{excite.py} needs to diagonalize and multiply matrices. By default, the Python package \textsc{NumPy} is used, if available. If the script does not find a \textsc{NumPy} installation, it will use a small Fortran code which comes with the \sharc\ suite. In order for this to work, you need to set the environment variable \ttt{\$SHARC} to the \ttt{bin/} directory within your \sharc\ installation. See section~\ref{sec:diagonalizer.x} for more details.

\subsection{Output}

\ttt{excite.py} writes all output to a file \ttt{<BASE>.excited}, where \ttt{<BASE>} is the name of the initial conditions file used as input. The output file is also an initial conditions file, but contains additional information regarding the excited states, the reference energy and the representation of the excited states. An initial conditions file with excited-state information is needed for the final preparatory step: setting up the dynamics with \ttt{setup\_traj.py}.
Additionally, \ttt{spectrum.py} can calculate absorption spectra from excited-state initial condition files.

\subsection{Specification of the \ttt{initconds.excited} file format}\label{sec:initcondsfile}

The initial conditions files \ttt{initconds} and \ttt{initconds.excited} contain lists of initial conditions, which are needed for the setup of trajectories. An initial condition is a set of initial coordinates of all atoms and corresponding initial velocities of each atom, and optionally a list of excited state informations. In the following, the format of this file is specified.

The file contains of a header, followed by the body of the file containing a list of the initial conditions. 

\paragraph{File header}

An examplary header looks like:
\begin{example}
\footnotesize\begin{verbatim}
SHARC Initial conditions file, version 0.2   <Excited>
Ninit     100
Natom     2
Repr      MCH
Eref         -0.50
Eharm           0.04
States    2 0 1 

Equilibrium
 H   1.0  0.0  0.0  0.0   1.00782503   0.0  0.0  0.0
 H   1.0  1.5  0.0  0.0   1.00782503   0.0  0.0  0.0
\end{verbatim}
\end{example}
The first line must read \ttt{SHARC Initial conditions file, version <VERSION>}, with the correct version string followed. The string \ttt{Excited} is optional, and marks an initial conditions file as being an output file of \ttt{excite.py} (\ttt{setup\_traj.py} will only accept files marked like this). The following lines contain:
\begin{enumerate}
  \item the number of initial conditions, 
  \item the number of atoms,
  \item the electronic state representation (a string which is \ttt{None}, \ttt{MCH} or \ttt{diag}),
  \item the reference energy (hartree),
  \item the harmonic energy (zero point energy in the harmonic approximation, hartree),
  \item optionally the number of states per multiplicity.
\end{enumerate}

After the header, first the equilibrium geometry is expected. It is demarked with the keyword \ttt{Equilibrium}, followed by $n_\text{atom}$ lines, each specifying one atom. Unlike the actual initial conditions, the equilibrium geometry does not have a list of excited states or defined energies.

\paragraph{File body}

The file body contains a list of initial conditions. Each initial condition is specified by a block starting with a line containing the string \ttt{Index} and the number of the initial condition. In the file, the initial conditions are expected to appear in order.

A block specifying an initial condition looks like:
\begin{example}
\footnotesize\begin{verbatim}
Index     1
Atoms
 H   1.0  -0.02  0.0  0.0   1.00782503  -0.001  0.0  0.0
 H   1.0   1.52  0.0  0.0   1.00782503   0.001  0.0  0.0
States
001    -0.49    -0.49  -0.16   0.0  -0.03   0.0   0.05   0.0   0.0   0.00 False
002    -0.25    -0.49   0.02   0.0   0.43   0.0  -1.77   0.0   6.5   0.53 True
003    -0.40    -0.49   0.00   0.0   0.00   0.0   0.00   0.0   2.5   0.00 False
004    -0.40    -0.49   0.00   0.0   0.00   0.0   0.00   0.0   2.5   0.00 False
005    -0.40    -0.49   0.00   0.0   0.00   0.0   0.00   0.0   2.5   0.00 False
Ekin        0.004 a.u.
Epot_harm   0.026 a.u.
Epot        0.013 a.u.
Etot_harm   0.030 a.u.
Etot        0.018 a.u.
\end{verbatim}
\end{example}
The formal structure of such a block is as follows. After the line containing the keyword \ttt{Index} and the index number, the keyword \ttt{Atoms} indicates the start of the list of atoms. Each atom is specified on one line:
\begin{enumerate}
  \item symbol,
  \item nuclear charge,
  \item $x$, $y$, $z$ coordinate in Bohrs,
  \item atomic mass,
  \item $x$, $y$ and $z$ component of nuclear velocity in atomic units.
\end{enumerate}

After the atom list, the keyword \ttt{States} indicates the list of electronic states. This list consists of one line per electronic state, but can be empty, if no information of the electronic states is available. Each line consists of: 
\begin{enumerate}
  \item state number (starting with 1),
  \item state energy in Hartree, 
  \item reference energy in Hartree (usually the energy of the lowest state),
  \item six numbers defining the transition dipole moment to the reference state (usually the lowest state),
  \item the excitation energy in eV,
  \item the oscillator strength,
  \item a string which is either \ttt{True} or \ttt{False}, specifying whether the electronic state was selected by \ttt{excite.py} as initial electronic state. 
\end{enumerate}
The transition dipole moments are specified by six floating point numbers, which are real part of the $x$ component, imaginary part of the $x$ component, then the real and imaginary parts for the $y$ and finally the $z$ component (the transition dipole moments can be complex in the diagonal representation). 

The electronic state list is terminated with the keyword \ttt{Ekin}, which at the same time gives the kinetic energy of all atoms. The remaining entries give the potential energy in the harmonic approximation and the actual potential energy, as well as the total energy.







% ========================================================================================================= %

\section{Setup of Trajectories: \ttt{setup\_traj.py}}\label{sec:setup_traj.py}

This interactive script prepares the input for the excited-state dynamics simulations with \sharc. It works similarly to \ttt{setup\_init.py}, reading an initial conditions file, prompting the user for a number of input parameters, and finally prepares one directory per trajectory. However, the \ttt{setup\_traj.py} input section is noticeably longer, because most options for the \sharc\ dynamics are covered.

\subsection{Input}

\paragraph{Initial conditions file}

Please be aware that \ttt{setup\_traj.py} needs an initial conditions file generated by \ttt{excite.py} (files generated by \ttt{wigner.py}, \ttt{amber\_to\_initconds.py}, or \ttt{sharctraj\_to\_initconds.py} are not allowed). The script reads the number of initial states, the representation, and the reference energy automatically from the file.

\paragraph{Number of states}

This is the total number of states per multiplicity included in the dynamics calculation. Affects the keyword \ttt{nstates} in the \sharc\ input file.

Only advanced users should use here a different number of states than given to \ttt{setup\_init.py}. In this case, the excited-state information in the initial conditions file might be inconsistent. For example, if 10 singlets and 10 triplets were included in the initial calculations, but only 5 singlets and 5 triplets in the dynamics, then the sixth entry in the initial conditions file corresponds to $S_5$, while \ttt{setup\_traj.py} assumes the sixth entry to correspond to $T_1$.

\paragraph{Active states}

States can be frozen for the dynamics calculation here. See section~\ref{met:activestates} for a general description of state freezing in \sharc. Only the highest states in each multiplicity can be frozen, it is not possible to, e.g., freeze the ground state in simulations where ground state relaxation is negligible. Affects the keyword \ttt{actstates}.

\paragraph{Contents of the initial conditions file}

Optionally, a map of the contents of the initial conditions file can be displayed during the execution of \ttt{setup\_traj.py}, showing for each state which initial conditions were selected (and which initial conditions do not have the necessary excited-state information). For each state, a table is given, where each symbol represents one initial condition. A dot ``\ttt{.}'' represents an initial condition where information about the current excited state is available, but which is not selected for dynamics. A hash mark ``\ttt{\#}'' represents an initial condition which is selected for dynamics. A question mark ``\ttt{?}'' represents initial conditions for which no information about the excited state is available (e.g.\ if the initial excited-state calculation failed). The tutorial shows an example of this output.

The content of the initial conditions file is also summarized in a table giving the number of initial conditions selected per state. 

\paragraph{Initial states for dynamics setup}

The user has to input all states from which trajectories should be launched. The numbers must be entered according to the above table giving the number of selected initial conditions per state. It is not allowed to specify inactive states as initial states. The script will give the number of trajectories which can be setup with the specified set of states. If no trajectories can be setup, the user has to specify another set of initial states. The initial state will be written to the \sharc\ input, specified in the same representation as given in the initial conditions file. The initial coefficients will be determined automatically by \sharc, according to the description in section~\ref{ssec:input:keywords}.

\paragraph{Starting index for dynamics setup}

Specifies the first initial condition within the initial condition file to be included in the setup. This is useful, for example, if the user might setup 50 trajectories starting with index 1. \ttt{setup\_traj.py} reports afterwards the last initial condition to be used for setup, e.g.\ index 90. Later, the user can setup additional trajectories, starting with index 91.

\paragraph{Random number generator seed}

The random number generator in \ttt{setup\_traj.py} is used to randomly generate RNG seeds for the \sharc\ input. Instead of typing an integer, typing ``\ttt{!}'' will initialize the RNG from the system time. Note that this will not be reproducible, i.e.\ repeating the \ttt{setup\_traj.py} run (with the same input) with ``\ttt{!}'' as random seed will give for the same trajectories different RNG seeds. Affects the keyword \ttt{RNGseed}.

\paragraph{Interface}

In this point, choose any of the displayed interfaces to carry out the ab initio calculations. Enter the corresponding number. The choice of the interface influences some dynamics options which can be set in the next section of the \ttt{setup\_traj.py} input.

\paragraph{Simulation Time}

This is the maximum time that \sharc\ will run the dynamics simulation. If trajectories need to be run for longer time, it is recommended to first let the simulation finish. Afterwards, increase the simulation time in the corresponding \sharc\ input file (keyword \ttt{tmax}) and add the restart keyword (also make sure that the \ttt{norestart} keyword is not present). Then the simulation can be restarted by running again the \ttt{run.sh} script. Sets the keyword \ttt{tmax} in the \sharc\ input files.

\paragraph{Simulation Timestep}

This gives the time step for the dynamics. The on-the-fly ab initio calculations are performed with this time step, as is the propagation of the nuclear coordinates. A shorter time step gives more accurate results, especially if light atoms (hydrogen) are subjected to high kinetic energies or steep gradients. Of course a shorter time step is computationally more expensive. A good compromise in many situations is 0.5~fs. Sets the keyword \ttt{stepsize} in the \sharc\ input files.

\paragraph{Number of substeps}

This gives the number of substeps for the interpolation of the Hamiltonian for the propagation of the electronic wave function. Usually, 25 substeps are sufficient. In cases where the diagonal elements of the Hamiltonian are very large (very large excitation energies or a badly chosen reference energy) more substeps are necessary. Sets the keyword \ttt{nsubsteps} in the \sharc\ input files.

\paragraph{Prematurely terminate trajectories}

Usually, trajectories which relaxed to the ground state do not recross to an excited state, but vibrate indefinitely in the ground state. If the user is not interested in these vibrations, such trajectories can be terminated prematurely in order to save computational resources. A threshold of 10--20~fs is usually a good choice to safely detect ground state relaxation. Sets the keyword \ttt{killafter} in the \sharc\ input files.

\paragraph{Representation for the dynamics}

Either the diagonal representation can be chosen (by typing ``\ttt{yes}'') to perform dynamics with the \sharc\ methodology, or the dynamics can be performed on the MCH states (spin-diabatic dynamics \cite{Granucci2012JCP}, FISH \cite{Mitric2009PRA}). Sets the keyword \ttt{surf} in the \sharc\ input files.

\paragraph{Spin-orbit couplings}

If more than just singlet states are requested, the script asks whether spin-orbit couplings should be computed. If the chosen interface cannot provide spin-orbit couplings, this question is automatically answered.

\paragraph{Non-adiabatic couplings}

Electronic propagation can be performed with temporal derivatives, nonadiabatic coupling vectors or overlap matrices (Local diabatization). Enter the corresponding number. Note that depending on the chosen interface, some options might not be available, as displayed by \ttt{setup\_traj.py}. Also note that currently, no interface can provide temporal derivatives (because their computation involves calculating the overlap matrix and then local diabatization can be done instead). Sets the keyword \ttt{coupling} in the \sharc\ input files.

If nonadiabatic coupling vectors are chosen, the user is asked whether overlap matrices should be computed anyways to provide wave function phase information. As the overlap calculations are usually fast compared to other steps, this is recommended.

\paragraph{Gradient transformation}

The nonadiabatic coupling vectors can be used to correctly transform the gradients to the diagonal representation. If nonadiabatic coupling vectors are used anyways, this option is strongly recommended, since it gives more accurate gradients for no additional cost. Sets the keyword \ttt{gradcorrect} in the \sharc\ input files. If the dynamics uses the MCH representation, this question is not asked. 

\paragraph{Surface hop treatment}

This option determines how the total energy is conserved after a surface hop and whether frustrated hops lead to reflection. Sets the keywords \ttt{ekincorrect} and \ttt{reflect\_frustrated} in the \sharc\ input files.

\paragraph{Decoherence correction}

For most applications, a decoherence correction should be enabled. This controls the \ttt{decoherence\_scheme} (and \ttt{decoherence\_param} keywords in the \sharc\ input files. 

Note that \ttt{setup\_traj.py} does not allow to modify the $\alpha$ parameter for the energy-based decoherence (keyword \ttt{decoherence\_param}). In order to change \ttt{decoherence\_param}, the user has to manually edit the \sharc\ input files.

\paragraph{Surface hopping scheme}

Choose one of the available schemes to compute the hopping probabilities or turn off hopping.

\paragraph{Scaling and Damping}

These two prompts set the keywords \ttt{scaling} and \ttt{damping} in the \sharc\ input files. The scaling parameter has to be positive, and the damping parameter has to be in the interval $[0,1]$.

\paragraph{Atom masking}

In some cases, the script will ask to specify the atoms to which decoherence/rescaling/reflection should be applied.
See section~\ref{chap:input} for explanations (keyword \ttt{atommask}).

\paragraph{Gradient and nonadiabatic coupling selection}

For dynamics in the MCH representation, selection of gradients is used by default, and only one gradient (of the current state) is calculated. Selection of nonadiabatic couplings is only relevant if they are used (for propagation, gradient correction or rescaling of the velocities after a surface hop). For the selection threshold, usually 0.5~eV is sufficient, except if spin-orbit coupling is very strong and hence the gradients mix strongly.
Sets the keywords \ttt{grad\_select} and \ttt{nac\_select} in the \sharc\ input files.

\paragraph{Laser file}

The user can specify to use an external laser field during the dynamics, and has to provide the path to the laser file (see section~\ref{sec:laser.x} and~\ref{sec:laserfile}). \ttt{setup\_traj.py} will check whether the number of steps and the time steps are compatible to the dynamics. If the interface can provide dipole moment gradients, \ttt{setup\_traj.py} will also ask whether dipole moment gradients should be included in the simulations.

\paragraph{Dyson norm calculation}

If the interface is compatible, the user can request that Dyson norms are calculated on-the-fly. This option is only asked if Dyson norms can be computed (i.e., if states are present which differ by one electron, e.g., singlets and doublets).

\paragraph{\textsc{TheoDORE} calculations}

If the interface is compatible, the user can request that \textsc{TheoDORE} is run on-the-fly. 


\subsection{Interface-specific input}

This input section is basically the same as for \ttt{setup\_init.py} (section~\ref{sec:setup_init.py:several} and following sections). Note that for the dynamics simulations an initial wave function file is even more strongly recommended than for the initial excited-state calculations.

\subsection{Output control}

\ttt{setup\_traj.py} will ask a number of questions regarding the content of the \ttt{output.dat} file, specifically about writing gradients, nonadiabatic coupling vectors, properties (1D and 2D), and overlap matrices to this file.
Note that this is only possible if these quantities are actually calculated; otherwise, \ttt{sharc.x} will ignore these requestes.

\subsection{Run script setup}

Also this input section is very similar to the one in \ttt{setup\_init.py} (see section~\ref{sec:setup_init.py}).

\subsection{Output}

\begin{figure}
  \centering
  \includegraphics[scale=1]{img/dirs_traj/dirs_traj.pdf}
  \caption[Directory structure created by \ttt{setup\_traj.py}.]{Directory structure created by \ttt{setup\_traj.py}. Directories are in blue, executable scripts in green and regular files in black and white. Interface files usually include initial MO coefficients, template files and interface input files.}
  \label{fig:dirs_traj}
\end{figure}

\ttt{setup\_traj.py} will create for each initial state a directory where all trajectories starting in this state will be put. If the initial conditions file specified that the initial conditions are in the MCH representation, then the initial states will be assumed to be in the MCH representation as well. In this case, the directories will be named \ttt{Singlet\_0}, \ttt{Singlet\_1}, ..., \ttt{Doublet\_0}, \ttt{Triplet\_1}, ... If the initial states are in the diagonal representation, then the directories are simply called \ttt{X\_1}, ... since they do not have a definite spin.

In each directory, subdirectories called \ttt{TRAJ\_\%05i} are created, where \ttt{\%05i} is the initial condition index, padded to 5 digits with zeroes. In each trajectory's directory, an \sharc\ input file called \ttt{input} will be created, which contains all the dynamics options chosen during the \ttt{setup\_traj.py} run. Also, files \ttt{geom} and \ttt{veloc} will be created. For trajectories setup with \ttt{setup\_traj.py}, the determination of the initial wave function coefficients is done by \sharc.
Furthermore, in each trajectory directory a subdirectory \ttt{QM} is created, where the \ttt{runQM.sh} script containing the call to the interface is put. In the directory \ttt{QM} also all interface-specific input files will be copied.

For each trajectory, a \ttt{run.sh} script will be created, which can be executed to run the dynamics simulation. You might need to adapt the run script to your cluster setup.

\ttt{setup\_traj.py} also creates a script \ttt{all\_run\_traj.sh}, which can be used to execute all trajectories sequentially.
Note that this is intended for small test trajectories, and should not be used for expensive production trajectories.
For the latter, \ttt{setup\_traj.py} can optionally create a script \ttt{all\_qsub\_traj.sh}, which can be executed to submit all trajectories to a queueing system. You might need to adapt also this script to your cluster setup.

The full directory structure created by \ttt{setup\_traj.py} is given in figure~\ref{fig:dirs_traj}.







% ========================================================================================================= %

\section{Laser field generation: \ttt{laser.x}}\label{sec:laser.x}

The Fortran code \ttt{laser.x} can generate files containing laser fields which can be used with \sharc. It is possible to superimpose several lasers, use different polarizations and apply a number of chirp parameters.

\subsection{Usage}

The program is simply called by 
\begin{verbatim}
user@host> $SHARC/laser.x
\end{verbatim}
It will interactively ask for the laser parameters. After input is complete, it writes the laser field to the file \ttt{laser} in the format which \sharc\ expects (see~\ref{sec:laserfile}).

Similar to the interactive Python scripts, \ttt{laser.x} will also write the user input to \ttt{KEYSTROKES.laser}. After modifying this file, it can be used to directly execute \ttt{laser.x} without doing the interactive input again:
\begin{verbatim}
user@host> $SHARC/laser.x < KEYSTROKES.laser
\end{verbatim}

\subsection{Input}

The first four options are global and need to be entered only once, all remaining input options need to be given for every laser pulse. For the definition of laser fields see section~\ref{met:laser_field}.

\paragraph{Number of lasers}

Any number of lasers can be used. The output file will contain the sum of all laser pulses defined.

\paragraph{Real-valued field}

If this is true, the output file will only contain the real parts of the laser field, while the columns defining the imaginary part of the field will be zero. Note, however, that \sharc\ will anyways only use the real part of the field in the simulations.

\paragraph{Time interval and steps}

The definitions of the starting time, end time and time step of the laser field must exactly match the simulation time and time substeps of the \sharc\ simulation. Note, that the laser field must always start at $t$=0 fs to be used with \sharc. The end time for the laser field must therefore coincide with the total simulation time given in the \sharc\ input. The number of time steps for the laser field is $t_\text{total}/\Delta t_\text{sub} +1$.

\paragraph{Files for debugging}

This option is normally not needed, and can be set to False. If set to True, the chirped and unchirped laser fields in both time and frequency domain will be written to files called \ttt{DEBUG\_\dots}.

\paragraph{Polarization vector}

The polarization vector $\VEC{p}$ (will be normalized).

\paragraph{Type of envelope}

There are two options possible for the envelope function $\mathcal{E}(t)$, either a Gaussian envelope or a sinusoidal one (see~\ref{met:laser_field}).

\paragraph{Field strength}

There are two input lines for the field strength $\mathcal{E}_0$, the first defining the unit in which the field strength is defined, the second gives the corresponding number. Field strength can be read in in GV/m, TW/cm$^{-2}$ or atomic units.

\paragraph{FWHM and time intervals}

This option depends on the type of envelope chosen. While in both cases all 5 numbers need to be entered, for a Gaussian pulse only the first and third number have an effect. For a sinusoidal pulse all but the first number has an effect.

For a Gaussian pulse, the first argument corresponds to FWHM in equation~\eqref{eq:laser_gauss_2} and the third argument to $t_c$ in ~\eqref{eq:laser_gauss_1}.

For a sinusoidal pulse, the second, third, fourth and fifth argument correspond to $t_0$, $t_c$, $t_{c2}$ and $t_e$, respectively, in equation~\eqref{eq:laser_sinus}.

\paragraph{Central frequency}

There are two input lines for the central frequency $\omega_0$. The first defines the unit (wavelength in nm, energy in eV, or atomic units). The second line gives the value.

\paragraph{Phase}

The total phase $\phi$ is given in multiples of $\pi$. For example, the input ``\ttt{1.5}'' gives a phase of $\frac{3\pi}{2}$.

\paragraph{Chirp parameters}

There are four lines giving the chirp parameters $b_1$, $b_2$, $b_3$ and $b_4$. See equation~\eqref{eq:laser_chirp} for the meaning of these parameters.







% ========================================================================================================= %

\section{Calculation of Absorption Spectra: \ttt{spectrum.py}}\label{sec:spectrum.py}

Aside from setting up trajectories, the \ttt{initconds.excited} files can also be used to generate absorption spectra based on the excitation energies and oscillator strengths in the file. The script \ttt{spectrum.py} calculates Gaussian, Lorentzian, or Log-normal convolutions of these data in order to obtain spectra. See section~\ref{met:spectrum} for further details.

\ttt{spectrum.py} evaluates the absorption spectrum on a grid for all states it finds in an initial conditions file. Using command-line options, some initial conditions can be omitted in the convolution, see table~\ref{tab:spectrum_opts}.

\subsection{Input}

The script is executed with the initial conditions file as argument:
\begin{verbatim}
user@host> $SHARC/spectrum.py [OPTIONS] initconds.excited
\end{verbatim}

The script accepts a number of command-line options, which are given in table~\ref{tab:spectrum_opts}.
\begin{table}
  \centering
  \caption{Command-line options for script \ttt{spectrum.py}.}
  \label{tab:spectrum_opts}
  \begin{tabular}{>{\ttfamily}lll}
    \hline
    \rmfamily Option        &Description      &Default\\
    \hline
    -h                  &Display help message and quit.         &---       \\
    -o FILENAME         &Output filename for the spectrum       &\ttt{spectrum.out}\\
    -n INTEGER          &Number of grid points                  &500       \\
    -e FLOAT FLOAT      &Energy range (eV) for the spectrum     &1 to 10 eV\\
    -i INTEGER INTEGER  &Index range for the initial conditions &1 to 1000\\
    -f FLOAT            &FWHM (eV) for the spectrum             &0.1 eV\\
    -G                  &Gaussian convolution                   &Gaussian\\
    -L                  &Lorentzian convolution                 &Gaussian\\
    -N                  &Log-normal convolution                 &Gaussian\\
    -s                  &Use only selected initial conditions   &Use all\\
    -l                  &Make a line spectrum                   &Convolution\\
    -D                  &Compute density of states (ignore $f_\text{osc}$)      &Compute absorption\\
    --gnuplot FILENAME  &Write a \textsc{Gnuplot} script        &No \textsc{Gnuplot} script\\
    -B INTEGER          &Perform \ttt{B} bootstrapping cycles (error estimation)        &0\\
    -b FILENAME         &Output filename for bootstrapping      &\ttt{spectrum\_bootstrap.out}\\
    -r INTEGER          &Seed for random number generator (for bootstrap) &16661\\
    \hline
  \end{tabular}
\end{table}

\subsection{Output}

The script writes the absorption spectrum to a file (by default \ttt{spectrum.out}). Using the \ttt{-o} option, the user can redirect the output to a suitable file. The output is a table containing $n+2$ columns, where $n$ is the number of states found in the initial conditions file. The first column gives the energy in eV, within the given energy interval. In columns 2 to $n+1$ the state-wise absorption spectra are given. The last column contains the total absorption spectrum, i.e., the sum over all states. The table has $n_{\text{grid}}+1$ rows. For line spectra the output format is exactly the same, however, the file will contain one row for each excited state of each initial condition in the initial conditions file. If density of states is computed, the script replaces the oscillator strength by a factor of 1 for all states.

Additionally, the script writes some information about the calculation to standard output, among these the maximum of the spectrum, which can be used in order to normalize the spectrum. The reported maximum is simply the largest value in the last column of the spectrum. 

If requested, the script generates a \textsc{Gnuplot} script, which can be used to directly plot the spectrum. 

\subsection{Error Analysis}

The shape of the spectrum is strongly influenced by the number of initial conditions included and by the width of the broadening function (FWHM).
In principle, the FWHM of the broadening function should be as small as possible and the number of initial conditions extremely large, in order to obtain a correctly sampled spectrum.
In reality, if only few initial conditions were considered, the FWHM should be chosen large enough to smooth out any artifical structure of the spectrum arising solely from the small sample size.

In order to estimate whether the number of initial conditions and the FWHM are well-chosen, \ttt{spectrum.py} can compute error estimates for the total absorption spectrum.
This estimate is computed by a bootstrapping procedure (similar to the one used in \ttt{bootstrap.py}).
In order to use it, use the \ttt{-B} option with a positive integer argument (the default is zero, and hence no bootstrapping is performed).
The procedure will generate a second output file, called \ttt{spectrum\_bootstrap.out} by default.
It contains in the first column the energy in eV, in the second the geometric average spectrum from all bootstrap cycles, in column 3 and 4 the positive and negative errors of the spectrum, and in all further columns the individual spectra obtained in the bootstrap cycles.
In \ttt{gnuplot}, in order to plot the average spectrum and the upper and lower error bounds, plot \ttt{u 1:2}, \ttt{u 1:(\$2+\$3)}, and \ttt{u 1:(\$2+\$4)}.

A suitable procedure is to start with a rather small FWHM, compute the spectrum with errors, and if the errors are unsatisfactorily large, increase stepwise the FWHM.
Note that the bootstrapping estimate will give very small errors if the FWHM is very large---even though the actual spectrum can look very different in this case.


% ========================================================================================================= %

\section{File transfer: \ttt{retrieve.sh}}\label{sec:retrieve}

Usually, \sharc\ will run on some temporary directory, and not in the directory where the trajectories have been submitted from. The shell script \ttt{retrieve.sh} is a simple \ttt{scp} wrapper, which can be executed (in a directory where a trajectory has been sent from) in order to retrieve the output files of this trajectory. This might not work for every cluster setup.

It relies on the presence of the file \ttt{host\_infos}. All trajectories set up with \ttt{setup\_traj.py} create this file after the trajectory has been started with \ttt{run.sh}. \ttt{retrieve.sh} reads \ttt{host\_infos} to determine the hostname and working directory of the trajectory and then uses \ttt{scp} to retrieve the output and restart files.

The script can be called with the option ``\ttt{-lis}'' in order to only retrieve the \ttt{output.lis} file, but not the other output files.

If the script is called with the option ``\ttt{-res}'' then also the restart files and the content of the \ttt{restart/} directory are copied.

It is advisable to configure public-key authentification for the hosts running the trajectories, so that not for every execution of \ttt{retrieve.sh} a password has to be entered.










% ========================================================================================================= %

\section{Data Extractor: \ttt{data\_extractor.x}}\label{sec:data_extractor.x}

\begin{table}[tb]
  \centering
  \caption{Command-line options for \ttt{data\_extractor.x}.}
  \label{tab:dataextractor_options}
  \begin{tabular}{>{\ttfamily}lll}
    \hline
    \rmfamily Option        &Description      &Default\\
    \hline
    -h          &Display help message and quit.                         &---                      \\
    -e          &Write \ttt{energy.out}                                 &True                     \\
    -d          &Write \ttt{fosc.out}                                   &True                     \\
    -da         &Write \ttt{fosc\_act.out}                              &True                     \\
    -sp         &Write \ttt{spin.out}                                   &True                     \\
    -cd         &Write \ttt{coeff\_diag.out}                            &True                     \\
    -cm         &Write \ttt{coeff\_MCH.out}                             &True                     \\
    -cb         &Write \ttt{coeff\_diab.out}                            &True (if overlaps present) \\
    -p          &Write \ttt{prob.out}                                   &True                     \\
    -x          &Write \ttt{expec.out}                                  &True                     \\
    -xm         &Write \ttt{expec\_MCH.out}                             &True                     \\
    -id         &Write \ttt{ion\_diag.out}                              &False                    \\
    -im         &Write \ttt{ion\_MCH.out}                               &False                    \\
    -z          &\ttt{-e}, \ttt{-cd}, \ttt{-cm}, \ttt{-p}, \ttt{-x}     &---                      \\
    -s          &All options except \ttt{-id} and \ttt{-im}             &---                      \\
    -a          &All options                                            &---                      \\
    \hline
  \end{tabular}
\end{table}

\begin{table}[tbp]
  \centering
  \caption[Content of the files written by \ttt{data\_extractor.x}.]{Content of the files written by \ttt{data\_extractor.x}. $n$ is the total number of states, $j$ is a state index ($j\in\{1..n\}$).}
  \label{tab:outputdata}
  \begin{tabular}{>{\ttfamily}lcll}
    \hline
    File  &\# Columns     &\multicolumn{2}{l}{Columns}\\
    \hline
    energy.out       &$4+n$
      &$1$ &Time $t$ (fs)\\
      &&$2$ &Kinetic energy (eV)\\
      &&$3$ &Potential energy (eV) of active state (diagonal)\\
      &&$4$ &Total energy (eV)\\
      &&$4+j$ &Potential energy (eV) of state $j$ (diagonal)\\
    %
    fosc.out       &$2+n$
      &$1$ &Time $t$ (fs)\\
      &&$2$ &Oscillator strength from lowest to active state (diagonal)\\
      &&$2+j$ &Oscillator strength from lowest state to state $j$ (diagonal)\\
    %
    fosc\_act.out       &$1+2n$
      &$1$ &Time $t$ (fs)\\
      &&$1+j$ &$E_j-E_\text{active}$ (in eV, diagonal)\\
      &&$1+n+j$ &Oscillator strength from active state to state $j$ (diagonal)\\
    %
    spin.out       &$2+n$
      &$1$ &Time $t$ (fs)\\
      &&$2$ &Total spin expectation value of active state\\
      &&$2+j$ &Total spin expectation value of state $j$\\
    %
    coeff\_diag.out       &$2+2n$
      &$1$ &Time $t$ (fs)\\
      &&$2$ &Norm of wave function $\sum_j |c_j^{\text{diag}}|^2$\\
      &&$1+2j$ &$\Re (c_j^{\text{diag}})$\\
      &&$2+2j$ &$\Im (c_j^{\text{diag}})$\\
    %
    coeff\_MCH.out       &$2+2n$
      &$1$ &Time $t$ (fs)\\
      &&$2$ &Norm of wave function $\sum_j |c_j^{\text{MCH}}|^2$\\
      &&$1+2j$ &$\Re (c_j^{\text{MCH}})$\\
      &&$2+2j$ &$\Im (c_j^{\text{MCH}})$\\
    %
    coeff\_diab.out       &$2+2n$
      &$1$ &Time $t$ (fs)\\
      &&$2$ &Norm of wave function $\sum_j |c_j^{\text{diab}}|^2$\\
      &&$1+2j$ &$\Re (c_j^{\text{diab}})$\\
      &&$2+2j$ &$\Im (c_j^{\text{diab}})$\\
    %
    prob.out       &$2+n$
      &$1$ &Time $t$ (fs)\\
      &&$2$ &Random number from surface hopping\\
      &&$2+j$ &Cumulated hopping probability $\sum_{k=1}^j P_k$\\
    %
    expec.out      &$4+3n$
      &$1$ &Time $t$ (fs)\\
      &&$2$ &Kinetic energy (eV)\\
      &&$3$ &Potential energy (eV) of active state (diagonal)\\
      &&$4$ &Total energy (eV)\\
      &&$4+j$ &Potential energy (eV) of state $j$ (diagonal)\\
      &&$4+n+j$ &Total spin expectation value of state $j$ (diagonal)\\
      &&$4+2n+j$ &Oscillator strength of state $j$ (diagonal)\\
    %
    expec\_MCH.out      &$4+3n$
      &$1$ &Time $t$ (fs)\\
      &&$2$ &Kinetic energy (eV)\\
      &&$3$ &Potential energy (eV) of approximate active state (MCH)\\
      &&$4$ &Total energy (eV)\\
      &&$4+j$ &Potential energy (eV) of state $j$ (MCH)\\
      &&$4+n+j$ &Total spin expectation value of state $j$ (MCH)\\
      &&$4+2n+j$ &Oscillator strength of state $j$ (MCH)\\
    %
    ion\_diag.out      &$4+3n$
      &$1$ &Time $t$ (fs)\\
      &&$1+j$ &$E_j-E_\text{active}$ (in eV, diagonal)\\
      &&$1+n+j$ &Dyson norm from active state to state $j$ (diagonal)\\
    %
    ion\_MCH.out      &$4+3n$
      &$1$ &Time $t$ (fs)\\
      &&$1+j$ &$E_j-E_\text{approximate active}$ (in eV, mCH)\\
      &&$1+n+j$ &Dyson norm from approximate active state to state $j$ (MCH)\\
    %
    \hline
  \end{tabular}
\end{table}

The output of \sharc\ is mainly written to \ttt{output.dat}. In order to obtain plottable files in tabular format, the Fortran program \ttt{data\_extractor.x} is used.

\subsection{Usage}

The \ttt{data\_extractor.x} is a command line tool, and is called with the \ttt{output.dat} file as an argument, and possibly with some options.
\begin{verbatim}
user@host> $SHARC/data_extractor.x [options] output.dat
\end{verbatim}
The program will create a directory \ttt{output\_data/} in the current working directory (not necessarily in the directory where \ttt{output.dat} resides). In this directory, several files are written, containing, e.g., the potential energies depending on time, populations depending on time, etc.
Which files are created can be controlled with the command line options, which are summarized in Table~\ref{tab:dataextractor_options}. For most applications, using the \ttt{-z} or \ttt{-s} flags should be sufficient. The default is equivalent to \ttt{-s}.

The program will extract the complete \ttt{output.dat} file until it reaches the EOF. 

The \ttt{data\_extractor.x} program will automatically detect the format of the \ttt{output.dat} file (the first \sharc\ release had a different file format than the current \sharc\ release).

\subsection{Output}

After the program finishes, the directory \ttt{output\_data/} contains a number of files. In each file, the number of columns is dependent in the total number $n$ of states $i\in\{1...n\}$. The content of the files is listed in Table~\ref{tab:outputdata}.

The file \ttt{expec.out} contains the information of \ttt{energy.out}, \ttt{spin.out} and \ttt{fosc.out} in one file. The content of \ttt{expec.out} can be conveniently plotted by using \ttt{make\_gnuscript.py} to generate a \textsc{Gnuplot} script.


% ========================================================================================================= %

\section{Plotting the Extracted Data: \ttt{make\_gnuscript.py}}\label{sec:make_gnuscript.py}

The contents of the output files of \ttt{data\_extractor.x} can be plotted with \textsc{Gnuplot}. In order to quickly generate an appropriate \textsc{Gnuplot} script, \ttt{make\_gnuscript.py} can be used. The usage is:
\begin{verbatim}
user@host> $SHARC/make_gnuscript.py <S> [<D> [<T> [<Q> ... ] ] ]
\end{verbatim}
\ttt{make\_gnuscript.py} takes between 1 and 8 integers as command-line arguments, specifying the number of singlet, doublet, triplet, etc.\ states. It writes an appropriate \textsc{Gnuplot} script to standard out, hence redirect the output to a file, e.g.:
\begin{verbatim}
user@host> $SHARC/make_gnuscript.py 3 0 2 > gnuscript.gp
\end{verbatim}


Then, \textsc{Gnuplot} can be run in the \ttt{output\_data} directory of a trajectory:
\begin{verbatim}
user@host> gnuplot gnuscript.gp
\end{verbatim}
This can also be accomplished in one step using a pipe, e.g.:
\begin{verbatim}
user@host> $SHARC/make_gnuscript.py 3 0 2 | gnuplot
\end{verbatim}

The created plot script generates four different plots (press ENTER in the command-line where you started \textsc{Gnuplot} to go to the next plot). The first plot shows the potential energy of all states in the dynamics over time in the diagonal representation. The currently occupied state is marked with black circles. A thin black line gives the total energy (sum of the kinetic energy and the potential energy of the currently occupied state). Each state is colored, with one color as contour and one color at the core of the line. The contour color represents the total spin expectation value of the state. The core color represents the oscillator strength of the state with the lowest state. See figure~\ref{fig:colors} for the relevant color code.
\begin{figure}
  \centering
  \includegraphics[scale=1]{img/colors/colors.pdf}
  \caption{Color code for plots generated with the use of \ttt{make\_gnuscript.py}.}
  \label{fig:colors}
\end{figure}
Note that by definition the ``oscillator strength'' of the lowest state with itself is exactly zero, hence the lowest state is also light grey. This dual coloring allows for a quick recognition of different types of states in the dynamics, e.g.\ singlets vs.\ triplets or $n\pi^*$ vs.\ $\pi\pi^*$ states.

The second plot shows the population $|c_i^{\text{MCH}}|^2$ of the MCH electronic states over time. The line colors are auto-generated in order to give a large spread of all colors over the excited states, but the colors might be sub-optimal, e.g.\ for printing. In this cases, the user should manually adjust the colors in the generated script.

The third plot shows the population $|c_i^{\text{diag}}|^2$ of the diagonal electronic states over time. These are the populations which are actually used for surface hopping. However, since these states are spin-mixed, it is usually difficult to interpret these populations.

The fourth plot shows the surface hopping probabilities over time. The plot is setup in such a way that the visible area corresponding to a certain state is proportional to the probability to hop into the state. Hence, if for a given time step the random number (black circles) lies within a colored area, a surface hop to the corresponding state is performed.





% ========================================================================================================= %

\section{Ensemble Diagnostics Tool: \ttt{diagnostics.py}}\label{sec:diagnostics.py}

The purpose of this script is to automatize the critical step of checking the trajectories in an ensemble for sanity before beginning the ensemble analysis.

The tool can check several different aspects of the trajectories.
First, it checks whether all relevant output files of the trajectories are present, and if they are complete and consistent (e.g., no missing lines due to network/file system problems).
Second, it checks simulation progress and status (e.g., whether the trajectory is running, crashed, finished, or stopped).
Third, it can check several energy-related requirements: total energy conservation, smoothness of kinetic and potential energy, and hopping energy differences.
It also checks for conservation of total population, and for trajectories using local diabatization also intruder states are checked.

Note that the diagnostics script can also be used to automatically run the \ttt{data\_extractor.x} for all trajectories.

Also note that \ttt{diagnostics.py} will not work if the \ttt{printlevel} in the \sharc\ trajectories was lower than 2.

\subsection{Usage}

The script is interactive, simply start it with no command-line arguments or options:
\begin{verbatim}
user@host> $SHARC/diagnostics.py
\end{verbatim}

\subsection{Input}

\paragraph{Paths to trajectories}

First the script asks the user to specify all directories for whose content the analysis should be performed. Enter one directory path at a time, and finish the directory input section by typing ``\ttt{end}''. Please do not specify each trajectory directory separately, but specify their parent directories, e.g.\ the directories \ttt{Singlet\_1} and \ttt{Singlet\_2}. \ttt{diagnostics.py} will automatically include all trajectories contained in these directories.

Unlike the ensemble analysis scripts (these are \ttt{populations.py}, \ttt{transition.py}, \ttt{crossing.py}, \ttt{trajana\_essdyn.py}, \ttt{trajana\_nma.py}, and \ttt{data\_collector.py}, see below), \ttt{diagnostics.py} ignores files which indicate the status of a trajectory (\ttt{CRASHED}, \ttt{RUNNING}, \ttt{DONT\_ANALYZE}) and carries out the diagnostics routines as long as it identifies a directory as a \sharc\ trajectory.

\paragraph{Settings}

The settings for the diagnostics run can be modified with a simple menu, which can be navigated with the commands \ttt{show}, \ttt{help}, \ttt{end}, and where the settings can be modified with \ttt{<key> <value>} (e.g., \ttt{hop\_energy 0.2} sets the corresponding option to 0.2~eV).
A list of the settings is given in Table~\ref{tab:diagnostics}.

Generally, the keywords \ttt{missing\_output}, \ttt{missing\_restart}, and \ttt{normal\_termination} should always be left at \ttt{True}, since checking them is cheap and the obtained information is important.
Note that \ttt{data\_extractor.x} is always run for all trajectories, except if \ttt{output.dat} is older than the files in \ttt{output\_data/}.
During the check of each trajectory, \ttt{output.lis}, \ttt{output\_data/energies.out}, and \ttt{output\_data/coeff\_diag.out} are furthermore checked for missing time steps.

\begin{table}
  \centering
  \caption{List of the settings for \ttt{diagnostics.py}.}
  \label{tab:diagnostics}
  \begin{tabular}{>{\ttfamily}lcp{11cm}}
    \hline
    Key  &Value     &Explanation\\
    \hline
    missing\_output     &Boolean        &Checks if "output.lis", "output.log", "output.xyz", "output.dat" are existing. Setting to \ttt{False} only suppresses output, but files are always checked.\\
    missing\_restart    &Boolean        &Checks if "restart.ctrl", "restart.traj", "restart/" are existing. Files are not checked if set to \ttt{False}.\\
    normal\_termination &Boolean        &Checks for status of trajectory:\\
                                       &&\ttt{RUNNING}: no finish message in \ttt{output.log}, last step started recently.\\
                                       &&\ttt{STUCK}: no finish message in \ttt{output.log}, last step started long ago.\\
                                       &&\ttt{CRASHED}: error message in \ttt{output.log}.\\
                                       &&\ttt{FINISHED}: finish message in \ttt{output.log}.\\
                                       &&\ttt{FINISHED (stopped by user)}: finished due to \ttt{STOP} file.\\
    etot\_window        &Float          &Maximum permissible drift (along full trajectory) in the total energy (in eV).\\
    etot\_step          &Float          &Maximum permissible total energy difference between two successive time steps (in eV).\\
    epot\_step          &Float          &Maximum permissible active state potential energy difference between two successive time steps (in eV). Not checked for time steps where a hop occurred.\\
    ekin\_step          &Float          &Maximum permissible kinetic energy difference between two successive time steps (in eV).\\
    pop\_window         &Float          &Maximum permissible drift in total population.\\
    hop\_energy         &Float          &Maximum permissible change in active state energy during a surface hop (in eV).\\
    intruders           &Boolean        &Checks if intruder state messages in "output.log" refer to active state.\\
    \hline
  \end{tabular}
\end{table}

\paragraph{Trajectory Flagging}

\ttt{diagnostics.py} determines for each trajectory a ``maximum usable time'' value ($T_\text{mu}$).
This value is either the total simulation time or the time when the first violation (problems with time step consistency, total energy conservation, potential/kinetic energy smoothness, hopping energy restriction, or intruder states) in the trajectory appeared.
The script prints the $T_\text{mu}$ values for all trajectories at the end.

The user can then give a threshold for $T_\text{mu}$, so that \ttt{diagnostics.py} excludes all trajectories with values smaller than the threshold from analysis (the script will create a file \ttt{DONT\_ANALYZE} in the directory of each affected trajectory).
In this way it is possible to perform ensemble analysis for a given simulation length while ignoring problematic trajectories.

When choosing the threshold for $T_\text{mu}$, keep in mind that a compromise usually has to be made.
A small value of the threshold will mean that many trajectories are admitted for analysis (because problems occurring late do not matter), giving good statistics, but that the analysis can only be carried out for the first part of the simulation time.
On the other hand, choosing a large threshold allows analysis of a satisfactory simulation time, but only few trajectories will be included in the analysis (only the ones where no problems occurred for many time steps).

It is advisable that the chosen threshold value is used as input for the ensemble analysis scripts which ask for a maximum analysis time (\ttt{populations.py}, \ttt{transition.py}, \ttt{crossing.py}, \ttt{trajana\_essdyn.py}, \ttt{trajana\_nma.py}).






% ========================================================================================================= %

\section{Calculation of Ensemble Populations: \ttt{populations.py}}\label{sec:populations.py}

For an ensemble of trajectories, usually one of the most relevant results are ensemble-averaged populations. The interactive script \ttt{populations.py} collects these populations from a set of trajectories. 

Different methods to obtain populations or quantities approximating populations can be collected, as described below.

\subsection{Usage}

The script is interactive, simply start it with no command-line arguments or options:
\begin{verbatim}
user@host> $SHARC/populations.py
\end{verbatim}

Depending on the analysis mode (see below) it might be necessary to run \ttt{data\_extractor.x} for each trajectory prior to running \ttt{populations.py} (but \ttt{populations.py} can also call \ttt{data\_extractor.x} for each subdirectory, if desired). 

\paragraph{Paths to trajectories}

First, the script asks the user to specify all directories for whose content the analysis should be performed. Enter one directory path at a time, and finish the directory input section by typing ``\ttt{end}''. Please do not specify each trajectory directory separately, but specify their parent directories, e.g.\ the directories \ttt{Singlet\_1} and \ttt{Singlet\_2}. \ttt{populations.py} will automatically include all trajectories contained in these directories.

If you want to exclude certain trajectories from the analysis, it is sufficient to create an empty file called \ttt{CRASHED} or \ttt{RUNNING} in the corresponding trajectory directory. \ttt{populations.py} will ignore all directories containing one of these files. The file name is case insensitive, i.e., also files like \ttt{crashed} or even \ttt{cRasHED} will lead to the trajectory being ignored.
Additionally, \ttt{populations.py} will ignore trajectories with a \ttt{DONT\_ANALYZE} file from \ttt{diagnostics.py}.

\paragraph{Analysis mode}

Using \ttt{populations.py}, there are two basic ways in obtaining the excited-state populations. The first way is to count the number of trajectories for which a certain condition holds. For example, the number of trajectories in each classical state can be obtained in this way. However, it is also possible to count the number of trajectories for which the total spin expectation value is within a certain interval. 
The second way to obtain populations is to obtain the sum of the absolute squares of the quantum amplitudes over all trajectories. Table~\ref{tab:Populations_modes} contains a list of all possible analysis modes.

\begin{table}
  \centering
  \caption[Analysis modes for \ttt{populations.py}.]{Analysis modes for \ttt{populations.py}. The last column indicates whether \ttt{data\_extractor.x} has to be run prior to the ensemble analysis.}
  \label{tab:Populations_modes}
  \begin{tabular}{lp{8.5cm}>{\ttfamily}lc}
    \hline
    Mode        &Description    &\rmfamily From which file?     &Extract?\\
    \hline
    1   &For each diagonal state count how many trajectories have this state as active state. &output.lis  &No\\
    2   &For each MCH state count how many trajectories have this state as approximate active state (see section~\ref{ssec:state_transform}). &output.lis  &No\\
    3   &For each MCH state count how many trajectories have this state as approximate active state (see section~\ref{ssec:state_transform}). Multiplet components are summed up. &output.lis  &No\\
    4   &Generate a histogram with definable bins (variable width). Bin the trajectories according to their total spin expectation value (of the currently active diagonal state).   &output.lis &No\\
    5   &Generate a histogram with definable bins (variable width). Bin the trajectories according to their state dipole moment expectation value (of the currently active diagonal state).   &output.lis &No\\
    6   &Generate a histogram with definable bins (variable width). Bin the trajectories according to the oscillator strength between lowest and currently active diagonal states.        &output\_data/fosc.out  &Yes\\
    7   &Calculate the sum of the absolute squares of the diagonal coefficients for each state.       &output\_data/coeff\_diag.out&Yes\\
    8   &Calculate the sum of the absolute squares of the MCH coefficients for each state.       &output\_data/coeff\_MCH.out&Yes\\
    9   &Calculate the sum of the absolute squares of the MCH coefficients for each state. Multiplet components are summed up.       &output\_data/coeff\_MCH.out&Yes\\
   10   &Transform option 1 to MCH basis (trajectory-wise transformation with $\VEC{U}$ matrix).   &output.dat&No\\
   11   &Transform option 1 to MCH basis (trajectory-wise transformation with $\VEC{U}$ matrix). Multiplet components are summed up.   &output.dat&No\\
   20   &Calculate the sum of the absolute squares of the diabatic coefficients for each state (Only for trajectories with local diabatization).      &output\_data/coeff\_diab.out&Yes\\
    \hline
  \end{tabular}
\end{table}

\paragraph{Run data extractor}

For analysis modes 6, 7, 8, 9 and 20 it is necessary to first run the data extractor (see section~\ref{sec:data_extractor.x}). This task can be accomplished by \ttt{populations.py}. However, for a large ensemble or for long trajectories this may take some time. Hence, it is not necessary to perform this step each time \ttt{populations.py} is run. 

\ttt{populations.py} will detect whether the file \ttt{output.dat} or the content of \ttt{output\_data/} is more recent. Only if \ttt{output.dat} is newer the \ttt{data\_extractor.x} will be run for this trajectory.

Note that mode 20 can only be used for trajectories using local diabatization propagation (keyword \ttt{coupling overlap} in \sharc\ input file) or .

\paragraph{Number of states}

For analysis modes 1, 2, 3, 7, 8 and 9 it is necessary to specify the number of states in each multiplicity. The number is auto-detected from the input file of one of the trajectories.

\paragraph{Intervals}

For analysis modes 4, 5 and 6 the user must specify the intervals (i.e., the histogram bins) for the classification of the trajectories. The user has to input a list of interval borders, e.g.:
\begin{example}
\begin{verbatim}
Please enter the interval limits, all on one line.
Interval limits: 1e-3 0.01 0.1 1
\end{verbatim}
\end{example}

Note that scientific notation can be used. Based on this input, for each time step a histogram is created with the number of trajectories in each interval. The histogram bins are:
\begin{enumerate}
  \item $x\leq10^{-3}$
  \item $10^{-3}<x\leq0.01$
  \item $0.01<x\leq0.1$
  \item $0.1<x\leq1$
  \item $1<x$
\end{enumerate}
Note that there is always one more bin that interval borders entered.

\paragraph{Normalization}

If desired, \ttt{populations.py} can normalize the populations by dividing the populations by the number of trajectories. 

\paragraph{Maximum simulation time}

This gives the maximum simulation time until which the populations are analyzed. For trajectories which are shorter than this value, the last population information is used to make the trajectory long enough. Trajectories which are longer are not analyzed to the end. \ttt{populations.py} prints the length of the shortest and longest trajectories after the analysis.

If \ttt{diagnostics.py} was executed previously, the user can enter here the threshold for the maximum usable time (see section~\ref{sec:diagnostics.py}).

\paragraph{Setup for bootstrapping}

The output file of \ttt{populations.py} is sufficient to perform kinetic model fits with the script \ttt{make\_fitscript.py}.
However, if error estimates for the kinetic model are desired (using \ttt{bootstrap.py}), the output file of \ttt{populations.py} is not enough.
The user can tell \ttt{populations.py} to save additional data which is required by \ttt{bootstrap.py}.

\paragraph{Gnuplot script}

\ttt{populations.py} can generate an appropriate \textsc{Gnuplot} script for the performed population analysis. 

\subsection{Output}

By default, \ttt{populations.py} writes the resulting populations to \ttt{pop.out}. If the file already exists, the user is ask whether it shall be overwritten, or to provide an alternative filename. Note that the output file is checked only after the analysis is completed, so the program might run for a considerable amount of time before asking for the output file.







% ========================================================================================================= %

\section{Calculation of Numbers of Hops: \ttt{transition.py}}\label{sec:transition.py}

Another important information from the trajectory ensemble is the number of hopping events and the involved states, for example to judge the relative importance of competing reaction pathways.

The interactive script \ttt{transition.py} calculates from an ensemble the number of hops between each pair of states and presents the results as ``transition matrices''.
Currently, the script employs the MCH active state information from \ttt{output.lis} for this computations. 
Note that since the MCH active state is only an approximate quantity (since hops are actually performed in the diagonal basis in \textsc{Sharc}), the results should be checked carefully.
The script \ttt{transition.py} is still partly work-in-progress.

\subsection{Usage}

The script is interactive, simply start it with no command-line arguments or options:
\begin{verbatim}
user@host> $SHARC/transition.py
\end{verbatim}

\paragraph{Paths to trajectories}

First the script asks the user to specify all directories for whose content the analysis should be performed. Enter one directory path at a time, and finish the directory input section by typing ``\ttt{end}''. Please do not specify each trajectory directory separately, but specify their parent directories, e.g.\ the directories \ttt{Singlet\_1} and \ttt{Singlet\_2}. \ttt{populations.py} will automatically include all trajectories contained in these directories.

If you want to exclude certain trajectories from the analysis, it is sufficient to create an empty file called \ttt{CRASHED} or \ttt{RUNNING} in the corresponding trajectory directory. \ttt{populations.py} will ignore all directories containing one of these files. The file name is case insensitive, i.e., also files like \ttt{crashed} or even \ttt{cRasHED} will lead to the trajectory being ignored.
Additionally, \ttt{transition.py} will ignore trajectories with a \ttt{DONT\_ANALYZE} file from \ttt{diagnostics.py}.

\paragraph{Analysis mode}

The different analysis modes for \ttt{transition.py} are given in Table~\ref{tab:Transition_modes}.

\begin{table}
  \centering
  \caption[Analysis modes for \ttt{transition.py}.]{Analysis modes for \ttt{transition.py}. The last column indicates whether \ttt{data\_extractor.x} has to be run prior to the ensemble analysis.}
  \label{tab:Transition_modes}
  \begin{tabular}{lp{8cm}>{\ttfamily}lc}
    \hline
    Mode        &Description    &\rmfamily From which     &Extract?\\
                               &&\rmfamily file?          &\\
    \hline
    1   &Get transition matrix in MCH basis. &output.lis  &No\\
    2   &Get transition matrix in MCH basis, ignoring hops within multiplets. &output.lis  &No\\
    3   &Write a tabular file with the transition matrix in the MCH basis for each time step. &output.lis  &No\\
    4   &Write a tabular file with the transition matrix in the MCH basis for each time step, ignoring hops within multiplets.   &output.lis &No\\
    \hline
  \end{tabular}
\end{table}


% ========================================================================================================= %

\section{Fitting population data to kinetic models including error estimation: \ttt{make\_fit.py}}\label{sec:make_fit.py}

Often it is interesting to fit some functions to the population data from a trajectory ensemble, in order to provide a way to abstract the data and to obtain some kind of rate constants for population transfer, which allows to compare to experimental works.
In simple cases, it might be sufficient to fit basic mono- and biexponential functions to the data, which provides the sought-after time constants.
However, often a more meaningful approach is based on a chemical kinetics model.
Such a model is specified by a set of chemical species (e.g., electronic states, reactants, products, etc) connected by elementary reactions with associated rate constants, which together form a reaction network graph.
For an explanation of those graphs, see section~\ref{met:globalfit}.

The script \ttt{make\_fit.py} helps the user in implementing and fitting such global fits to a kinetics model.

The script works in a stand-alone fashion, unlike its predecessors (\ttt{make\_fitscript.py} and \ttt{bootstrap.py}).
It solves the differential equations numerically using a Runge-Kutta 5th order algorithm and fits the kinetic parameters using a number of different optimization algorithms.
The script requires Python2 with \textsc{NumPy} and \textsc{SciPy}.
If these are not available, use \ttt{make\_fitscript.py} and \ttt{bootstrap.py}.

Often, one is also interested in obtaining an estimate of the error associated to these rate constants, e.g., in order to decide whether enough trajectories were computed.
A possible way to obtain such error estimates is the statistical bootstrapping procedure.
The idea of bootstrapping is to generate \emph{resamples} of the original ensemble; for an ensemble of $n$ trajectories, one draws $n$ random trajectories with replacement to obtain one resample.
The resample can then be fitted like the original ensemble to obtain a second estimate of the rate constants. 
By generating many resamples, one can thus obtain a ``probability'' distribution of the rate constants, from which a statistical error measure can be calculated.
For details on these statistical measures, see section~\ref{met:bootstrapping}.

The script \ttt{make\_fit.py} implements this resampling--fitting--statistics procedure.
It is dependent on the output of \ttt{populations.py}, but otherwise works in a stand-alone fashion.



\subsection{Usage}

The script is interactive, simply start it with no command-line arguments or options:
\begin{verbatim}
user@host> $SHARC/make_fit.py
\end{verbatim}

Before you start the script, you need to prepare a file with the relevant populations data (usually, the output file of \ttt{populations.py} will suffice).
You also might want to run \ttt{transition.py} first, which can help in developing a suitable kinetic model.

\subsection{Input}

The interactive input for this script consists of specifying the reaction network graph, the initial conditions and the data file.
Additionally, the user has to specify which species should be fitted to which data columns in the file.
Optionally, the user can specify the bootstrap settings for estimating errors.

\paragraph{Kinetic model species}

As a first step, the user has to specify the set of species in the model.
Each species is fully described by its label.
A label must start with a letter and can be followed by letters, numbers and underscores (although an underscore must not be directly followed by another underscore).

During input, the user can add one or several labels to the set of species, remove labels and display the current set of defined labels.
It is not possible to add a label twice.
Once all labels are defined, the keyword \ttt{end} brings the user to the next input section (hence, \ttt{end} is not a valid label).

\paragraph{Kinetic model elementary reactions}

Next, the reactions have to be defined.
A reaction is specified by its initial species, final species, and reaction rate label.
Reaction rate labels are under the same restrictions as species labels and must not be already used as a species label.
Furthermore, initial and final species must be both defined previously and they must be different.
There can only be one reaction from any species to another species (If a second reaction is defined, the first reaction label is simply overwritten).
Note that reaction rate labels can be used in several reactions (in this way, different rates can be restricted to be the same).

During input, the user can add and remove reactions and display the currently defined reactions (displayed as a matrix).
Unlike species labels, only one reaction can be added per line.
Once all reactions are defined, the keyword \ttt{end} brings the user to the next input section.

\paragraph{Kinetic model initial values}

In order to specify the initial values for each species, the user simply has to define which species have non-zero initial population.
These species will then be assigned an initial population constant, which can be fitted along with the reaction rates.

During input, the user can add and remove species from the set of species with non-zero initial population.
Once all reactions are defined, the keyword \ttt{end} brings the user to the next input section.

\paragraph{Operation mode}

The script can either read a \ttt{pop.out} file for a simple global fit, or a \ttt{bootstrap\_data/} directory for a global fit with error estimate.
If the latter is chosen, the user needs to enter the number of bootstrap cycles.


\paragraph{Populations data file}

The user has to specify a path (autocomplete is enabled) to the file containing the population data to which the model functions should be fitted.
In bootstrap mode, instead the path to the \ttt{bootstrap\_data/} directory (can be prepared with \ttt{populations.py}) needs to be given.
The script reads the file/files and automatically detects the maximum time and the number of data columns.

The file/files should be formatted as a table, with one time step per line (e.g., an output file of \ttt{populations.py}). 
On each line, the first number is interpreted as the time in femtoseconds and all consecutive numbers (separated by spaces) as the populations at this time.
Note that the first entry must be at $t$=0 and all subsequent lines must be in strictly increasing order.
Time steps can be unevenly spaced if necessary.

\paragraph{Species--Data mapping}

In the next setup section, the user has to specify which functions should be fitted to which data column from the data file.
In the simplest case, one species is fitted to a single data column (e.g., the species \ttt{S0} is fitted to data column 2).
However, it is also possible to fit the sum of two species to a column (this can be useful, e.g., to describe biexponential processes) and to fit a species to the sum of several columns (e.g., one can fit to the total triplet population to obtain a total ISC rate constant).
In general, it is also possible to fit sums of species to sums of columns.

It is not allowed to use one species or one column in more than one mapping.
However, it is possible to leave species or data columns unused in the global fit. 
While unused species still affect the outcome (through the reaction network definitions), unused data columns are simply ignored in the fit.

During input, the user can add one mapping per line as well as display the current mappings.
For the column definitions, ranges can be given with the tilde symbol, e.g., \ttt{5\textasciitilde 9} is interpreted as \ttt{5 6 7 8 9}.
If a typo is made, the user can reset the mappings and repeat only the mapping input without the need to repeat the previous sections.
Once all mappings are defined, the keyword \ttt{end} finishes the input section.

\paragraph{Fitting procedure}

In the last section, the user can edit the initial guesses for the rate constants and initial populations.
To change a value, enter \ttt{label = value}.
Use \ttt{show} to print the current values for all constants.
\ttt{end} finishes the guess edit step.

Afterwards, the script asks whether the initial populations should be optimized or not. 
This is usually only useful if several species have non-zero initial populations.
If you optimize initial populations, note that their sum might differ from 1 after optimization.

The script also asks whether the rate constants should be constrained to positive values.
If answered with \ttt{yes}, then the optimized rate constants are restricted to the range 0.000\,001 to infinity (i.e., the time constants are constrained between 1\,000\,000~fs and 0~fs).
Note that with constraints \textsc{SciPy} uses the Trust Region Reflective algorithm, and the Levenberg-Marquardt algorithm for unconstrained cases.

\subsection{Output}

The script will write the output to standard out.
It will first print the iterations of the fit, and the obtained results afterwards (all fitted parameters with errors).
The script will also write two files, \ttt{fit\_results.txt} and \ttt{fit\_results.gp}. 
The latter is a \textsc{Gnuplot} script that can be used to plot the global fit, which is useful for visual inspection.

Note that the errors printed for normal runs are just the intrinsic fitting errors, which assume that the population data is error-free.
To obtain realistic fitting errors that take into account the uncertainty due to the finite trajectory ensemble, use the bootstrap mode.

In bootstrap mode, the script initially will perform the same steps as in normal mode, using the average population from the bootstrap directory.
After writing the fit results and the two files, the script will start performing the bootstrap iterations, writing the fitted parameters in each iteration.
In this way, the user can monitor the convergence of these values, to decide whether more iterations are required.
Typically, the values and errors will vary strongly during the first iterations and stabilize later.
The convergence rate is strongly dependent on the fitting model and the data.

After all iterations are done (or the script is interrupted with \ttt{Ctrl-C}), the script will print a summary of the statistical analysis.
For each fitting parameter (all time constants and all initial populations), the script will list the arithmetic mean and standard deviation (absolute and relative), the geometric mean and standard deviation (separately for $+$ and $-$, absolute and relative), and the minimum and maximum values.
The script will also print a histogram with the obtained distribution for each parameter.
For details on these statistical measures, see section~\ref{met:bootstrapping}.

\ttt{bootstrap.py} also creates an output file  once it is finished.
The file, \ttt{fit\_bootstrap.txt}, contains the summary of the statistical analysis with the computed statistical measures and the histograms.
Additionally, at the end this file contains a table with all obtained fitting parameters for resamples (e.g., for further statistical or correlation analysis).







% ========================================================================================================= %

\section{Fitting population data to kinetic models: \ttt{make\_fitscript.py}}\label{sec:make_fitscript.py}

Often it is interesting to fit some functions to the population data from a trajectory ensemble, in order to provide a way to abstract the data and to obtain some kind of rate constants for population transfer, which allows to compare to experimental works.
In simple cases, it might be sufficient to fit basic mono- and biexponential functions to the data, which provides the sought-after time constants.
However, often a more meaningful approach is based on a chemical kinetics model.
Such a model is specified by a set of chemical species (e.g., electronic states, reactants, products, etc) connected by elementary reactions with associated rate constants, which together form a reaction network graph.
For an explanation of those graphs, see section~\ref{met:globalfit}.

The script \ttt{make\_fitscript.py} helps the user in implementing and executing such global fits to a kinetics model.

The script employs the open-source computer algebra system \textsc{Maxima} as back-end to solve the differential equation systems which describe the model kinetics.
The script writes a \textsc{Gnuplot} script containing all commands necessary for the global fit. 
The fitting itself is performed with \textsc{Gnuplot}.

\subsection{Usage}

The script is interactive, simply start it with no command-line arguments or options:
\begin{verbatim}
user@host> $SHARC/make_fitscript.py
\end{verbatim}

Before you start the script, you need to prepare a file with the relevant populations data (usually, the output file of \ttt{populations.py} will suffice).
You also might want to run \ttt{transition.py} first, which can help in developing a suitable kinetic model.

\subsection{Input}

The interactive input for this script consists of specifying the reaction network graph, the initial conditions and the data file.
Additionally, the user has to specify which species should be fitted to which data columns in the file.

\paragraph{Kinetic model species}

As a first step, the user has to specify the set of species in the model.
Each species is fully described by its label.
A label must start with a letter and can be followed by letters, numbers and underscores (although an underscore must not be directly followed by another underscore).

During input, the user can add one or several labels to the set of species, remove labels and display the current set of defined labels.
It is not possible to add a label twice.
Once all labels are defined, the keyword \ttt{end} brings the user to the next input section (hence, \ttt{end} is not a valid label).

\paragraph{Kinetic model elementary reactions}

Next, the reactions have to be defined.
A reaction is specified by its initial species, final species, and reaction rate label.
Reaction rate labels are under the same restrictions as species labels and must not be already used as a species label.
Furthermore, initial and final species must be both defined previously and they must be different.
There can only be one reaction from any species to another species (If a second reaction is defined, the first reaction label is simply overwritten).

During input, the user can add and remove reactions and display the currently defined reactions (displayed as a matrix).
Unlike species labels, only one reaction can be added per line.
Once all reactions are defined, the keyword \ttt{end} brings the user to the next input section.

\paragraph{Kinetic model initial values}

In order to specify the initial values for each species, the user simply has to define which species have non-zero initial population.
These species will then be assigned an initial population constant (e.g., for species \ttt{A} the initial population constant would be \ttt{A\_\_0}).
These constants will show up in the \textsc{Gnuplot} script and their value can edited there.
It is also possible to fit the initial populations to the data.

During input, the user can add and remove species from the set of species with non-zero initial population.
Once all reactions are defined, the keyword \ttt{end} brings the user to the next input section.

\paragraph{Populations data file}

The user has to specify a path (autocomplete is enabled) to the file containing the population data to which the model functions should be fitted.
The script reads the file and automatically detects the maximum time and the number of data columns.

The file should be formatted as a table, with one time step per line (e.g., an output file of \ttt{populations.py}). 
On each line, the first number is interpreted as the time in femtoseconds and all consecutive numbers (separated by spaces) as the populations at this time.
Note that the first entry must be at $t$=0 and all subsequent lines must be in strictly increasing order.
Time steps can be unevenly spaced if necessary.

\paragraph{Species--Data mapping}

In the final setup section, the user has to specify which functions should be fitted to which data column from the data file.
In the simplest case, one species is fitted to a single data column (e.g., the species \ttt{S0} is fitted to data column 2).
However, it is also possible to fit the sum of two species to a column (this can be useful, e.g., to describe biexponential processes) and to fit a species to the sum of several columns (e.g., one can fit to the total triplet population to obtain a total ISC rate constant).
In general, it is also possible to fit sums of species to sums of columns.

It is not allowed to use one species or one column in more than one mapping.
However, it is possible to leave species or data columns unused in the global fit. 
While unused species still affect the outcome (through the reaction network definitions), unused data columns are simply ignored in the fit.

During input, the user can add one mapping per line as well as display the current mappings.
If a typo is made, the user can reset the mappings and repeat only the mapping input without the need to repeat the previous input.
Once all mappings are defined, the keyword \ttt{end} finishes the input section.

\paragraph{Running \textsc{Maxima}}

Before the script attempts to call \textsc{Maxima}, it prints the command to be executed and asks the user for permission to execute.
If permission is denied, the user may enter another command to call maxima.
After permission is granted, the script calls \textsc{Maxima}.
If the call takes more than a few seconds, the standard output of \textsc{Maxima} is printed and standard input is switched to \textsc{Maxima}.
In this way, the user can answer any questions by \textsc{Maxima} (e.g., whether a constant/combination of constants is larger than zero).
To answer these questions, type the answer followed by a semi-colon (e.g., \ttt{pos;}).

\subsection{Output}

After successful execution of the script, two files are created, \ttt{model\_fit.dat} and \ttt{model\_fit.gp}.
The former file contains the populations data formatted appropriately for the global fit.
The latter file contains the \textsc{Gnuplot} script which can be executed by
\begin{verbatim}
user@host> gnuplot model_fit.gp
\end{verbatim}
to perform the global fit.
Please follow the instructions printed by \ttt{make\_fitscript.py} at the end of the execution, in particular by adjusting the starting guesses for the fitting parameters (you have to open and edit \ttt{model\_fit.gp} for this).
Please do not change the structure of \ttt{model\_fit.gp} if you intend to use this file with \ttt{bootstrap.py}.


% ========================================================================================================= %

\section{Estimating Errors of Fits: \ttt{bootstrap.py}}\label{sec:bootstrap.py}

As was described in section~\ref{sec:make_fitscript.py}, the population data from \ttt{populations.py} can be fitted to a kinetic model to obtain interpretable rate constants.
Often, one is also interested in obtaining an estimate of the error associated to these rate constants, e.g., in order to decide whether enough trajectories were computed.
A possible way to obtain such error estimates is the statistical bootstrapping procedure.
The idea of bootstrapping is to generate \emph{resamples} of the original ensemble; for an ensemble of $n$ trajectories, one draws $n$ random trajectories with replacement to obtain one resample.
The resample can then be fitted like the original ensemble to obtain a second estimate of the rate constants. 
By generating many resamples, one can thus obtain a ``probability'' distribution of the rate constants, from which a statistical error measure can be calculated.
For details on these statistical measures, see section~\ref{met:bootstrapping}.

The script \ttt{bootstrap.py} implements this resampling--fitting--statistics procedure.
It is dependent on the output of \ttt{populations.py} and \ttt{make\_fitscript.py}, and employs \textsc{Gnuplot} to perform the actual fits.

\subsection{Usage}

The script is interactive, simply start it with no command-line arguments or options:
\begin{verbatim}
user@host> $SHARC/bootstrap.py
\end{verbatim}

Before you start the script, you need to run \ttt{populations.py} to generate the bootstrapping data (\ttt{populations.py} asks the related question near the end of the input query).

Additionally, you need to use \ttt{make\_fitscript.py} to generate a \textsc{Gnuplot} fitting script. Please, do not modify the script (beyond initial values for the fitting constants), as \ttt{bootstrap.py} relies on the proper format of this file.
It is advisable to create the \textsc{Gnuplot} fitting script from one of the files contained in the bootstrap data directory (because then the simulation time is consistent between the fitting script and the bootstrapping data).

\subsection{Input}

The interactive input consists in specifying the paths to the bootstrapping data directory (from \ttt{populations.py}), the path to the fitting script, the number of resamples, and some other minor options.

\paragraph{Path to the bootstrapping data directory}

The user should enter here the path to the directory which was created by \ttt{populations.py} and which contains the bootstrapping raw data (i.e., the populations for each individual trajectory before summing up over the ensemble).
\ttt{bootstrap.py} automatically detects the number of trajectories, time step, number of steps, and number of data columns.

Note that \ttt{bootstrap.py} only considers files in this directory if their filename starts with \ttt{pop\_}.
Also note that \ttt{bootstrap.py} creates a temporary subdirectory inside the bootstrapping data directory as scratch area.

\paragraph{Number of bootstrapping cycles}

Here, the number of resamples to generate and fit needs to be entered.
The default, 10 resamples, is very small and not sufficient to achieve convergence in the errors for most applications.
It is advisable to employ several hundred or thousand resamples to achieve good statistical convergence.

Note that the script can be interrupted during the iterations (using \ttt{Ctrl-C}) and then skips to the final analysis, so it is no problem to enter a too large number of cycles.

\paragraph{Random number generator seed}

The script employs random numbers to generate the random resamples.
For reproducible results, do not use the default option (\ttt{!}), but enter an integer.

\paragraph{Path to the fitting script}

The user should enter the path to the appropriate \textsc{Gnuplot} fitting script, generated with \ttt{make\_fitscript.py}.
It is strongly advisable to adjust the initial guesses for the fitting parameters in the script in order to ensure quick and stable convergence of the fits.
Please, do not modify the generated \textsc{Gnuplot} script in any other way, as this might break \ttt{bootstrap.py}.

\paragraph{Command to execute}

Here, the user should enter the command to be used to call \textsc{Gnuplot}. In most cases, this will simply be \ttt{gnuplot}, but the user might want to use another installation of \textsc{Gnuplot} for the fitting.

\paragraph{Number of CPUs}

\ttt{bootstrap.py} can be run in parallel mode, where multiple \textsc{Gnuplot} processes are employed at the same time.
Currently, this parallelization is not very efficient due to process creation overheads, but for complicated fitting scripts (where each fit takes relatively long), large data sets, or many resamples it can be useful.

Note that if more than one CPU is used, users should not prematurely terminate \ttt{bootstrap.py} with \ttt{Ctrl-C}, as some of the subprocesses might get stuck and the script will not properly go to the final analysis.

\subsection{Output}

During the resample iterations, \ttt{bootstrap.py} prints the current values and errors (geometric mean and geometric standard deviation) of the fitting parameters every few iterations.
In this way, the user can monitor the convergence of these values, to decide whether more iterations are required.
Typically, the values and errors will vary strongly during the first iterations and stabilize later.
The convergence rate is strongly dependent on the fitting model and the data.

After all iterations are done (or the script is interrupted with \ttt{Ctrl-C}), the script will print a summary of the statistical analysis.
For each fitting parameter (all time constants and all initial populations), the script will list the arithmetic mean and standard deviation (absolute and relative), the geometric mean and standard deviation (separately for $+$ and $-$, absolute and relative), and the minimum and maximum values.
The script will also print a histogram with the obtained distribution for each parameter.
For details on these statistical measures, see section~\ref{met:bootstrapping}.

\ttt{bootstrap.py} also creates two output files.
The first file, \ttt{bootstrap\_cycles.out}, is written on-the-fly while the resample iterations are running.
It contains the same tabular output as is shown in standard output, and can be used to visualize the convergence behavior of the parameters. For example, the first parameter can be monitored using \textsc{Gnuplot} with this command: \ttt{plot "bootstrap\_cycles.out" using 1:6:8 w yerrorbars}.
The scond file, \ttt{bootstrap.out}, is written once the final analysis is complete. It contains the summary of the statistical analysis with the computed statistical measures and the histograms.
Additionally, at the end this file contains a table with all obtained fitting parameters for resamples (e.g., for further statistical or correlation analysis).










% ========================================================================================================= %

\section{Obtaining Special Geometries: \ttt{crossing.py}}\label{sec:crossing.py}

In many cases, it is also important to obtain certain special geometries from the trajectories. The script \ttt{crossing.py} extracts geometries fulfilling special conditions from an ensemble of trajectories. 

Currently, \ttt{crossing.py} finds geometries where the approximate MCH state (see section~\ref{ssec:state_transform}) of the last time step is different from the MCH state of the current time step (i.e.\ \ttt{crossing.py} finds geometries where surface hops occured). 

\subsection{Usage}

The script is interactive, simply start it with no command-line arguments or options:
\begin{verbatim}
user@host> $SHARC/crossing.py
\end{verbatim}

The input to the script is very similar to the one of \ttt{populations.py}. 

\paragraph{Paths to trajectories}

First the script asks the user to specify all directories for whose content the analysis should be performed. Enter one directory path at a time, and finish the directory input section by typing ``\ttt{end}''. Please do not specify each trajectory directory separately, but specify their parent directories, e.g.\ the directories \ttt{Singlet\_1} and \ttt{Singlet\_2}. \ttt{crossing.py} will automatically include all trajectories contained in these directories.

If you want to exclude certain trajectories from the analysis, it is sufficient to create an empty file called \ttt{CRASHED} or \ttt{RUNNING} in the corresponding trajectory directory. \ttt{crossing.py} will ignore all directories containing one of these files.
Additionally, \ttt{crossing.py} will ignore trajectories with a \ttt{DONT\_ANALYZE} file from \ttt{diagnostics.py}.

\paragraph{Analysis mode}

Currently, \ttt{crossing.py} only supports one analysis mode, where \ttt{crossing.py} is scanning for each trajectory the file \ttt{output.lis}. If the occupied MCH state (column 4 in output file \ttt{output.lis}) changes from one time step to the next, it is checked whether the old and new MCH states are the ones specified by the user. If this is the case, the geometry corresponding to the new time step ($t$) is retrieved from \ttt{output.xyz} (lines $t(n_{\text{atom}}+2)+1$ to $t(n_{\text{atom}}+2)+n_{\text{atom}}$). 

\paragraph{States involved in surface hop}

First, the user has to specify the permissible old MCH state. The state has to be specified with two integers, the first giving the multiplicity (\ttt{1}=singlet, ...), the second the state within the multiplicity (\ttt{1 1}=$S_0$, \ttt{1 2}=$S_1$, etc.). If a state of higher multiplicity is given, \ttt{crossing.py} will report all geometries where the old MCH state is any of the multiplet components. 

For the new MCH state, the same is valid.

Third, the direction of the surface hop has to be specified. Choosing ``Backwards'' has the same effect as exchanging the old and new MCH states. 

\subsection{Output}

All geometries are in the end written to an output file, by default \ttt{crossing.xyz}. The file is in standard xyz format. The comment of each geometry gives the path to the trajectory where this geometry was extracted, the simulation time and the diagonal and MCH states at this simulation time. 






% ========================================================================================================= %

\section{Internal Coordinates Analysis: \ttt{geo.py}}\label{sec:geo.py}

\sharc\ writes at every time step the molecular geometry to the file \ttt{output.xyz}. The non-interactive script \ttt{geo.py} can be used in order to extract internal coordinates from xyz files. The usage is:
\begin{verbatim}
user@host> $SHARC/geo.py [options] < Geo.inp > Geo.out
\end{verbatim}
By default, the coordinates are read from \ttt{output.xyz}, but this can be changed with the \ttt{-g} option (see table~\ref{tab:Geo_options}). Note that the internal coordinate specifications are read from standard input and the result table is written to standard out. 

\subsection{Input}

The specifications for the desired internal coordinates are read from standard input. It follows a simple syntax, where each internal coordinate is specified by a single line of input. Each line starts with a one-letter key which specifies the type of internal coordinate (e.g.\ bond length, angle, dihedral, ...). The key is followed by a list of integers, specifying which atoms should be measured. As a simple example, \ttt{r 1 2} specifies the bond length (\ttt{r} is the key for bond lengths) between atoms 1 and 2. Note that the numbering of the atoms starts with 1. Each line of input is checked for consistency (whether any atom index is larger than the number of atoms, repeated atom indices, misspelled keys, wrong number of atom indices, ...), and erroneous lines are ignored (this is indicate by an error message).

Table~\ref{tab:Geo_input} lists the available types of internal coordinates. The output is a table, where the first column is the time (Actually, the geometries are just enumerated starting with zero, and the number multiplied by the time step from the \ttt{-t} option). The successive columns in the output table list the results of the internal coordinates calculations. Each request generates at least one column, see table~\ref{tab:Geo_input}. 

Note that for most internal coordinates, the order of the atoms is crucial, since e.g.\ $a_{123}\neq a_{213}$. This also holds for the Cremer-Pople parameter requests. For these input lines, the atoms should be listed in the order they appear in the ring (clockwise or counter-clockwise).

\begin{table}[htb]
  \centering
  \caption{Possible types of internal coordinates in \ttt{geo.py}. }
  \label{tab:Geo_input}
  \begin{tabular}{>{\ttfamily}c>{\bfseries\ttfamily}lll}
    \hline
    \rmfamily Key         &\normalfont\rmfamily Atom      &Description    &Output columns\\
                          &\normalfont\rmfamily Indices   &               &\\
    \hline
    x   &a              &$x$ coordinate of atom \ttt{a}                                         &$x$\\
    y   &a              &$y$ coordinate of atom \ttt{a}                                         &$y$\\
    z   &a              &$z$ coordinate of atom \ttt{a}                                         &$z$\\
    r   &a b            &Bond length between \ttt{a} and \ttt{b}                                &$r$\\
    a   &a b c          &Angle between \ttt{a}--\ttt{b} and \ttt{b}--\ttt{c}                    &$a$\\
    d   &a b c d        &Dihedral, i.e., angle between normal vectors of (\ttt{a,b,c}) and (\ttt{b,c,d})   &$d$\\
                       &&(between -180$^\circ$ and 180$^\circ$, same sign conventions as \textsc{Molden})\\
    p   &a b c d        &Pyramidalization angle: 90$^\circ$ minus angle between bond \ttt{a}--\ttt{b}   &$p$\\
                       &&and normal vector of (\ttt{b,c,d})&\\
    q   &a b c d        &Pyramidalization angle (alternative definition; angle between  &$q$\\
                       &&bond \ttt{a}--\ttt{b} and average of bonds \ttt{b}--\ttt{c} and \ttt{b}--\ttt{d}&\\
    5   &a b c d e      &Cremer-Pople parameters for 5-membered rings \cite{Cremer1975JACS} and&$q_2$, $\phi_2$, Boeyens\\
                       && comformation classification.&\\
    6   &a b c d e f    &Cremer-Pople parameters for 6-membered rings \cite{Cremer1975JACS} and&$Q$, $\phi$, $\theta$, Boeyens\\
                       &&Boeyens classification \cite{Boeyens1976JCMS}.&\\
    i   &a - f          &Angle between average plane through 3-rings (\ttt{a - c}) and (\ttt{d - f})      &$i$\\
    j   &a - h          &Angle between average plane through 4-rings (\ttt{a - d}) and (\ttt{e - f})      &$j$\\
    k   &a - j          &Angle between average plane through 5-rings (\ttt{a - e}) and (\ttt{f - j})      &$k$\\
    l   &a - l          &Angle between average plane through 6-rings (\ttt{a - f}) and (\ttt{g - l})      &$l$\\
    c   &               &Writes the comment (second line of the &Comment\\
                       &&xyz format) to the table.&\\
    \hline
  \end{tabular}
\end{table}

As an advice, it is always a good idea to put the comment as the \textit{last} request, if needed. Since the comment may contain blanks, having the comment not as the very last column might make it impossible to plot the resulting table.

The Boeyens classification symbols which are output for 6-membered rings are reported in \LaTeX\ math code. Note that in the Boeyens classification scheme by definition a number of symbols are equivalent, and only one symbol is reported. 
These are the equivalent symbols: $^1C_4={}^3C_6={}^5C_2$, $^4C_1={}^6C_3={}^2C_5$, $^1T_3={}^4T_6$, $^2T_6={}^5T_3$, $^2T_4={}^5T_1$, $^3T_1={}^6T_4$, $^6T_2={}^3T_5$ and $^4T_2={}^1T_5$.

For 5-membered rings, the classification symbols are chosen similar to the Boeyens symbols. For the $^aE$ and $E_a$ symbols, atom $a$ is puckered out of the plane and the four other atoms are coplanar, while for the $^aH_b$ symbols the neighboring atoms $a$ and $b$ are puckered out of the plane in opposite directions and only the three remaining atoms are coplanar.

It is also possible to measure angles between the average planes through two $n$-membered rings.
Currently, this is only possible if both rings have the same number of atoms (3, 4, 5, or 6).

\subsection{Options}

\ttt{geo.py} accepts a number of command-line options, see table~\ref{tab:Geo_options}. All options have sensible defaults. However, especially if long comments should be written to the output file, it might be necessary to increase the field width. Note that the minimum column width is 20 so that the table header can be printed correctly.
Also note that the column for the comment is enlarged by 50 characters.

\begin{table}[htb]
  \centering
  \caption{Command-line options for \ttt{geo.py}. }
  \label{tab:Geo_options}
  \begin{tabular}{>{\ttfamily}lll}
    \hline
    \rmfamily Option         &Description    &Default\\
    \hline
    -h          &Display help message and quit.         &---       \\
    -b          &Report $x$, $y$, $z$, $r$, $q_2$ and $Q$ in Bohrs       &Angstrom\\
    -r          &Report $a$, $d$, $p$, $q$, $\phi_2$, $\phi$, $\theta$, $i$, $j$, $k$, and $l$ in Radians         &Degrees\\
    -g FILENAME &Read coordinates from the specified file       &\ttt{output.xyz}\\
    -t FLOAT    &Assumed time step between successive geometries (fs)    &1.0 fs\\
    -w INTEGER  &Width of each column (min=20)                   &20\\
    -p INTEGER  &Precision (Number of decimals, min=width--3)         &4\\
    \hline
  \end{tabular}
\end{table}





% ========================================================================================================= %

\section{Essential Dynamics Analysis: \ttt{trajana\_essdyn.py}}\label{sec:trajana_essdyn.py}

An essential dynamics analysis~\cite{Amadei1993PSFB} is a procedure to find the most active vibrational modes in an ensemble of trajectories.
It is based on the computation of the covariance matrix between all Cartesian (or mass-weighted Cartesian) coordinates of all steps of all trajectories and a singular value decomposition of this covariance matrix.
For details on the computation, see section~\ref{met:essdyn}.

The interactive script \ttt{trajana\_essdyn.py} can be used to perform such an analysis.

\subsection{Usage}

\ttt{trajana\_essdyn.py} is an interactive script, which is started with:
\begin{verbatim}
user@host> $SHARC/trajana_essdyn.py
\end{verbatim}
Note that before executing the script you should prepare an XYZ geometry file with the reference geometry (e.g., the ground state minimum or the average geometry from the trajectories).

\subsection{Input}

During interactive input, the script queries for the paths to the trajectories, the path to the reference structure, and a few other settings.

\paragraph{Path to the trajectories}

First the script asks the user to specify all directories for whose content the analysis should be performed. Enter one directory path at a time, and finish the directory input section by typing ``\ttt{end}''. Please do not specify each trajectory directory separately, but specify their parent directories, e.g.\ the directories \ttt{Singlet\_1} and \ttt{Singlet\_2}. \ttt{trajana\_essdyn.py} will automatically include all trajectories contained in these directories.

If you want to exclude certain trajectories from the analysis, it is sufficient to create an empty file called \ttt{CRASHED} or \ttt{RUNNING} in the corresponding trajectory directory. \ttt{trajana\_essdyn.py} will ignore all directories containing one of these files.
Additionally, \ttt{trajana\_essdyn.py} will ignore trajectories with a \ttt{DONT\_ANALYZE} file from \ttt{diagnostics.py}.

\paragraph{Path to reference structure}

For the essential dynamics analysis, a reference structure is required. This structure is substracted from all geometries for the correlation analysis.
The structure can be in any format understandable by \textsc{OpenBabel}, but the type of format needs to be specified.
In most cases, the reference structure will be either in XYZ or \textsc{Molden} format.

\paragraph{Mass-weighted coordinates}

If enabled, the correlation analysis will be carried out in mass-weighted Cartesian coordinates.
In the output file, the mass-weighting will be removed properly.

\paragraph{Number of steps and time step}

These parameters are automatically detected and suggested as defaults.
It is not necessary to change them.

\paragraph{Time step intervals}

By default, \ttt{trajana\_essdyn.py} will analyze the full length of the simulations together. 
However, it is also possible to compute the essential dynamics for different time intervals (e.g., if the molecular motion is different in the beginning of the dynamics).
Multiple, possibly overlapping, intervals can be entered; for each of the intervals, one set of output files is produced.

\paragraph{Results directory}

The path to the directory where the output files are stored.
The path has to be entered as a relative or absolute path.
If it does not exist, \ttt{trajana\_essdyn.py} will create it.

\subsection{Output}

Inside the results directory, \ttt{trajana\_essdyn.py} will create two subdirectories, \ttt{total\_cov/} and \ttt{cross\_av/}.
In the directory \ttt{total\_cov/} the results of the full covariance analysis are stored (i.e., essential modes found here have large total activity, but the trajectories could behave very differently).
On the contrary, \ttt{cross\_av/} will contain the results of the analysis of the average trajectory (i.e., essential modes found here have strongly coherent activity, where all trajectories behave similarly).

For each time step interval entered, one output file (e.g., \ttt{0-1000.molden}) is created in each of the two subdirectories.




% ========================================================================================================= %

\section{Normal Mode Analysis: \ttt{trajana\_nma.py}}\label{sec:trajana_nma.py}

A normal mode analysis~\cite{Kurtz2001JCP} is a procedure to find the activity of given normal modes in an ensemble of trajectories
For details on the computation, see section~\ref{met:nma}.

The interactive script \ttt{trajana\_nma.py} can be used to perform such an analysis.

\subsection{Usage}

\ttt{trajana\_nma.py} is an interactive script, which is started with:
\begin{verbatim}
user@host> $SHARC/trajana_nma.py
\end{verbatim}
Note that before executing the script you should prepare a \textsc{Molden} file with the relevant normal modes.

\subsection{Input}

During interactive input, the script queries for the paths to the trajectories, the path to the normal mode file, and a few other settings.

\paragraph{Path to the trajectories}

First the script asks the user to specify all directories for whose content the analysis should be performed. Enter one directory path at a time, and finish the directory input section by typing ``\ttt{end}''. Please do not specify each trajectory directory separately, but specify their parent directories, e.g.\ the directories \ttt{Singlet\_1} and \ttt{Singlet\_2}. \ttt{trajana\_nma.py} will automatically include all trajectories contained in these directories.

If you want to exclude certain trajectories from the analysis, it is sufficient to create an empty file called \ttt{CRASHED} or \ttt{RUNNING} in the corresponding trajectory directory. \ttt{trajana\_nma.py} will ignore all directories containing one of these files.
Additionally, \ttt{trajana\_nma.py} will ignore trajectories with a \ttt{DONT\_ANALYZE} file from \ttt{diagnostics.py}.

\paragraph{Path to normal mode file}

The normal mode file (in \textsc{Molden} format) needs to contain the normal modes for which the analysis should be carried out.
The file needs to have the same atom ordering as the trajectories.

\paragraph{Mass-weighted coordinates}

If enabled, the correlation analysis will be carried out in mass-weighted Cartesian coordinates.
In the output file, the mass-weighting will be removed properly.

\paragraph{Number of steps and time step}

These parameters are automatically detected and suggested as defaults.
It is not necessary to change them.

\paragraph{Automatic plot creation}

If available, the user can choose to automatically create total activity and temporal plots of the normal mode analysis.

\paragraph{Non-totally symmetric modes}

For symmetry reasons, the average value of a non-totally symmetric normal mode will always be close to zero for a sufficiently large ensemble.
In order to still obtain useful information for such modes, the user can specify modes whose absolute value should be considered.
Note that in the input it is possible to specify ranges, e.g., \ttt{2\textasciitilde8} for all modes from 2 to 8.

\paragraph{Multiplication by -1}

The user can choose to multiply certain normal modes by -1.
This is only for convenience when viewing the results.
Note that in the input it is possible to specify ranges, e.g., \ttt{2\textasciitilde8} for all modes from 2 to 8.

\paragraph{Time step intervals}

By default, \ttt{trajana\_nma.py} will analyze the full length of the simulations together. 
However, it is also possible to compute the essential dynamics for different time intervals (e.g., if the molecular motion is different in the beginning of the dynamics).
Multiple, possibly overlapping, intervals can be entered; for each of the intervals, one set of output files is produced.

\paragraph{Results directory}

The path to the directory where the output files are stored.
The path has to be entered as a relative or absolute path.
If it does not exist, \ttt{trajana\_nma.py} will create it.

\subsection{Output}

Inside the results directory, \ttt{trajana\_nma.py} will create one subdirectory, \ttt{nma/}.
By default, four output files are written into this subdirectory.
The file \ttt{total\_std.txt} will contain the mode activity for each normal mode and for each time interval; a large value indicates that across all time steps and trajectories there is a large variance in that normal mode.
Similarly, the file \ttt{cross\_av\_std.txt} will contain the mode activity for each normal mode and for each time interval; this is computed from the average trajectory, therefore only showing coherent mode activity.
The files \ttt{mean\_against\_time.txt} and \ttt{std\_against\_time.txt} contain for all normal modes and for all time steps the mean and standard deviation, respectively.

Also by default, additionally, inside each of the trajectory directories, \ttt{trajana\_nma.py} generates three output files.
The file \ttt{nma\_nma.txt} is a table containing the normal mode coordinates of the trajectory for each time step; functionally, it is very similar to the output of \ttt{geo.py}.
The files \ttt{nma\_nma\_av.txt} and \ttt{nma\_nma\_std.txt} contain for each time interval and each normal mode the mean and average for that trajectory.

Finally, if automatic plots were requested, the results subdirectory will contain two directories, \ttt{bar\_graphs/} and \ttt{time\_plots/}.
The former contains plots of the data in \ttt{total\_std.txt} and \ttt{cross\_av\_std.txt}, whereas the latter will contain plots of \ttt{mean\_against\_time.txt} and \ttt{std\_against\_time.txt}.







% ========================================================================================================= %

\section{General Data Analysis: \ttt{data\_collector.py}}\label{sec:data_collector.py}

Whereas most of the other analysis scripts in the \sharc\ suite are intended for rather specific tasks, \ttt{data\_collector.py} is aimed at providing a general analysis tool to carry out a large variety of tasks.
The primary task of \ttt{data\_collector.py} is to collect tabular data from files which are present in all trajectories, possibly perform some analysis procedures (smoothing, averaging, convoluting, integrating, summing), and output the combined data as a single file.
Possible applications of this functionality are statistical analysis of internal coordinates (e.g., mean and variation in a bond length), the creation of hair figures (e.g., a specific bond length plotted for all trajectories), data convolutions (e.g., distribution of bond length over time, simulation of time- and energy-resolved spectra), or data integration (e.g., computation of time-resolved intensities).

\subsection{Usage}

\ttt{data\_collector.py} is an interactive script, which is started with:
\begin{verbatim}
user@host> $SHARC/data_collector.py
\end{verbatim}

\subsection{Input}

In general, the first step in \ttt{data\_collector.py} is to collect tabular data files which exist in the directories of multiple trajectories.
For each trajectory, this file needs to have the same file name and the same tabular format; for example, one could read for all trajectories the \ttt{output.lis} files.

\paragraph{Collecting}

Hence, in the first input section, the script asks the user to specify all directories for whose content the analysis should be performed. Enter one directory path at a time, and finish the directory input section by typing ``\ttt{end}''. Please do not specify each trajectory directory separately, but specify their parent directories, e.g.\ the directories \ttt{Singlet\_1} and \ttt{Singlet\_2}. \ttt{data\_collector.py} will automatically include all trajectories contained in these directories.
If you want to exclude certain trajectories from the analysis, it is sufficient to create an empty file called \ttt{CRASHED} or \ttt{RUNNING} in the corresponding trajectory directory. \ttt{data\_collector.py} will ignore all directories containing one of these files.
Additionally, \ttt{data\_collector.py} will ignore trajectories with a \ttt{DONT\_ANALYZE} file from \ttt{diagnostics.py}.

In the second step, \ttt{data\_collector.py} displays all files which appear in multiple trajectories and which might be suitable for analysis (the script ignores files which it knows to be not suitable, e.g., \ttt{output.dat}, \ttt{output.xyz}, most files in the \ttt{QM/} or \ttt{restart/} subdirectories, ...). 
All other files (e.g., \ttt{output.lis}, files in \ttt{output\_data/}, ...) will be displayed, together with the number of appearances.

Once one of the files has been selected, one needs to assign the different data columns.
(i) One column is designated the time column T, which defines the sequentiality of the data:
(ii) Multiple columns can then be designated as data columns, called X columns in the following.
(iii) The same number of columns is designated as weight columns, called Y columns here (weights can be set equal to 1 by selecting column ``0'' in the relevant menu).
For example, for a time-resolved spectrum, the transition energies would be the X data, whereas the oscillator strengths would be the Y data (weights).
With these assignments, the full data set is defined:
\begin{itemize}
  \item For each trajectory $a$
  \begin{itemize}
    \item For each time step $t$
    \begin{itemize}
      \item For each X column $i$ there will be a value pair ($x^a_i(t)$,$y^a_i(t)$) or ($x^a_i(t)$,1).
    \end{itemize}
  \end{itemize}
\end{itemize}
In the simplest case, there will be exactly one ($x^a(t)$,1) pair for each trajectory and time, which is a two-dimensional data set.
Keep in mind that in general, each trajectory could have different time steps at this point.
We refer to this kind of data set (independent trajectories with possibly different time axes) as \ttt{Type1} data set.
As will be described below, in \ttt{data\_collector.py}, during certain processing steps the format of the data set is changed, which will create \ttt{Type2} or \ttt{Type3} data sets.

Once this data set is collected from the files (where too short or commented lines are ignored), \ttt{data\_collector.py} allows for a number of subsequent processing steps, which are summarized in Figure~\ref{fig:data_collector}.

\begin{figure}[htb]
  \centering
  \includegraphics[scale=1]{img/data_collector/data_collector.pdf}
  \caption[Possible workflows in \ttt{data\_collector.py}. ]{
  Possible workflows in \ttt{data\_collector.py}. 
  Grey boxes denote the different computational actions, yellow diamonds denote the different decisions which are queried in the input dialog, and the boxes at the denote the three different data set types (dark blue=\ttt{Type1}, light blue=\ttt{Type2}, green=\ttt{Type3}).
  The different actions and data set types are explained in the text.
  }
  \label{fig:data_collector}
\end{figure}

\paragraph{Smoothing}

In this step, each trajectory is individually smoothed, using one of several smoothing kernels (Gaussian, Lorentzian, Log-normal, rectangular). 
Smoothing does not change the size or format of the data set, each value is simply replaced by the corresponding smoothed value; hence, a new \ttt{Type1} data set is obtained.
Smoothing is applied to each X and each Y column independently, but always with the same kernel.
% \todo{For the underlying equations, see section~\ref{met:data_collector}.}
\begin{equation}
  X^a_i(t):=\frac{\sum_{t'}X^a_i(t') f(t,t')}{\sum_{t'}f(t,t')}\quad\text{and analogously for $Y_i^a(t)$.}
\end{equation}
Here, $f(t,t')$ is the smoothing kernel.


\paragraph{Synchronizing}

In this step, the \ttt{Type1} data set is reformatted, by merging all trajectories together.
This step creates a \ttt{Type2} data set, which has a common T axis for all trajectories (simply the union of the T columns from all trajectories).
For each time step, all X and Y values of all trajectories are collected.
If a trajectories does not have data at a particular time step, NaNs will be inserted.
In this way, a rectangular, two-dimensional data set is obtained, with as many rows as time steps, and $2n_\text{traj}n_\text{X}$ data columns.

A simple application of Collecting+Synchronizing could be to generate a table with the bond length for all time steps for all trajectories, in order to generate a ``hair figure''.
This task could in principle also be accomplished with Bash tools like \ttt{awk} and \ttt{paste}, but this is troublesome if the trajectories are of different length, with different time steps, or if the table files contain comments.

\paragraph{Averaging}

The \ttt{Type2} data set from the Synchronizing step can contain a large number of data columns ($2n_\text{traj}n_\text{X}$).
In order to reduce this amount of information, the Averaging step can be used to compute the mean and standard deviation across all trajectories, separately for each time step.
This will create a new \ttt{Type2} data set, which still has a common time axis, but will only contain $4n_\text{X}+1$ data columns; these are the mean and standard deviation of all X and Y columns, plus one column giving the number of trajectories for each time step.

Currently, this step can be performed with either arithmetic mean/standard deviation or geometric mean/standard deviation.

\paragraph{Statistics}

Similar to the Averaging step, the Statistics step computes mean and standard deviations from a \ttt{Type2} data set.
The difference is that during Statistics, these values are computed for all values from the first to the current time step.
The data in the last time step thus gives the total statistics over all time steps and trajectories.
The \ttt{Type2} data set from the Statistics step  contains the same number of data columns as the one from the Averaging step.

Currently, this step can be performed with either arithmetic mean/standard deviation or geometric mean/standard deviation.

With the Averaging and Statistics steps, it is possible to compute the same data as with \ttt{trajana\_nma.py} (if the appropriate files are read), namely the total (Statistics) and coherent (Averaging+Statistics) activity of the normal modes.
Using \ttt{data\_collector.py}, the same analysis can also be applied to internal coordinates computed with \ttt{geo.py}.

\paragraph{Convoluting(X)}

In order to create a data set which has common T \emph{and} X axes (a \ttt{Type3} data set), it is in general necessary to perform some kind of data convolution involving the X column data.
In order to do this, \ttt{data\_collector.py} creates a grid along the X axis ($n_\text{grid}$ points from the minimum value to the maximum value of all X data in the data set, plus some padding).
% \todo{For the underlying equations, see section~\ref{met:data_collector}.}
\begin{equation}
  Y_i(t,X):=\sum_a Y_i^a(t) f(X,X_i^a(t)).
\end{equation}
The created \ttt{Type3} data set has $n_\text{X}$ data points for each time step and each X grid point.

Using energies as X columns and oscillator strengths/intensities as Y columns, in this way it is possible to compute time-dependent spectra.
% \todo{Renormalize if number of trajectories changes.}

\paragraph{Summing(Y)}

When $n_\text{X}$ is larger than one, the Summing step can be used to compute the sum over all data points for each time step and each X grid point.
\begin{equation}
  Y(t,X):=\sum_i Y_i(t,X).
\end{equation}
This creates a new \ttt{Type3} data set, which will only contain one data point for each time step and each X grid point.

For example, for a transient spectrum involving multiple final states ($n_\text{X}>1$), after the Convoluting(X) step one obtains one transient spectrum for each final state. With the Summing(Y) step, one can then compute the total transient absorption spectrum.

\paragraph{Integrating(X)}

When computing transient spectra, one is often interested in integrating the spectrum within a certain energy window.
This can be done with the Integrating(X) step.
After entering a lower and upper X limit, a new \ttt{Type3} data set is created, with only three data points per time step.
The first data point contains the integral from minus infinity to the lower limit, the second data point the integral between lower and upper limit, and the third data point the integral from the upper limit to infinity.
If Summing(Y) was not carried out, this integration is carried out independently for each of the $n_\text{X}$ data columns.

\paragraph{Convoluting(T)}

The \ttt{Type3} data set can also be convoluted along the time axis (e.g., in order to apply an instrument response function to a time-resolved spectrum).
In order to do this, a uniform grid along the T axis is generated (with $n_\text{Tgrid}$ points from the minimum value to the maximum value of the previous T axis, plus some padding).
\begin{equation}
  Y_i(t',X):=\sum_{t} Y_i(t,X) f(t,t').
\end{equation}
The created \ttt{Type3} data set has as many data points for each X grid point as before, but the number of time steps is now $n_\text{Tgrid}$.
Convoluting(T) can be applied also if Summing(Y) and/or Integrating(X) were used (in this case, the kernel is applied to the summed up or integrated data).

\paragraph{Integrating(T)}

This step carries out a cumulative summation along the T axis.
\begin{equation}
  Y_i(t,X):=\sum_{t'=0}^t Y_i(t',X).
\end{equation}
In this way, the data in the last time step constitute the integral over all time steps.
Since all the partial cumulative sums are also computed, integrals within some bounds can simply be computed as differences between partial cumulative sums.

\paragraph{Converting}

At the end of the workflow, a \ttt{Type3} data set can be converted back into a \ttt{Type2} data set, which affects how the output file is formatted.
This is usually a good idea if the Integrating(X) step was performed, but might not be a good idea otherwise.
See below for how the different data set types are formatted on output.

\subsection{Output}

After the input dialog is finished, \ttt{data\_collector.py} will start carrying out the requested analyses.
For each of the workflow steps, one output file is written, so that all intermediate results can be used as well.
Output files have automatically generated filenames, which describe how the data was obtained.
Filenames are always in the form \ttt{collected\_data\_<T>\_<X>\_<Y>\_<steps>.<type>.txt}, where \ttt{<T>} is an integer giving the T column index, \ttt{<X>} and \ttt{<y>} are lists of integers of the X and Y column indices, \ttt{<steps>} is a string denoting which workflow steps were carried out, and \ttt{<type>} denotes the data set type.
For example, an output file could be named \ttt{collected\_data\_1\_2\_0\_sy\_cX.type3.txt}, where column 1 was the T column, column 2 the X column, no Y column was used, Synchronizing and Convoluting(X) were performed, resulting in a \ttt{Type3} data set.

\paragraph{Format of \ttt{Type1} data set output}

\ttt{Type1} data sets are formatted such that each trajectory is given as a continuous block, separated by an empty line.
Within each block, each line contains the data of one time step, order increasingly.
Each line contains a trajectory index, the relative file path, the time, all X column data, and then all Y column data.
\begin{example}
\footnotesize\begin{verbatim}
#    1                               2               3               4               5               6               7
#Index                        Filename            Time    X Column   5    X Column   6    Y Column   0    Y Column   0
     0 Singlet_1//TRAJ_00001/./Geo.out  0.00000000E+00  1.13340000E-01  7.99096900E+00  1.00000000E+00  1.00000000E+00
     0 Singlet_1//TRAJ_00001/./Geo.out  5.00000000E-01  1.59173000E-01  7.94395200E+00  1.00000000E+00  1.00000000E+00
     0 Singlet_1//TRAJ_00001/./Geo.out  1.00000000E+00  2.10868000E-01  7.89084000E+00  1.00000000E+00  1.00000000E+00
     ...

     1 Singlet_1//TRAJ_00002/./Geo.out  0.00000000E+00  5.03990000E-02  7.99078100E+00  1.00000000E+00  1.00000000E+00
     1 Singlet_1//TRAJ_00002/./Geo.out  1.00000000E+00  3.80370000E-02  8.00349700E+00  1.00000000E+00  1.00000000E+00  
     1 Singlet_1//TRAJ_00002/./Geo.out  2.00000000E+00  1.09515000E-01  7.93073500E+00  1.00000000E+00  1.00000000E+00 
     ...

     2 Singlet_1//TRAJ_00004/./Geo.out  0.00000000E+00  2.10908000E-01  8.29417600E+00  1.00000000E+00  1.00000000E+00
     2 Singlet_1//TRAJ_00004/./Geo.out  5.00000000E-01  1.49506000E-01  8.35651800E+00  1.00000000E+00  1.00000000E+00
     2 Singlet_1//TRAJ_00004/./Geo.out  1.00000000E+00  1.05887000E-01  8.40056700E+00  1.00000000E+00  1.00000000E+00
     ...
\end{verbatim}
\end{example}
Note here in the example that the second trajectory has a time step of 1.0~fs and thus no data at 0.5~fs.

\paragraph{Format of \ttt{Type2} data set output}

\ttt{Type2} data sets are formatted such that all trajectories share a common time axis, hence for each time step there will be one line of data.
Each line starts with the time, followed columns with the data for the first trajectory, followed by the data for the second trajectory, etc.
Within each trajectory, first all X columns, then all Y columns are given. If a Y column contains only unit weights (using special file column ``0''), then this Y column is omitted from the \ttt{Type2} formatted output.
\begin{example}
\footnotesize\begin{verbatim}
#             1               2               3               4               5               6               7  ...
#          Time    X Column   5    X Column   6    X Column   5    X Column   6    X Column   5    X Column   6  ...
 0.00000000E+00  1.13340000E-01  7.99096900E+00  5.03990000E-02  7.99078100E+00  2.10908000E-01  8.29417600E+00  ...
 5.00000000E-01  1.59173000E-01  7.94395200E+00             NaN             NaN  1.49506000E-01  8.35651800E+00  ...
 1.00000000E+00  2.10868000E-01  7.89084000E+00  3.80370000E-02  8.00349700E+00  1.05887000E-01  8.40056700E+00  ...
 ...
\end{verbatim}
\end{example}
Note here in the example that the second trajectory does not have data at 0.5~fs.

\paragraph{Format of \ttt{Typ3} data set output}

\ttt{Type3} data sets are formatted such that all trajectories share common time and X axes.
The data is formatted block-wise, with the first block corresponding to the first time step and containing all points on the X grid, followed by an empty line, followed by the second block, etc.
Each block consists of $n_\text{grid}$ lines, each starting with the time and X value in the first two columns and followed by $n_\text{X}$ columns with the convoluted data.
\begin{example}
\footnotesize\begin{verbatim}
#             1               2               3               4 
#          Time          X_axis       Conv(5,0)       Conv(6,0) 
 0.00000000E+00 -1.45534000E-01  2.38544715E-05  0.00000000E+00 
 0.00000000E+00  2.30671958E-01  1.51462322E+00  0.00000000E+00 
 0.00000000E+00  6.06877917E-01  1.07930050E-01  0.00000000E+00 
 ...

 5.00000000E-01 -1.45534000E-01  1.31312692E-04  0.00000000E+00 
 5.00000000E-01  2.30671958E-01  1.28614756E+00  0.00000000E+00 
 5.00000000E-01  6.06877917E-01  4.46251462E-10  0.00000000E+00 
 ...

 1.00000000E+00 -1.45534000E-01  8.75871124E-05  0.00000000E+00 
 1.00000000E+00  2.30671958E-01  2.42291042E+00  0.00000000E+00 
 1.00000000E+00  6.06877917E-01  1.60277894E-16  0.00000000E+00 
 ...
\end{verbatim}
\end{example}


% ========================================================================================================= %

\section{Optimizations: \ttt{orca\_External} and \ttt{setup\_orca\_opt.py}}\label{sec:Orca_External}

All \sharc\ interfaces can deliver gradients for (multiple) ground and excited states in a uniform manner.
This allows in principle to perform optimizations of excited-state minima, conical intersections, or crossing points.
In order to employ a high-quality geometry optimizer for this task, the \sharc\ suite is interfaced to the external optimizer feature of \textsc{Orca}.
This is accomplished by providing the script \ttt{orca\_External}, which is called by \textsc{Orca}, runs any of the \sharc\ interfaces, constructs the appropriate gradient, and returns that to \text{Orca}.
For the methodology used to construct the gradients, see section~\ref{met:orcaopt}.

In order to easily prepare the input files for such an optimization, the script \ttt{setup\_orca\_opt.py} can be used.
It takes a geometry file, interface input files, and the path to \textsc{Orca}, and creates a directory containing all relevant input files.
In the following, \ttt{setup\_orca\_opt.py} is described first, because it is the script which the user directly employs. Afterwards, \ttt{orca\_External} is specified.

\subsection{Usage}

\ttt{setup\_orca\_opt.py} is an interactive script, which is started with:
\begin{verbatim}
user@host> $SHARC/setup_orca_opt.py
\end{verbatim}
Note that before executing the script you should prepare a template for the interface you want to use (as, e.g., in \ttt{setup\_init.py} or \ttt{setup\_traj.py}).

\subsection{Input}

In the input section, the script asks for: (i) the path to \textsc{Orca}, (ii) the input geometries, (iii) the optimization settings, (iv) the interface settings.

\paragraph{Path to \textsc{Orca}}

Here the user is prompted to provide the path to the \textsc{Orca} directory. Note that the script will not expand the user (\ttt{\textasciitilde}) and shell variables (since possibly the calculations are running on a different machine than the one used for setup). \ttt{\textasciitilde} and shell variables will only be expanded during the actual calculation.

\paragraph{Interface}

In this point, choose any of the displayed interfaces to carry out the ab initio calculations. Enter the corresponding number. 

\paragraph{Input geometry}

Here the user is prompted for a geometry file in XYZ format, containing one or more geometries (with consistent number of atoms).
For each geometry in this file, a directory with all input files is created, in order to carry out multiple optimizations (e.g., with output from \ttt{crossing.py}).

\paragraph{Number of states}

Here the user can specify the number of excited states to be calculated. Note that the ground state has to be counted as well, e.g., if 4 singlet states are specified, the calculation will involve the $S_0$, $S_1$, $S_2$ and $S_3$. Also states of higher multiplicity can be given, e.g.\ triplet or quintet states. 

\paragraph{States to optimize}

Two different optimization tasks can be carried out: optimization of a minimum (ground or excited state) or optimization of a crossing point (either a conical intersection between states of the same multiplicity or a minimum-energy crossing points between states of different multiplicities; this is detected automatically).

For minima, the state to optimize needs to be specified.
For crossing points, the two involved states need to be specified.
In all cases, the specified states need to be included in the number of states given before.

\paragraph{CI optimization parameters}

If you are optimizing a conical intersection (states of same multiplicity) with an interface which cannot provide nonadiabatic coupling vectors (e.g., \ttt{SHARC\_RICC2.py}, \ttt{SHARC\_ADF.py}, or \ttt{SHARC\_GAUSSIAN.py}), then the optimization will employ the the penalty function method of Levine, Coe, and Mart\'inez~\cite{Levine2008JPCB}.
In this method, a penalty function is optimized, which depends on the energies of the two states and on two parameters, $\sigma$ and $\alpha$ (see section~\ref{met:orcaopt} for their mathematical meaning).

Practically, the parameters affect how close the minimum of the penalty function is to the true minimum on the crossing seam and how hard the optimization will be to converge.
Generally, a large $\sigma$ will allow going closer to the true conical intersection, but will make the penalty function more stiff (steeper harmonic) and thus harder to optimize.
A small $\alpha$ will also allow going closer to the true conical intersection, but will make the penalty function less harmonic; at $\alpha=0$, the penalty function will have a cusp at the minimum, making it unoptimizable because the gradient never becomes zero.

The default values, 3.5 and 0.02, are the ones suggested in~\cite{Levine2008JPCB}.
They can be regarded as relatively soft, i.e., they enable a very smooth convergence but might lead to unacceptably large energy gaps at convergence (i.e., the minimum of the penalty function is too far from the true minimum of the crossing seam).
In this case, it is advisable to restart the optimization from the last point with increased $\sigma$ (e.g., by a factor of 4), and simultaneously reducing the maximum step (see next point).
The $\alpha$ is best left at the suggested value of 0.02 to avoid the cusp problem.

\paragraph{Maximum allowed displacement}

Within \textsc{Orca}, it is possible to restrict the maximum step (the trust radius) of the optimizer. 
A larger maximum step might decrease the number of iterations necessary, but might also lead to instabilities in the optimization (if the potential energy surface is very steep or anharmonic).
Hence, it can be advisable to reduce the maximum allowed step (from the default of 0.3 a.u.), especially if the starting geometry is already very good (e.g., after restart with increase of $\sigma$) or if the potential is known to be stiff (strong bond, large $\sigma$, small $\alpha$, ...).
Note that the maximum step can be restricted even the penalty function method is not used and $\sigma$ and $\alpha$ are not relevant.

\paragraph{Interface-specific input}

This input section is basically the same as for \ttt{setup\_init.py} (sections~\ref{sec:setup_init.py:several}). Also for the optimizations an initial wave function file should be provided, especially for the multi-reference methods.

\paragraph{Run script setup}

Here the user needs to provide the path to the directory where the optimizations should be setup.


\subsection{Output}

Inside the specified directory, \ttt{setup\_orca\_opt.py} creates one subdirectory for each geometry in the input geometry file.
Each subdirectory is prepared with the corresponding initial geometry file (\ttt{geom.xyz}), the \textsc{Orca} input file (\ttt{orca.inp}), the appropriate interface-specific files (template, resources, QM/MM), and a shell script for execution (\ttt{run\_EXTORCA.sh}).

In order to run one of the optimizations, execute the shell script or send it to a batch queuing system.
Note that \$SHARC needs to be added to the \$PATH so that \textsc{Orca} can find \ttt{orca\_External} (this is automatically done inside \ttt{run\_EXTORCA.sh}).

When the shell script is started, \textsc{Orca} will write a couple of output files, where the two most relevant are \ttt{orca.trj} and \ttt{orca.log}.
The former is an XYZ file with all geometries from the optimization steps.
The latter (the \textsc{Orca} standard output) contains all details of the optimization (convergence, step size, etc) as well as a summary of what \ttt{orca\_External} did (after the line \ttt{EXTERNAL SHARC JOB}).
This summary contains all relevant energies and shows how the gradient is constructed.
Note that in each iteration, a line starting with \ttt{\textgreater\textgreater\textgreater} is written, which contains the energies of the optimized state(s). This line can easily be extracted with \ttt{grep} to follow the optimization of a crossing point.

\subsection{Description of \ttt{orca\_External}}

\ttt{orca\_External} provides a connection between the external optimizer of \textsc{Orca} and any of the \sharc\ interfaces.
In figure~\ref{fig:interface_orcaExt}, the file communication between \textsc{Orca}, \ttt{orca\_External}, and the interfaces is presented.

\begin{figure}[htb]
  \centering
  \includegraphics[scale=1]{img/interfaces/general_orcaExternal.pdf}
  \caption{Communication between \textsc{Orca}, \ttt{orca\_External}, the interfaces, and the quantum chemistry codes.}
  \label{fig:interface_orcaExt}
\end{figure}

As can be seen, \ttt{orca\_External} writes \ttt{QM.in} and \ttt{QM.out} files, in the same way that \ttt{sharc.x} is doing.
All information to write the \ttt{QM.in} file comes from the \textsc{Orca} communication file \ttt{orca.extcomp.inp} (geometry) and the \textsc{Orca} input file (number of states, interface, states to optimize).
To provide the latter information, \ttt{orca\_External} reads specially marked comments from the \textsc{Orca} input file which are ignored by \textsc{Orca}.
These comments start with \ttt{\#SHARC:}, followed by a keyword (\ttt{states}, \ttt{interface}, \ttt{opt}, or \ttt{param}) and the keyword arguments.


% ========================================================================================================= %

\section{Single Point Calculations: \ttt{setup\_single\_point.py}}\label{sec:setup_single_point.py}

It is possible to run single point calculations through the \sharc\ interfaces.
This is useful, e.g., to do a computation in exactly the same way as during the dynamics simulations.
Single point calculations using the interfaces can also be easily automatized.


\subsection{Usage}

\ttt{setup\_single\_point.py} is an interactive script, which is started with:
\begin{verbatim}
user@host> $SHARC/setup_single_point.py
\end{verbatim}
Note that before executing the script you should prepare a template for the interface you want to use (as, e.g., in \ttt{setup\_init.py} or \ttt{setup\_traj.py}).

\subsection{Input}

In the input section, the script asks for: (i) the input geometries, (ii) the number of states, and (iii) the interface settings.

\paragraph{Interface}

First, choose any of the displayed interfaces to carry out the ab initio calculations. Enter the corresponding number. 

\paragraph{Input geometry}

Here the user is prompted for a geometry file in XYZ format, containing one or more geometries (with consistent number of atoms).
For each geometry in this file, a directory with all input files is created, in order to carry out multiple optimizations (e.g., with output from \ttt{crossing.py}).

\paragraph{Number of states}

Here the user can specify the number of excited states to be calculated. Note that the ground state has to be counted as well, e.g., if 4 singlet states are specified, the calculation will involve the $S_0$, $S_1$, $S_2$ and $S_3$. Also states of higher multiplicity can be given, e.g.\ triplet or quintet states. 

\paragraph{Interface-specific input}

This input section is basically the same as for \ttt{setup\_init.py} (sections~\ref{sec:setup_init.py:several}). Also for the optimizations an initial wave function file should be provided, especially for the multi-reference methods.

\paragraph{Run script setup}

Here the user needs to provide the path to the directory where the optimizations should be setup.


\subsection{Output}

Inside the specified directory, \ttt{setup\_single\_point.py} creates one subdirectory for each geometry in the input geometry file.
Each subdirectory is prepared with the corresponding geometry file (\ttt{QM.in}), the appropriate interface-specific files (template, resources, QM/MM), and a shell script for execution (\ttt{run.sh}).

In order to run one of the optimizations, execute the shell script or send it to a batch queuing system.

When the shell script is started, the chosen \sharc\ interface is executed.
The interface writes a file called \ttt{QM.log} which contains details of the computation and progress status.
The final results of the computation are written to \ttt{QM.out}, which can be inspected manually.
Alternatively, some basic data (excitation energies, oscillator strengths) can be computed with \ttt{QMout\_print.py} (section \ref{sec:QMout_print.py}).



% ========================================================================================================= %

\section{Format Data from \ttt{QM.out} Files: \ttt{QMout\_print.py}}\label{sec:QMout_print.py}

With the script \ttt{QMout\_print.py} one can print a table with energies and oscillator strengths from a \ttt{QM.out} file, as it is produced by the interfaces.

\subsection{Usage}

\ttt{QMout\_print.py} is a command line tool, and is executed like this:
\begin{verbatim}
user@host> $SHARC/QMout_print.py [options] QM.out
\end{verbatim}
The options are summarized in Table~\ref{tab:QMoutprint_options}

\begin{table}[htb]
  \centering
  \caption{Command-line options for \ttt{QMout\_print.py}. }
  \label{tab:QMoutprint_options}
  \begin{tabular}{>{\ttfamily}lll}
    \hline
    \rmfamily Option         &Description    &Default\\
    \hline
    -h          &Display help message and quit.         &---       \\
    -i FILENAME &Path to \ttt{QM.in} file (to read number of states)    &---\\
    -s INTEGERS &List of numbers of states per multiplicity             &1\\
    -e FLOAT    &Absolute energy shift in Hartree                       &0.0\\
%     -n INTEGER  &Number of atoms (currently no need to specify)         &1\\
    -D          &Output diagonal states                                 &MCH states\\
    \hline
  \end{tabular}
\end{table}

\subsection{Output}

The script prints a table with state index, state label, energy, relative energy, oscillator strength, and spin expectation value to standard output.










% ========================================================================================================= %

\section{Diagonalization Helper: \ttt{diagonalizer.x}}\label{sec:diagonalizer.x}

The small program \ttt{diagonalizer.x} is used by the Python scripts to diagonalize matrices if the \textsc{NumPy} library is not available. Currently, only \ttt{excite.py} and \ttt{SHARC\_Analytical.py} need to diagonalize matrices. The program \ttt{diagonalizer.x} is implemented as a simple front-end to the LAPACK libary.

The program reads from stdin. The first line consists of the letter ``r'' or ``c'', followed by two integers giving the matrix dimensions. For ``r'', the program assumes a real symmetric matrix, for ``c'' a Hermitian matrix. The matrix must be square.
The second line is a comment and is ignored.
In the following lines, the matrix elements are given. On each line one row of the matrix has to be written. If the matrix is complex, each line contains the double number of entries, where real and imaginary part are always given subsequently.
The following is an example input:
\begin{example}
\footnotesize\begin{verbatim}
c 2 2
comment
1.0  0.0    2.0  0.1
2.0 -0.1    6.0  0.0
\end{verbatim}
\end{example}

\normalsize
for the matrix 
\begin{equation}
  A=\begin{pmatrix}
      1 &2+0.1i\\
      2-0.1i&6
    \end{pmatrix}.
\end{equation}
The diagonal matrix and the matrix of eigenvectors is written to stdout.



% ========================================================================================================= %
% ========================================================================================================= %
% ========================================================================================================= %

\chapter{Methodology}
\label{chap:met}

In this chapter different aspects of \sharc\ simulations are discussed in detail. The topics are ordered alphabetically.

% ========================================================================================================= %

\section{Absorption Spectrum}\label{met:spectrum}

Using \ttt{spectrum.py}, an absorption spectrum can be calculated as the sum over the absorption spectra of each individual initial condition:
\begin{equation}
  \sigma(E)=\sum\limits_i^{n_\text{init}} \sigma_i(E),
\end{equation}
where $i$ runs over the initial conditions.

The spectrum of a single initial condition is the convolution of its line spectrum, defined through a set of tuples $(E^\alpha,f_\text{osc}^\alpha)$ for each electronic state $\alpha$, where $E^\alpha$ is the excitation energy and $f_\text{osc}^\alpha)$ is the oscillator strength.

The convolution of the line spectrum can be performed with \ttt{spectrum.py} using either Gaussian or Lorentzian functions. The contribution of a state $\alpha$ to the absorption spectrum $\sigma^\alpha(E)$ is given by:
\begin{align}
  \sigma_{\text{Gaussian}}^\alpha(E)&=
  \left(f_{\text{osc}}\right)_i^\alpha 
  \E^{c\left(E-E_i^\alpha\right)^2},\\
  \text{with}\qquad
  c&=-\frac{4\ln(2)}{\text{FWHM}^2},
\end{align}
or
\begin{align}
  \sigma_{\text{Lorentzian}}^\alpha(E)&=
  \frac{\left(f_{\text{osc}}\right)_i^\alpha}{\frac{1}{c}\left(E-E_i^\alpha\right)^2+1},\\
  \text{with}\qquad
  c&=\frac{1}{4}\text{FWHM}^2,
\end{align}
or
\begin{align}
  \sigma_{\text{Log-normal}}^\alpha(E)&=
  \left(f_{\text{osc}}\right)_i^\alpha 
  \frac{E_i^\alpha}{E}
  \E^{-\frac{c}{4\ln(2)}-\frac{\ln(2)}{c}\left(\ln(E)-\ln(E^\alpha_i)\right)^2},\\
  \text{with}\qquad
  c&=\left[\ln\left(\frac{\text{FWHM}+\sqrt{\text{FWHM}^2+4(E^\alpha_i)^2}}{2E^\alpha_i}\right)\right]^2,
\end{align}
where FWHM is the full width at half maximum.

% ========================================================================================================= %

\section{Active and inactive states}\label{met:activestates}

\sharc\ allows to ``freeze'' certain states, which then do not participate in the dynamics. Only energies and dipole moments are calculated, but all couplings are disabled. In this way, these states are never visited (hence also no gradients and nonadiabatic couplings are calculated, making the inclusion of these states cheap). Example:

\newcommand{\R}{\cellcolor{RL!30}}
\begin{example}
  \verb|nstates 2 0 2|

  \verb|actstates 2 0 1|
\end{example}

In the example given, state $T_2$ is frozen. The corresponding Hamiltonian looks like:

\begin{equation}
  \VEC{H}=
  \begin{pmatrix}
&      E(S_0)    &             &   a^*_{01} &\R a^*_{02}   &   b^*_{01} &\R b^*_{02}   &a_{01}       &\R a_{02}&\\
&                &   E(S_1)    &   a^*_{11} &\R a^*_{12}   &   b^*_{11} &\R b^*_{12}   &a_{11}       &\R a_{12}&\\
&      a_{01}    &   a_{11}    &   E(T_1)   &\R p^*_{12}   &            &\R{-q^*_{12}} &             &\R&\\
&  \R a_{02}    &\R a_{12}    &\R p_{12}   &   E(T_2)     &\R q^*_{12} &\R            &\R           &\R&\\
&      b_{01}    &   b_{11}    &            &\R -q_{12}    &   E(T_1)   &\R            &             &\R -q^*_{12}&\\
&  \R b_{02}    &\R b_{12}    &\R q_{12}   &\R            &\R          &   E(T_2)     &\R q^*_{12}  &\R&\\
&      a^*_{01}  &   a^*_{11}  &            &\R            &            &\R -q_{12}    &   E(T_1)    &\R p_{12}&\\
&  \R a^*_{02}  &\R a^*_{12}  &\R          &\R            &\R q_{12}   &\R            &\R p^*_{12}  &   E(T_2)&\\
  \end{pmatrix}
\end{equation}
where all matrix elements marked \begin{tabular}{c}\cellcolor{RL!50}red\end{tabular} are deleted, since $T_2$ is frozen. 

The corresponding matrix elements are also deleted from the nonadiabatic coupling and overlap matrices. For propagation including laser fields, also the corresponding transition dipole moments are neglected, while the transition dipole moments still show up in the output (in order to characterize the frozen states).

Active and frozen states are defined with the \ttt{states} and \ttt{actstates} keywords in the input file. Note that only the highest states in each multiplicity can be frozen, i.e., it is not possible to freeze the $T_1$ while having $T_2$ active. However, it is possible to freeze all states of a certain multiplicity.

% ========================================================================================================= %

\section{Amdahl's Law}\label{met:amdahl}

Some of the interfaces (\ttt{SHARC\_MOLCAS.py}, \ttt{SHARC\_ADF.py} \ttt{SHARC\_GAUSSIAN.py}, \ttt{SHARC\_ORCA.py}) use Amdahl's law to predict the most efficient way to run multiple calculations in parallel, where each calculation itself is parallelized.
For example, in \ttt{SHARC\_GAUSSIAN.py} it might be necessary to compute the gradients of five states, using four CPU cores.
The most efficient way to run these five jobs depends on how well the \textsc{Gaussian} computation scales with the number of cores---for bad scaling, running four jobs on one core each followed by the fifth job might be best, whereas for good scaling, running each job on four cores subsequently is better because no core is idle at any time.
In order to automatically distribute the jobs efficiently, the interfaces use Amdahl's law, which can be stated as:
\begin{equation}
  T(n_\text{core})
  =
  T(1)
  \left(
    1-r+\frac{r}{n_\text{core}}
  \right).
\end{equation}
Here, $T(1)$ is the run time of the job with one CPU core, and $r$ is the fraction of $T(1)$ which benefits from parallelization.
The parameter $r$ can be given to the interfaces; it is between 0 and 1, where 0 means that the calculation does not get faster at all with multiple cores, whereas 1 means that the run time scales linearly with the number of cores.

% ========================================================================================================= %

\section{Bootstrapping for Population Fits}\label{met:bootstrapping}

Bootstrapping, in the context of population fitting, is a statistical method to obtain the statistical distribution of the fitted parameters, which can be used to infer the error associated with the fitted parameter.
The general idea of bootstrapping is to take the original sample (the set of trajectories in the ensemble), and generate new samples (resamples) by randomly drawing trajectories ``with replacement'' from the original ensemble.
These resamples will differ form the original ensemble by containing some trajectories multiple times while other trajectories might be missing.
For each of the resamples, the fitting parameter are obtained normally and saved for later analysis.

After many resamples, we obtain a list of many ``alternative'' parameters, which can be plotted in a histogram to see the statistical distribution of the fitting parameter.
The number of resamples should generally be large (several hundred or thousand resamples), although with \ttt{bootstrap.py}, one can inspect the convergence of the fitting parameters/errors to decide how many resamples are sufficient.

From the computed list of parameters, error measures can be computed.
Assume that $\{x_i\}$ is the set of fitting parameters obtained.
\ttt{bootstrap.py} and \ttt{make\_fit.py} compute the arithmetic mean and standard deviation like this:
\begin{align}
  \bar{x}_\text{arith}&=\frac{1}{N}\sum_i^Nx_i,\\
  \sigma_\text{arith}(x)&=\sqrt{\frac{1}{N-1}\sum_i^N(x-\bar{x})^2}.
\end{align}
Because the distribution of $\{x_i\}$ might be skewed (e.g., contains some very large values but few very small ones), the script also computes the geometric mean and standard deviation like this:
\begin{align}
  \bar{x}_\text{geom}&=\E^{\frac{1}{N}\sum_i^N\ln(x_i)},\\
  \sigma_\text{geom}(x)&=\E^{\sqrt{\frac{1}{N-1}\sum_i^N(\ln(x)-\ln(\bar{x}))^2}}
\end{align}
Note that the geometric standard deviation is a dimensionless factor (unlike the arithmetic standard deviation, which has the same dimension as the mean).
Therefore, within \ttt{bootstrap.py} and \ttt{make\_fit.py} the geometric errors are always displayed with separate upper and lower bounds as $\bar{x}\substack{+\bar{x}(\sigma(x)-1) \\ -\bar{x}(1/\sigma(x)-1)}$. 
Note that \ttt{bootstrap.py} and \ttt{make\_fit.py} will always report one times the standard deviation in the output.
If larger confidence intervals are desired, simply multiply the arithmetic error as usual.
For the geometric error, use for example $\bar{x}\substack{+\bar{x}(\sigma(x)^2-1) \\ -\bar{x}(1/\sigma(x)^2-1)}$.


% ========================================================================================================= %

\section{Damping}\label{met:damping}

If damping is activated in \sharc\ (keyword \ttt{dampeddyn}), in each time step the following modification to the velocity vector is made
\begin{equation}
  \VEC{v}^\prime=\VEC{v}\cdot\sqrt{C}
\end{equation}
where $C$ is the damping factor given in the input. Hence, in each time step the kinetic energy is modified by
\begin{equation}
  E_{\text{kin}}^\prime=E_{\text{kin}}\cdot C
\end{equation}
The damping factor $C$ must be between 0 and 1.



% % ========================================================================================================= %
% 
% \section{Data Collector Procedures}\label{met:data_collector}


% ========================================================================================================= %

\section{Decoherence}\label{met:decoherence}

In surface hopping, without any corrections the coherence between the states is usually too large \cite{Granucci2007JCP}. A trajectory in state $\beta$, but where state $\alpha$ has a large coefficient, will still travel according to the gradient of state $\beta$. However, the gradients of state $\alpha$ are almost certainly different to the ones of state $\beta$. As a consequence, too much population of state $\alpha$ is following the gradient of state $\beta$. Decoherence corrections damp in different ways the population of all states $\alpha\neq\beta$, so that only population of $\beta$ follows the gradient of state $\beta$.

Currently, in \sharc\ there are two decoherence corrections implemented, ``energy-based decoherence'', or EDC, as given in \cite{Granucci2010JCP} and ``augmented fewest-switches surface hopping'', or AFSSH, as described in \cite{Jain2016JCTC}.

\subsection{Energy-based decoherence}

In this scheme, after the surface hopping procedure, when the system is in state $\beta$, the coefficients are updated by the following relation
\begin{align}
  c_\alpha^\prime=&
  c_\alpha\cdot\exp
  \left[
    -\Delta t\frac{|E_\alpha-E_\beta|}{\hbar}
    \left(
      1+\frac{C}{E_{\text{kin}}}
    \right)^{-1}
  \right],\qquad \alpha\neq\beta,\\
  c_\beta^\prime=&
  \frac{c_\beta}{|c_\beta|}\cdot
  \left[
    1-\sum\limits_{\alpha\neq\beta}^{\vphantom{\alpha}} |c^\prime_\alpha|^2
  \right]^{\frac{1}{2}}
\end{align}
where $C$ is the decoherence parameter. The decoherence correction can be activated with the keyword \ttt{decoherence} in the input file. The decoherence parameter $C$ can be set with the keyword \ttt{decoherence\_param} (the default is 0.1~hartree, as suggested in \cite{Granucci2010JCP}).



\subsection{Augmented FSSH decoherence}

Augmented FSSH by Subotnik and coworkers is described in \cite{Jain2016JCTC}.
For \sharc, the augmented FSSH algorithm was adjusted to the case of the diagonal representation.

The basic idea is that besides the actual trajectory, the program maintains an auxiliary trajectory for each state.
The auxiliary trajectories are propagated using the gradients of the associated (not active) state, and because the gradients are different, the auxiliary trajectories eventually diverge from each other and from the main trajectory.
From this diverging, one can compute decoherence rates which can be used to stochastically set the electronic coefficients of the diverging state to zero.

First, we compute two matrices:
\begin{align}
  \VEC{S}^\text{diag}(t,t+\Delta t)&=\VEC{U}^\dagger(t)\VEC{S}(t,t+\Delta t)\VEC{U}(t+\Delta t),\\
  \VEC{H}^\text{olddiag}(t+\Delta t)&=\VEC{U}^\dagger(t)\VEC{S}(t,t+\Delta t)\VEC{H}^\text{MCH}(t+\Delta t)\VEC{S}^\dagger(t,t+\Delta t)\VEC{U}(t),
\end{align}
where $\VEC{S}$ is the overlap matrix, as used in~\ref{met:propagate}.
Hence, $\VEC{S}^\text{diag}(t,t+\Delta t)$ is the overlap matrix in the diagonal basis and $\VEC{H}^\text{olddiag}(t+\Delta t)$ is the Hamiltonian at time $t+\Delta t$ expressed in the diagonal basis of time step $t$.

We then propagate the auxiliary trajectories for each state $j$, considering that the active state is $\alpha$.
For this, we need the gradient matrix $\VEC{G}^\text{diag}$ from section~\ref{met:gradtra}.
We define:
\begin{align}
  \sigma_j=|c_j(t)|^2,\\
\end{align}
We do the $X$ step of the velocity-Verlet algorithm:
\begin{align}
  \VEC{a}^j_A(t)&=-\frac{((\VEC{G}^\text{diag})_{jj}-(\VEC{G}^\text{diag})_{\alpha\alpha})_A}{M_A},\\
  \VEC{R}^j(t+\Delta t)&= \VEC{R}^j(t)+\VEC{v}^j(t)\Delta t +\frac{1}{2}\VEC{a}^j(t)\Delta t^2\sigma_j,
\end{align}
where $A$ goes over the atoms.
Then, we compute the new gradient:
\begin{equation}
  \VEC{g}^j(t+\Delta t)=
  -(\VEC{G}^\text{diag})_{\alpha\alpha}
  +
  \sum_i(\VEC{S}^\text{diag}(t,t+\Delta t))_{ji}(\VEC{G}^\text{diag})_{ii}.
\end{equation}
We then carry out the $v$ step:
\begin{align}
  \VEC{a}^j_A(t+\Delta t)&=\frac{1}{2}\VEC{a}^j_A(t)
  -\frac{(\VEC{g}^j(t+\Delta t))_A}{M_A},\\
  \VEC{v}^j(t+\Delta t)&=\VEC{v}^j(t)+\VEC{a}^j_A(t+\Delta t)\Delta t\sigma_j.
\end{align}
We then perform a diabatization of the auxiliary trajectories (e.g., for a trivially avoided crossing, the moments between the crossing states are interchanged):
\begin{align}
  \VEC{R}^j&=\sum_i(\VEC{S}^\text{diag}(t,t+\Delta t))_{ij}\VEC{R}^i,\\
  \VEC{v}^j&=\sum_i(\VEC{S}^\text{diag}(t,t+\Delta t))_{ij}\VEC{v}^i,\\
\end{align}
to transform the data back into the adiabatic basis at time $t+\Delta t$.

Finally, we carry out the decoherence correction procedure for each auxiliary trajectory.
We compute the displacement from the auxiliary trajectory of the active state:
\begin{equation}
  \VEC{D}=\VEC{R}^j-\VEC{R}^\alpha
\end{equation}
We compute two rate constants:
\begin{align}
  r_1^j&=-\frac{1}{2}\VEC{g^j}(t+\Delta t)\cdot\VEC{D},\\
  r_2^j&=2|(\VEC{G}^\text{diag})_{\alpha j}\cdot\VEC{D}|,
\end{align}
or if no nonadiabatic coupling vectors are available:
\begin{align}
  r_2^j&=2\frac{|H^\text{olddiag}_{\alpha j}|}{\Delta t}\frac{\VEC{D}\cdot\VEC{v}}{\VEC{v}\cdot\VEC{v}}.
\end{align}

We draw a random number (identical for all states) $r$ between 0 and 1.
If $r<\Delta t(r^j_1-r^j_2)$, then we collapse state $j$, by setting its coefficient to zero and enlarging the coefficient of the active state such that the total norm is conserved; we also set the moments $\VEC{R}^j$ and $\VEC{v}^j$ to zero.
If no collapse occurred, if $r<-\Delta t r^j_1$, we set the moments $\VEC{R}^j$ and $\VEC{v}^j$ to zero.

After a surface hop occurred, we reset all auxiliary trajectories.






% ========================================================================================================= %

\section{Essential Dynamics Analysis}\label{met:essdyn}

As an alternative to normal mode analysis (see section~\ref{met:nma}), essential dynamics analysis can be used to identify important modes in the dynamics.
This procedure is a principal component analysis of the geometric displacements.\cite{Amadei1993PSFB,Plasser2009}
Unlike normal mode analysis, it does not depend on the availability of the normal mode vectors.

The covariance matrix is computed from the following equation ($\mu$ and $\nu$ are indices over the $3N_\text{atom}$ degrees of freedom):
\begin{equation}
  \bar{R}_\mu=
  \frac{1}{N_\text{traj}(k_\text{end}-k_\text{start})}
  \sum_{i=1}^{N_\text{traj}}
  \sum_{k=k_\text{start}}^{k_\text{end}} 
  R_\mu^i(k\Delta t)
\end{equation}
and
\begin{equation}
  A_{\mu\nu}=
  \frac{1}{N_\text{traj}(k_\text{end}-k_\text{start})}
  \sum_{i=1}^{N_\text{traj}}
  \sum_{k=k_\text{start}}^{k_\text{end}} 
  R_\mu^i(k\Delta t)
  R_\nu^i(k\Delta t)
\end{equation}
as a matrix $\VEC{C}$ with elements:
\begin{equation}
  C_{\mu\nu}=
  A_{\mu\nu}-R_\mu R_\nu.
\end{equation}

Diagonalization of the symmetric matrix $\VEC{C}$ gives a set of eigenvalues and eigenvectors.
The eigenvectors represent the essential dynamics modes, and the corresponding eigenvalues are the variances of the modes.
Modes with large variances show strong motion in the dynamics, whereas small variances are found for modes which show weak motion.
Because the modes are uncorrelated, the few modes with the largest variance describe most of the molecular motion, which shows that essential dynamics analysis can be used to for data reduction.

% ========================================================================================================= %

\section{Excitation Selection}\label{met:exc_selection}

\ttt{excite.py} can select initial active states for the dynamics based on the excitation energies $E_{k,\alpha}$ and the oscillator strengths $f^{\text{osc}}_{k,\alpha}$ for each initial condition $k$ and excited state $\alpha$. 

First, for all excited states of all initial conditions, the maximum value $p_{\text{max}}$ of 
\begin{equation}
  p_{k,\alpha}=\frac{f^{\text{osc}}_{k,\alpha}}{E_{k,\alpha}^2} \label{eq:exc_prob}
\end{equation}
is found. Then for each excited state, a random number $r_{k,\alpha}$ is picked from $[0,1]$. If
\begin{equation}
  r_{k,\alpha}<\frac{p_{k,\alpha}}{p_{\text{max}}}
\end{equation}
then the excited state is selected as a valid initial condition. This excited-state selection scheme is taken from \cite{Barbatti2011}.

Within \ttt{excite.py} it is possible to restrict the selection to a subset of all excited states (only certain adiabatic states/within a given energy range). In this case, also $p_{\text{max}}$ is only determined based on this subset of excited states.

\subsection{Excitation Selection with Diabatization}\label{met:exc_diabatic}

The active states can be selected based on a diabatization.
This necessitates the wave function overlaps between a reference geometry (\ttt{ICOND\_00000/}) and the current initial condition $k$.

The overlap matrix with elements
\begin{equation}
  S^{0k}_{ij}=\left\langle \Psi_i(\VEC{R_0})|\Psi_j(\VEC{R}_k)\right\rangle
\end{equation}
can be computed with \ttt{wfoverlap.x} (calculations can be setup with \ttt{setup\_init.py}).
This overlap matrix is rescaled during the excitation selection procedure such $S^{0k}_{11}$ is equal to one (by dividing all elements by $|S^{0k}_{11}|^2$).
Then, assume we want to start all trajectories in the state which corresponds to state $x$ at the reference geometry.
The excitation selection will select a state $y$ as initial state if $S^{0k}_{xy}>0.5$.


% ========================================================================================================= %

\section{Global fits and kinetic models}\label{met:globalfit}

In this section, we specify the basic assumptions of the chemical kinetic models used with the scripts \ttt{make\_fitscript.py} and \ttt{make\_fit.py} and the globals fits of these models to data.

\subsection{Reaction networks}

The kinetic models used by \ttt{make\_fitscript.py} and \ttt{make\_fit.py} are based on a chemical reaction network, where chemical \textit{species} react via unimolecular \textit{reactions}.

The reaction networks allowed in the script are a simple directed graphs, where the species are the vertices and the reactions are the directed edges.
Each reaction is characterized by an associated rate constant and connects exactly one reactant species to exactly one product species.

In order to obtain rate laws which can be integrated easily, there are a number of restrictions imposed on the network graphs.
Some restrictions and possible features of the graphs are depicted exemplarily in Figure~\ref{fig:graph_restrictions}.
First, each species and each reaction rate needs to have a unique label. 
There cannot be two species with the same label, but there can be two reactions with the same reaction rate constant.
Second, the graph must be a simple directed graph, hence there cannot be any (self-) loops or more than one reaction with the same initial and the same final species.
All restrictions marked as ``Not allowed'' in the Figure are enforced in the input dialogue of \ttt{make\_fitscript.py}
However, back reactions are allowed (as a back reaction has a different initial and a different final species as the corresponding reaction).
Except from these restrictions, the graph may contain combinations of sequential and parallel reactions.
The graph may also be disjoint, i.e., it can be the union of several independent sub-networks.
Disjoint graphs with repeated reaction labels can be useful to fit population data from ensembles with identical settings but different initial conditions (in this case, merge the population files with \ttt{paste} before starting the fit.

There are two kinds of cycles possible, called here \textit{parallel pathways} and \textit{closed walks}.
Parallel pathways are independent sequences of reactions with the same initial and final species (e.g., $A\rightarrow C$ and $A\rightarrow B\rightarrow C$).
A closed walk is a sequence of reactions where the initial species is equal to the final species.
These cycles can sometimes lead to problems.
If you use \ttt{make\_fitscript.py}, then it is necessary that the system of differential equations of the rate laws can be integrated in closed form by \textsc{Maxima}.
Since \ttt{make\_fit.py} solves the differential equation system numerically, it is not necessary that the solution can be given in closed form.
However, cycles can also lead to problems in the fitting procedure (rate constants in parallel pathways can be strongly correlated and cause large errors and bad convergence in fitting).

\begin{figure}[htb]
  \centering
  \includegraphics[scale=1]{img/reaction_networks/allowed_notallowed.pdf}
  \caption{Forbidden and allowed features of the reaction network graphs.}
  \label{fig:graph_restrictions}
\end{figure}

An example reaction network graph is shown in Figure~\ref{fig:example_graph}.
In this graph, there are 5~species ($S_2$, $S_1$, $S_0$, $T_2$, and $T_1$) and 6~reactions, each with a rate constant ($k_S$, $k_{Rlx}$, $k_{22}$, $k_{11}$, $k_{21}$, $k_{12}$).
This graph shows some features which are allowed in the reaction networks for \ttt{make\_fitscript.py}: sequential reactions, parallel reactions, back reactions, and converging reaction pathways.
Note that this reaction network is likely to cause problems in the fitting step (large errors).

\begin{figure}[htb]
  \centering
  \includegraphics[scale=1]{img/reaction_networks/example_graph.pdf}
  \caption{Example reaction network graph. For explanation see text.}
  \label{fig:example_graph}
\end{figure}

\subsection{Kinetic models}

Based on the reaction network graph, a system of differential equations describing the rate laws of all species can be setup.
The system of equations (equivalently, the matrix differential equation) can be written as:
\begin{equation}
  \frac{\partial}{\partial t} \VEC{s}(t) = \VEC{A} \cdot \VEC{s}(t),
  \label{eq:rate_law}
\end{equation}
where $\VEC{s}$ is the vector of the time-dependent populations of each species and $\VEC{A}$ is the matrix containing the rate constants.
In order to construct $\VEC{A}$, start with $\VEC{A}=0$ and, for each reaction from species $i$ to species $j$ with rate $k$, substract $k$ from $A_{ii}$ and add $k$ to $A_{ji}$.

In order to integrate this system of equations, in practice one also needs to define initial conditions. 
In the present context, the initial conditions are fully specified by $\VEC{s}(t=0)=\VEC{s}_0$, where $\VEC{s}_0$ are constant expressions defined by the user.

Solving~\eqref{eq:rate_law} yields (in not too complicated cases) the closed-form expressions for the functions $\VEC{s}(t)$, which contain as parameters all rate constants $k$ and all initial values $\VEC{s}_0$.

\subsection{Global fit}

Suppose there is a data set $\VEC{p}(t)=(p_1(t),p_2(t),\dots)$ of time-dependent populations of several states $k=1,2,\dots$ and which is given at several points of time $\{t_i: 0<t_i<t_\mathrm{max}\}$.
We can \textit{fit} to one data set $p_k(t)$ a function $s_l(t)$ from $\VEC{s}(t)$ by optimizing the parameters (mainly the rate constants) such that $\sum_i |p_k(t_i)-s_l(t_i)|^2$ becomes minimal.

In order to perform global fit including several species $l$ and several states $k$, we construct piecewise global functions from $\VEC{p}(t)$ and $\VEC{s}(t)$ and then optimize the parameters accordingly.




% ========================================================================================================= %

\section{Gradient transformation}\label{met:gradtra}

Since the actual dynamics is performed on the PESs of the diagonal states, also the gradients have to be transformed to the diagonal representation. To this end, first a generalized gradient matrix $\VEC{G}^{\text{MCH}}$ is constructed from the gradients %
%%tth: $\VEC{g}^{\text{MCH}}_\alpha=-\nabla_RH_{\alpha\alpha}^{\text{MCH}}$
\tthdump{%
$\VEC{g}^{\text{MCH}}_\alpha=-\nabla_\VEC{R}H_{\alpha\alpha}^{\text{MCH}}$%
}
and the nonadiabatic coupling vectors 
$\VEC{K}_{\beta\alpha}^{\text{MCH}}$:
\begin{equation}
  \VEC{G}^{\text{MCH}}=
  \begin{pmatrix}
    \VEC{g}_1   &-(H_{11}-H_{22})\VEC{K}_{12} &-(H_{11}-H_{33})\VEC{K}_{13} &\cdots\\
    -(H_{22}-H_{11})\VEC{K}_{21}      &\VEC{g}_2      &-(H_{22}-H_{33})\VEC{K}_{23}&\cdots\\
    -(H_{33}-H_{11})\VEC{K}_{31}      &-(H_{33}-H_{22})\VEC{K}_{32} &\VEC{g}_3\\
    \vdots      &\vdots         &       &\ddots
  \end{pmatrix}
\end{equation}
Note that all quantities in the matrix are in the MCH representation, the superscripts were omitted for brevity.

The matrix $\VEC{G}^{\text{MCH}}$ is subsequently transformed into the diagonal representation, using the transformation matrix $\VEC{U}$:
\begin{equation}
  \VEC{G}^{\text{diag}}=\VEC{U}^\dagger\VEC{G}^{\text{MCH}}\VEC{U}.
\end{equation}
The diagonal elements of $\VEC{G}^{\text{diag}}$ now contain the gradients of the diagonal states, while the off-diagonal elements contain the scaled nonadiabatic couplings $-(H^{\text{diag}}_{\beta\beta}-H^{\text{diag}}_{\alpha\alpha})\VEC{K}_{\beta\alpha}^{\text{diag}}$. The gradients are needed in the Velocity Verlet algorithm in order to propagate the nuclear coordinates. The nonadiabatic couplings are necessary if the velocity vector is to be rescaled along the nonadiabatic coupling vector.

Since the matrix $\VEC{G}^{\text{MCH}}$ contains elements which are itself vectors with $3N$ components, the transformation is done component-wise (e.g.\ first a matrix $G_{x,\text{atom} 1}$ is constructed from the $x$ components of all gradients (and nonadiabatic couplings) for atom $1$, this matrix is transformed and written to the $x$, atom $1$ component of $\VEC{G}^{\text{diag}}$, then this is repeated for all components).

Since the calculation of the nonadiabatic couplings $\VEC{K}^{\text{MCH}}_{\beta\alpha}$ might add considerable computational cost, there is the input keyword \ttt{nogradcorrect} which tells \sharc\ to neglect the $\VEC{K}^{\text{MCH}}_{\beta\alpha}$ in the gradient transformation.

\subsection{Dipole moment derivatives}\label{met:dipolegrad}

For strong laser fields, the derivative of the dipole moments might be a non-negligible contribution to the gradients. In this case, they should be included in the gradient transformation step:
\begin{equation}
  \VEC{G}^{\text{diag}}=\VEC{U}^\dagger
  \left[
    \VEC{G}^{\text{MCH}}
    -\boldsymbol{\epsilon}(t)\cdot\frac{\partial}{\partial R}\boldsymbol{\mu}
  \right]\VEC{U}.
\end{equation}
This can be activated with the keyword \ttt{dipole\_grad} in the \sharc\ input file.


% ========================================================================================================= %

\section{Internal coordinates definitions}\label{met:geo}

In this section, the internal coordinates available in \ttt{geo.py} are defined.

\paragraph{Bond length}

The bond length between two atoms $a$ and $b$ is:
\begin{equation}
  r_{ab}=
  \sqrt{
    (x_a-x_b)^2+(y_a-y_b)^2+(z_a-z_b)^2
  }
\end{equation}

\paragraph{Bond angle}

The bond angle for atoms $a$, $b$ and $c$ is defined as the angle
\begin{equation}
  \theta=
  \cos^{-1}\left(
    \frac{\VEC{v}_{ba}\cdot\VEC{v}_{bc}}{|\VEC{v}_{ba}|\cdot|\VEC{v}_{bc}|}
  \right)\label{eq:angle}
\end{equation}
where $\VEC{v}_{ba}$ is the vector from atom $b$ to atom $a$.

\paragraph{Dihedral}

The dihedral angle is defined via the vectors $\VEC{w}_1$ and $\VEC{w}_2$:
\begin{equation}
  \VEC{w}_1=\VEC{v}_{ab}\times\VEC{v}_{bc}\qquad\text{and}\qquad\VEC{w}_2=\VEC{v}_{bc}\times\VEC{v}_{cd}.
\end{equation}
The dihedral is given as the angle between $\VEC{w}_1$ and $\VEC{w}_2$ according to equation~\eqref{eq:angle}.
In order to distinguish left- and right-handed rotation, also the vector $\VEC{Q}=\VEC{w}_1\times\VEC{w}_2$ is computed, and if the angle between $\VEC{Q}$ and $\VEC{v}_{bc}$ is larger than 90$^\circ$ then the value of the dihedral is multiplied by -1.

\paragraph{Pyramidalization angle}

The pyramidalization angle is defined via the vectors $\VEC{v}_{ba}$ and 
\begin{equation}
  \VEC{w}_1=\VEC{v}_{bc}\times\VEC{v}_{bd}.
\end{equation}
The pyramidalization angle is then given as $90^\circ - \theta(\VEC{v}_{ba},\VEC{w}_1)$.
This definition of the pyramidalization angle works best for nearly planar arrangements, like in amino groups.

An alternative definition of the pyramidalization angle works better for strongly pyramidalized situations (e.g., \emph{fac}-arranged atoms in a octahedral metal complex).
This pyramidalization angle is defined as 180$^\circ$ minus the angle between $\VEC{v}_{ab}$ and the average of $\VEC{v}_{bc}$ and $\VEC{v}_{bd}$.

\paragraph{Cremer-Pople parameters}

The definitions of the Cremer-Pople parameters for 5- and 6-membered rings is described in \cite{Cremer1975JACS}.

\paragraph{Boeyens classification}

For 6-membered rings, the Boeyens classification scheme is described in \cite{Boeyens1976JCMS}.

\paragraph{Angle between two rings}

In order to compute angles between the mean planes of two rings, we compute the normal vector of the two rings as in \cite{Cremer1975JACS}.
We then compute the angle between the two normal vectors.

% ========================================================================================================= %

\section{Kinetic energy adjustments}\label{met:ekinadj}

There are several options how to adjust the kinetic energy after a surface hop occurred. The simplest option is to not adjust the kinetic energy at all (input: \ttt{ekincorrect none}), but this obviously leads to violation of the conservation of total energy.

Alternatively, the velocities of all atoms can be rescaled so that the new kinetic energy and the potential energy of the new state $\beta$ again sum up to the total energy.
\begin{align}
  f&=\sqrt{\frac{E_{\text{total}}-E_\beta}{E_{\text{kin}}}}\\
  \VEC{v}^\prime&=f\VEC{v}\\
  E_{\text{kin}}^\prime&=f^2E_{\text{kin}}
\end{align}
Within this methodology, when the energy of the old state $\alpha$ was lower than the energy of the new state $\beta$ the kinetic energy is lowered. Since the kinetic energy must be positive, this implies that there might be states which cannot be reached (their potential energy is above the total energy). A hop to such a state is called ``frustrated hop'' and will be rejected by \sharc. Rescaling parallel to the nuclear velocity vector is requested with \ttt{ekincorrect parallel\_vel}.

Alternatively, according to Tully's original formulation of the surface hopping method \cite{Tully1990JCP}, after a hop from $\alpha$ to $\beta$ only the component of the velocity along the direction of the nonadiabatic coupling vector $\VEC{K}_{\beta\alpha}$ should be rescaled. With
\begin{align}
  a&=\sum\limits_i^{N_\mathrm{atom}} \frac{\VEC{K}_{\beta\alpha, i}\cdot\VEC{K}_{\beta\alpha, i}}{2M_i}\\
  b&=\sum\limits_i^{N_\mathrm{atom}} \VEC{v}_{i}\cdot\VEC{K}_{\beta\alpha, i}
\end{align}
the available energy can be calculated:
\begin{equation}
  \Delta=
  4a
  \left(
    E_\alpha-E_\beta
  \right)+b^2.
\end{equation}
If $\Delta<0$, the hop is frustrated and will be rejected. Otherwise, the scaled velocities $\VEC{v}^\prime$ can be calculated as
\begin{align}
  \VEC{v}_i^\prime&=\VEC{v}_i-f\frac{\VEC{K}_{\beta\alpha, i}}{M_i}\\
  \mathrm{with}\quad f&=
  \begin{cases}
    \frac{b+\sqrt{\Delta}}{2a},\qquad b<0,\\
    \frac{b-\sqrt{\Delta}}{2a},\qquad b\geq 0.
  \end{cases}
\end{align}
This procedure can be requested with \ttt{ekincorrect parallel\_nac}. Note that in this case \sharc\ will request the nonadiabatic coupling vectors, even if they are not used for the wave function propagation.

\subsection{Reflection for frustrated hops}\label{met:refl_frust}

As suggested by Tully \cite{Tully1990JCP}, during a frustrated hop one might want to reflect the trajectory (i.e., invert some component of the velocity vector).
In \sharc, there are three possible options to this, the first being no reflection (\ttt{reflect\_frustrated none}).
Alternatively, one can invert the total velocity vector $\VEC{v}:=-\VEC{v}$ (\ttt{reflect\_frustrated parallel\_vel}).

As this leads to a nearly complete time reversal and might be inappropriate, as a third option one can choose to only reflect the velocity component parallel to the nonadiabatic coupling vector between the active state and the state to which the frustrated hop was attempted.
The condition for reflection in this case is based on three scalar products:
\begin{align}
  k_1&=\VEC{g}_\alpha\cdot \VEC{t}_{\alpha f},\\
  k_2&=\VEC{g}_f\cdot \VEC{t}_{\alpha f},\\
  k_3&=\sum_A M_Av_A(t_{\alpha f})_A,
\end{align}
Reflection is only carried out if $k_1k_2<0$ and $k_2k_3<0$.
In order to reflect, we compute:
\begin{equation}
  \VEC{v}_A:=\VEC{v}_A-2\frac{\VEC{v}_A\cdot\VEC{t}_A}{\VEC{t}_A\cdot\VEC{t}_A}\VEC{t}_A.
\end{equation}
where $\VEC{t}_A$ is the component of $\VEC{t}_{\alpha f}$ corresponding to atom $A$.



% ========================================================================================================= %

\section{Laser fields}\label{met:laser_field}

The program \ttt{laser.x} can calculate laser fields as superpositions of several analytical, possibly chirped, laser pulses. In the following, the laser parametrization is given (see \cite{Marquetand2007} for further details).

\subsection{Form of the laser field}

In general, the laser field $\boldsymbol{\epsilon}(t)$ is a linear superposition of a number of laser pulses $l_i(t)$:
\begin{equation}
  \boldsymbol{\epsilon}(t)=\sum\limits_i \VEC{p}_il_i(t),
\end{equation}
where $\VEC{p}_i$ is the normalized polarization vector of pulse $i$.

A pulse $l(t)$ is formed as the product of an envelope function and a field function. 
\begin{equation}
  l(t)=\mathcal{E}(t)f(t)
\end{equation}

\subsection{Envelope functions}

There are two types of envelope function defined in \ttt{laser.x}, which are Gaussian and sinusoidal.

The Gaussian envelope is defined as:
\begin{align}
  \mathcal{E}(t)&=\mathcal{E}_0 \E^{-\beta(t-t_c)^2}\label{eq:laser_gauss_1}\\
  \beta&=\frac{4\ln 2}{\mathrm{FWHM}^2}\label{eq:laser_gauss_2}
\end{align}
where $\mathcal{E}_0$ is the peak field strength, FWHM is the full width at half maximum and $t_c$ is the temporal center of the pulse.

The sinusoidal envelope is defined as:
\begin{equation}
  \mathcal{E}(t)=\mathcal{E}_0
  \begin{cases}
    0                                                   &\text{if } t<t_0,\\
    \sin^2\left(\frac{\pi(t-t_0)}{2(t_c-t_0)}\right)      &\text{if } t_0<t<t_c,\\
    1                                                   &\text{if } t_c<t<t_{c2},\\
    \cos^2\left(\frac{\pi(t-t_{c2})}{2(t_e-t_{c2})}\right)      &\text{if } t_{c2}<t<t_e,\\
    0                                                   &\text{if } t_e<t,\label{eq:laser_sinus}
  \end{cases}
\end{equation}
where again $\mathcal{E}_0$ is the peak field strength, $t_0$ and $t_c$ define the interval where the field strength increases, and $t_{c2}$ and $t_e$ define the interval where the field strength decreases. Figure~\ref{fig:laser_envelope} shows the general form of the envelope functions and the meaning of the temporal parameters $t_0$, $t_c$, $t_{2c}$ and $t_e$.

\begin{figure}[htb]
  \centering
  \includegraphics[scale=1]{img/laser_envelope/laser_envelope.pdf}
  \caption{Types of laser envelopes implemented in \ttt{laser.x}.}
  \label{fig:laser_envelope}
\end{figure}

\subsection{Field functions}

The field function $f(t)$ is defined as:
\begin{equation}
  f(t)=\E^{\I \left(\omega_0(t-t_c)+\phi\right)},
\end{equation}
where $\omega_0$ is the central frequency and $\phi$ is the phase of the pulse. Even though the laser field is complex in this expression, in the propagation of the electronic wave function in \sharc\ only the real part is used.

\subsection{Chirped pulses}

In order to apply a chirp to the laser pulse $l(t)$, it is first Fourier transformed to the frequency domain, giving the function $\tilde{l}(\omega)$. The chirp is applied by calculating:
\begin{equation}
  \tilde{l}^\prime(\omega)=
  \tilde{l}(\omega)
  \E^{-i\left[
  b_1|\omega-\omega_0|
  +\frac{b_2}{2}(\omega-\omega_0)^2
  +\frac{b_3}{6}(\omega-\omega_0)^3
  +\frac{b_4}{24}(\omega-\omega_0)^4
  \right]}\label{eq:laser_chirp}
\end{equation}
The chirped laser in the time domain $l^\prime(t)$ is then obtained by Fourier transform of the chirped pulse $\tilde{l}^\prime(\omega)$.


\subsection{Quadratic chirp without Fourier transform}

If \ttt{laser.x} was compiled without the FFTW package, the only accessible chirps are quadratic chirps for Gaussian pulses:
\begin{align}
  l(t)=&
  \mathcal{E}_0^\prime
  \E^{-\beta^\prime(t-t_c)^2}
  \E^{-\I\left(
    \omega_0(t-t_c)+a_2(t-t_c)^2+\phi
  \right)}\\
  \beta=&\frac{4\ln 2}{\mathrm{FWHM}^2}\\
  \beta^\prime=&\frac{1}{\frac{1}{\beta}+4\beta b_2^2}\\
  a_2=&\frac{b_2}{\frac{1}{4\beta^2}+b^2_2}\\
  \mathcal{E}_0^\prime=&\mathcal{E}_0\sqrt{\frac{1}{2\I b_2\beta+1}}
\end{align}
Other chirps are only possible with the Fourier transformation.


% ========================================================================================================= %

\section{Laser interactions}\label{met:laser}

The laser field $\boldsymbol{\epsilon}$ is included in the propagation of the electronic wave function. In each substep of the propagation, the interaction of the laser field with the dipole moments is included in the Hamiltonian. The contribution $\VEC{V}_i$ is in each time step added to the Hamiltonian in equations~\eqref{eq:ham_propn} or \eqref{eq:ham_propl}, respectively:
\begin{align}
  \VEC{V}_i=&
  -
  \Re\left(
    \boldsymbol{\mu}_i\cdot
    \boldsymbol{\epsilon}_i
  \right),\\
  \boldsymbol{\mu}_i=&
  \boldsymbol{\mu}^{\text{MCH}}(t) + \frac{i}{n}
  \left(
    \boldsymbol{\mu}^{\text{MCH}}(t+\Delta t)-\boldsymbol{\mu}^{\text{MCH}}(t)
  \right),\\
  \boldsymbol{\epsilon}_i=&\boldsymbol{\epsilon}\left(t+\frac{i}{n}\Delta t\right)
\end{align}
where $i$, $n$ $t$ and $\Delta t$ are defined as in section \ref{met:propagate}.

\subsection{Surface Hopping with laser fields}

If laser fields are present, there can be two fundamentally different types of hops: laser-induced hops and nonadiabatic hops. The latter ones are the same hops as in the laser-free simulations, and demand that the total energy is conserved. The laser-induced hops on the other hand demand that the momentum (kinetic energy) is conserved. Hence, \sharc\ needs to decide for every hop whether it is laser-induced or not. 

Consider a previous state $\alpha$ and a new state $\beta$. Currently, the hop is classified based on the energy gap $\Delta E=|E_\beta^\text{diag}-E_\alpha^\text{diag}|$ and the instantaneous central energy of the laser pulse $\omega$. 
The hop is assumed to be laser-induced if
\begin{equation}
  |\Delta E-\omega| < W,
\end{equation}
where $W$ is a fixed parameter. $W$ can be set using the input keyword \ttt{laserwidth}.

If a hop has been classified as laser-free, the momentum is adjusted according to the equations given in section~\ref{met:ekinadj}.


% ========================================================================================================= %

\section{Normal Mode Analysis}\label{met:nma}

The normal mode analysis can be used to find important vibrational modes in the excited-state dynamics.\cite{Kurtz2001JCP,Plasser2009}

Given a matrix $\VEC{Q}$ containing the normal mode vectors and a reference geometry $\VEC{R}^\text{ref}$, calculate for each trajectory
\begin{equation}
  R^i(t)=\VEC{Q}^{-1}(\VEC{R}^i(t)-\VEC{R}^\text{ref})
\end{equation}
to obtain the displacements in normal mode coordinates.
Averaging over the displacements gives the average trajectory:
\begin{equation}
  \bar{R}(t)=\frac{1}{N_\text{traj}}
  \sum_{i=1}^{N_\text{traj}} R^i(t)
\end{equation}
which should contain only coherent motion, since random motion cancels out in an ensemble.

A measure for the coherent activity in a mode is the standard deviation (over time) of the average trajectory:
\begin{equation}
  R_\text{coh}^2=
  \frac{1}{k_\text{end}-k_\text{start}}
  \sum_{k=k_\text{start}}^{k_\text{end}} \bar{R}(k\Delta t)^2
  -
  \left(
  \frac{1}{k_\text{end}-k_\text{start}}
  \sum_{k=k_\text{start}}^{k_\text{end}} \bar{R}(k\Delta t)
  \right)^2,
\end{equation}
where $k_\text{start}$ and $k_\text{end}$ are the start and end time steps for the analysis.
$R_\text{coh}$ a vector with one number per normal mode, where larger number mean that there is more coherent activity in this mode.

A measure for the total motion in a mode is the total standard deviation:
\begin{equation}
  R_\text{total}^2=
  \frac{1}{N_\text{traj}(k_\text{end}-k_\text{start})}
  \sum_{i=1}^{N_\text{traj}}
  \sum_{k=k_\text{start}}^{k_\text{end}} 
  R^i(k\Delta t)^2
  -
  \left(
  \frac{1}{N_\text{traj}(k_\text{end}-k_\text{start})}
  \sum_{i=1}^{N_\text{traj}}
  \sum_{k=k_\text{start}}^{k_\text{end}} 
  R^i(k\Delta t)
  \right)^2.
\end{equation}

% ========================================================================================================= %

\section{Optimization of Crossing Points}\label{met:orcaopt}

With \ttt{orca\_External} it is possible to optimize different kinds of crossing points.
In all cases, these optimizations involve the energies of the lower state $E_l$ and upper state $E_u$, the energy difference $\Delta E=E_u-E_l$, the gradients of the lower state $\VEC{g}_l$ and upper state $\VEC{g}_u$, the gradient difference vector $\VEC{d}$, and/or the nonadiabatic coupling vector $\VEC{t}$.

The simplest case is the optimization of minimum-energy crossing points between states of different multiplicity, because in this case the nonadiabatic coupling vector is zero and the branching space is one-dimensional.
In this case \cite{Bearpark1994CPL}, the energy to optimize is $E_u$ inside the intersection space, and $\Delta E$ inside the branching space.
The corresponding gradient to follow $\VEC{F}$ can be written as:
\begin{equation}
  \VEC{F}=\VEC{g}_u-\frac{\VEC{g}_u\cdot\VEC{d}}{\VEC{d}\cdot\VEC{d}}\VEC{d}+2(E_u-E_l)\frac{\VEC{d}}{|\VEC{d}|}.
\end{equation}

More complicated is the optimization of a conical intersection, between states of the same multiplicity, because the branching space is two-dimensional.
The corresponding gradient to follow $\VEC{F}$ is:
\begin{equation}
  \VEC{F}=\VEC{g}_u
  -\frac{\VEC{g}_u\cdot\VEC{d}}{\VEC{d}\cdot\VEC{d}}\VEC{d}
  -\frac{\VEC{g}_u\cdot\VEC{t}}{\VEC{t}\cdot\VEC{t}}\VEC{t}
  +2(E_u-E_l)\frac{\VEC{d}}{|\VEC{d}|},
\end{equation}
where $\VEC{d}$ and $\VEC{t}$ need to be orthogonalized.

If no nonadiabatic coupling vector is available because the interface cannot deliver them, conical intersections are optimized with the penalty function method of Levine et al.~\cite{Levine2008JPCB}.
The effective energy to optimize is defined as:
\begin{equation}
  E_\text{eff}=
  \frac{E_l+E_u}{2}
  +\sigma\frac{(E_u-E_l)^2}{E_u-E_l+\alpha}.
\end{equation}
This equation is a combination of the two main targets of the optimization, the average energy and the energy gap. 
The parameter $\sigma$ allows prioritizing either of the two, with a larger $\sigma$ leading to smaller energy gaps.
The parameter $\alpha$ is there to avoid the discontinuity at $E_u=E_l$.
The corresponding gradient to follow $\VEC{F}$ is:
\begin{equation}
  \VEC{F}=
  \frac{\VEC{g}_l+\VEC{g}_u}{2}
  +
  2\sigma\left[\frac{E_u-E_l}{E_u-E_l+\alpha}-\frac{1}{2}\left(\frac{E_u-E_l}{E_u-E_l+\alpha}\right)^2\right]\VEC{d}.
\end{equation}
Note that $\sigma$ and $\alpha$ might strongly influence the quality (i.e., with the penalty function method the optimization will not converge to the true minimum energy conical intersection point) of the result and the convergence behavior.
A large $\sigma$ and a small $\alpha$ will improve the quality of the result, but make the optimization harder to converge.


% ========================================================================================================= %

\section{Phase tracking}

\subsection{Phase tracking of the transformation matrix}\label{met:phase_track}

A Hermitian matrix $\VEC{H}^{\text{MCH}}$ can always be diagonalized. Its eigenvectors form the rows of a unitary matrix $\VEC{U}$, which can be used to transform between the original basis and the basis of the eigenfunctions of $\VEC{H}$. 
\begin{equation}
  \VEC{H}^{\text{diag}}=\VEC{U}^\dagger\VEC{H}^{\text{MCH}}\VEC{U}.
\end{equation}

However, the condition that $\VEC{U}$ diagonalizes $\VEC{H}^{\text{MCH}}$ is not sufficient to define $\VEC{U}$ uniquely. Each normalized eigenvector $\VEC{u}$ can be multiplied by a complex number on the unit circle and still remains a normalized eigenvector:
\begin{align}
  \VEC{H}\VEC{u}=h\VEC{u}
  \qquad&\text{and}\qquad
  \VEC{u}^\dagger\VEC{u}=1\\
  &\Rightarrow\nonumber\\
  \VEC{H}\left(\E^{\I\phi}\VEC{u}\right)
  =\E^{\I\phi}\left(\VEC{H}\VEC{u}\right)
  &=\E^{\I\phi}h\VEC{u}
  =h\left(\E^{\I\phi}\VEC{u}\right)\\
  &\text{and}\nonumber\\
  \left(\E^{\I\phi}\VEC{u}\right)^\dagger\left(\E^{\I\phi}\VEC{u}\right)
  =\VEC{u}^\dagger\E^{-\I\phi}&\E^{\I\phi}\VEC{u}
  =\VEC{u}^\dagger\VEC{u}=1
\end{align}
Thus, for all diagonal matrices $\boldsymbol{\Phi}$ with elements 
%%tth: $\delta_{\beta\alpha}\exp(\I\phi_\beta)$, 
\tthdump{
  $\delta_{\beta\alpha}\E^{\I\phi_\beta}$, 
}
also the matrix $\VEC{U}^\prime=\VEC{U}\boldsymbol{\Phi}$ diagonalizes $\VEC{H}^{\text{MCH}}$ (if $\VEC{U}$ diagonalizes it).

The propagation of the coefficients in the diagonal basis is written as (see section~\ref{met:propagate}):
\begin{equation}
  \VEC{c}^{\text{diag}}(t+\Delta t)=\underbrace{\VEC{U}^\dagger(t+\Delta t)\VEC{R}^{\text{MCH}}(t+\Delta t,t)\VEC{U}(t)}_{\VEC{R}^{\text{diag}}(t+\Delta t,t)}\VEC{c}^{\text{diag}}(t)
\end{equation}
where $\VEC{U}(t)$ and $\VEC{U}(t+\Delta t)$ are determined independently from diagonalizing the matrices $\VEC{H}^{\text{MCH}}(t)$ and $\VEC{H}^{\text{MCH}}(t+\Delta t)$, respectively. However, depending on the implementation of the diagonalization, $\VEC{U}(t)$ and $\VEC{U}(t+\Delta t)$ may carry unrelated, random phases. Even if $\VEC{H}^{\text{MCH}}(t)$ and $\VEC{H}^{\text{MCH}}(t+\Delta t)$ were identical, $\VEC{U}(t)$ and $\VEC{U}(t+\Delta t)$ might still differ, e.g.:
\begin{equation}
  \VEC{U}(t)=
  \begin{pmatrix}1&0\\0&1\end{pmatrix}
  \qquad\text{and}\qquad 
  \VEC{U}(t+\Delta t)=
  \begin{pmatrix}\I&0\\0&-\I\end{pmatrix}
\end{equation}
The result is that the coefficients $\VEC{c}$ pick up random phases during the propagation, leading to random changes in the direction of population transfer, invalidating the whole propagation.

In order to make the phases of $\VEC{U}(t)$ and $\VEC{U}(t+\Delta t)$ as similar as possible, \sharc\ employs a projection technique. First, we define the overlap matrix $\VEC{V}$ between $\VEC{U}(t)$ and $\VEC{U}(t+\Delta t)$:
\begin{equation}
  \VEC{V}=\VEC{U}^\dagger(t+\Delta t)\VEC{U}(t)
\end{equation}
For $\Delta t=0$, clearly
\begin{equation}
  \VEC{U}(t+\Delta t)\VEC{V}=\VEC{U}(t)
\end{equation}
and $\VEC{V}$ can be identified with the phase matrix $\boldsymbol{\Phi}$.

For $\Delta t\neq 0$, we must now find a matrix $\VEC{P}$ so that
\begin{equation}
  \VEC{U}(t+\Delta t)\VEC{P}=\VEC{U}^\prime(t+\Delta t)
\end{equation}
still diagonalizes $\VEC{H}^{\text{MCH}}(t+\Delta t)$, but which minimizes the phase change with regard to $\VEC{U}(t)$.
The matrix $\VEC{P}$ has elements
\begin{equation}
  P_{\beta\alpha}=V_{\beta\alpha}
  \delta\left(
    E_\beta-E_\alpha
  \right).
\end{equation}
where $E_\beta$ is the $\beta$-th eigenvalue of $\VEC{H}^{\text{MCH}}(t+\Delta t)$.

Within the \sharc\ algorithm, the phase of $\VEC{U}(t+\Delta t)$ is adjusted to be most similar to $\VEC{U}(t)$ by calculating first $\VEC{V}$, generating $\VEC{P}$ from $\VEC{V}$ and the eigenvalues of $\VEC{H}^{\text{MCH}}(t+\Delta t)$ and calculating the phase-corrected matrix $\VEC{U}^\prime(t+\Delta t)$ as $\VEC{U}(t+\Delta t)\VEC{P}$.

\subsection{Tracking of the phase of the MCH wave functions}

Additionally, within the quantum chemistry programs, the phases of the electronic wave functions may change from one time step to the next one. This will result in changes of the phase of all off-diagonal matrix elements (spin-orbit couplings, transition dipole moments, nonadiabatic couplings). \sharc\ has several possibilities to correct for that:
\begin{itemize}
  \item The interface can provide wave function phases through \ttt{QM.out}.
  \item If the overlap matrix is available, its diagonal contains the necessary phase information.
  \item Otherwise the scalar products of old and new nonadiabatic couplings and the relative phase of SOC matrix elements can be used to construct phase information.
\end{itemize}

% ========================================================================================================= %

\section{Random initial velocities}\label{met:veloc}

Random initial velocities are calculated with a given amount of kinetic energy $E$ per atom $a$. For each atom, the velocity is calculated as follows, with two uniform random numbers $\theta$ and $\phi$, from the interval $[0,1[$:
\begin{equation}
  \VEC{v}=\sqrt{2E/m_a}
  \begin{pmatrix}
    \cos{\theta}\sin{\phi}\\
    \sin{\theta}\sin{\phi}\\
    \cos{\phi}
  \end{pmatrix}
\end{equation}
This procedure gives a uniform probability distribution on a sphere with radius $\sqrt{2E/m_a}$.

Note that the translational and rotational components of random initial velocities are not projected out in the current implementation.

Random initial velocities can be requested in the input with \ttt{veloc random $E$}, where $E$ is a float defining the kinetic energy per atom (in eV).

% ========================================================================================================= %

\section{Representations}\label{sec:repr}

Within \sharc, two different representations for the electronic states are used. The first is the so-called MCH basis, which is the basis of the eigenfunctions of the molecular Coulomb Hamiltonian. The molecular Coulomb Hamiltonian is the standard electronic Hamiltonian employed by the majority of quantum chemistry programs. It contains only the kinetic energy of the electrons and the potential energy arising from the Coulomb interaction between the electrons and nuclei.
\begin{equation}
  \hat{H}_{\text{el}}^{\text{MCH}}
  =\hat{K}_{\E}
  +\hat{V}_{\text{ee}}
  +\hat{V}_{\text{ne}}
  +\hat{V}_{\text{nn}}.
\end{equation}
With this hamiltonian, states of the same multiplicity couple via the nonadiabatic couplings, while states of different multiplicity do not interact at all. 

The second representation used in \sharc\ is the so-called diagonal representation. It is the basis of the eigenfunctions of the total Hamiltonian.
\begin{equation}
  \hat{H}_{\text{el}}^{\text{total}}
  =\hat{H}_{\text{el}}^{\text{MCH}}
  +\hat{H}_{\text{el}}^{\text{coup}}.
\end{equation}
The term $\hat{H}_{\text{el}}^{\text{coup}}$ contains additional couplings not contained in the molecular Coulomb Hamiltonian. The most common couplings are spin-orbit couplings and interactions with an external electric field.
\begin{equation}
  \hat{H}_{\text{el}}^{\text{coup}}=\hat{H}_{\text{el}}^{\text{SOC}}-\boldsymbol{\mu}\boldsymbol{\epsilon}^{\text{ext}}
\end{equation}
Both of these couplings introduce off-diagonal elements in the total Hamiltonian. Thus, the eigenfunctions of the molecular Coulomb Hamiltonian are not the eigenfunctions of the total Hamiltonian. 

Within \sharc, usually quantum chemistry information is read in the MCH representation, while the surface hopping is performed in the diagonal one.

\subsection{Current state in MCH representation}\label{ssec:state_transform}

Oftentimes, it is very useful to know to which MCH state the currently active diagonal state corresponds. If $\hat{H}_{\text{el}}^{\text{coup}}$ is small or the state separation is large, then each diagonal state approximately corresponds to one MCH state. Only in the case of large couplings and/or near-degenerate states are the MCH states strongly mixed in the diagonal states.

In order to obtain for a given time step from the currently active diagonal state $\beta$ the corresponding MCH state $\alpha$, a vector $\VEC{c}^\text{diag}$ with $c_i^\text{diag}=\delta_{i\beta}$ is generated. The vector is transformed into the MCH representation
\begin{equation}
  \VEC{c}^\text{MCH}=\VEC{U}\VEC{c}^\text{diag}.
\end{equation}
The corresponding MCH state $\alpha$ is the index of the (absolute) largest element of vector $\VEC{c}^\text{MCH}$.

% ========================================================================================================= %

\section{Sampling from Wigner Distribution}\label{met:wigner}

The sampling is based on references~\cite{Dahl1988JCP, Schinke1995}.

Besides the equilibrium geometry $\VEC{R}_{\text{eq}}$, the optimization plus frequency calculation provides a set of vibrational frequencies $\{\nu_i\}$ and the corresponding normal mode vectors $\{\VEC{n}_i\}$, where $i$ runs from 1 to $N=3n_{\text{atom}}$.

The normal mode vectors need to be provided in terms of mass-weighted Cartesian normal modes, in units of $[l[\text{a.u.}]\cdot\sqrt{m[\text{a.u.}]}]$.
Most quantum chemistry programs follow different conventions when writing \textsc{Molden} files. \textsc{Molpro} and \textsc{Molcas} write these files with unweighted Cartesian normal modes, with units of $[l[\text{a.u.}]]$. \textsc{Gaussian}, \textsc{Turbomole}, \textsc{Q-Chem}, \textsc{ADF}, and \textsc{Orca} employ what could be called the "\textsc{Gaussian} convention", which are normalized Cartesian normal modes. \textsc{Columbus} uses yet another convention in the output of their \ttt{suscal.x} module.
The script \ttt{wigner.py} automatically transforms these different conventions; it does so by applying all possible transformations to the input data until it finds one transformation which produces an orthonormal normal mode matrix.
The latter one is then used for the Wigner sampling.

In order to create an initial condition $(\VEC{R},\VEC{v})$, the following procedure is applied. Initially, $\VEC{R}_0=\VEC{R}_{\text{eq}}$ and $\VEC{v}_0=0$. Then, for each normal mode $i$, two random numbers $P_i$ and $Q_i$ are chosen uniformly from the interval $[-5,5]$. The value of a ground state quantum Wigner distribution for these values is calculated:
\begin{equation}
  W_i=\E^{-(P_i^2+Q_i^2)}.
\end{equation}
$W_i$ is compared to a uniform random $r_i$ number from $[0,1]$. If $W_i>r_i$, then $P_i$ and $Q_i$ are accepted and the coordinates and velocities are updated:
\begin{align}
  \VEC{R}_i=&\VEC{R}_{i-1} + \frac{Q}{\sqrt{2\nu_i}}\VEC{n}_i           \label{eq:wigner1}\\
  \VEC{v}_i=&\VEC{v}_{i-1} + \frac{P\sqrt{\nu_i}}{\sqrt{2}}\VEC{n}_i    \label{eq:wigner2}
\end{align}
The random number procedure and updates are repeated for all normal modes, until $(\VEC{R}_N,\VEC{v})_N$ is obtained, which constitutes one initial condition. Finally, the center of mass is restored and translational and rotational components are projected out of $\VEC{v}$. The harmonic potential energy is given by:
\begin{equation}
  E_{\text{pot}}=\frac{1}{2}\sum\limits_i \nu_iQ_i^2
\end{equation}

\subsection{Sampling at Non-zero Temperature}

% \todo{Temperature reference?}

In the case of a non-zero temperature, the molecule might not be in the vibrational ground state of the harmonic oscillator, but rather in an excited vibrational state.
For a given mode $i$, the probability to be in any given vibrational state $j$ ($j=0$ is the ground state) is:
\begin{equation}
  w_{ij}=\E^{-y\frac{j+1}{2}}\left(\frac{\E^{-\frac{y}{2}}}{1-\E^{-y}}\right)^{-1},
\end{equation}
where $y$ is $\nu_i$ divided by $k_BT$.
In order to find the vibrational state for mode $i$, a random number is drawn (from $[0,1]$) and used as in equation~\eqref{eq:cumuprob}.

The displacements and velocity contributions for mode $i$ in state $j$ are then obtained as in equations~\eqref{eq:wigner1} and \eqref{eq:wigner2}, except that the Wigner distribution for state $j$ is calculated as:
\begin{equation}
  W_{ij}
  =
  (-1)^j
  \E^{-(P_i^2+Q_i^2)}
  \sum_m^j
  (-1)^m
  \frac{j!}{(j-m)!(m!)^2}
  \left(2P_i^2+2Q_i^2\right)^m.
\end{equation}




% ========================================================================================================= %

\section{Scaling}\label{met:scaling}

The scaling factor (keyword \ttt{scaling}) applies to all energies and derivatives of energies. Hence, the full Hamiltonian is scaled, and the gradients are scaled. Nothing else is scaled (no dipole moments, nonadiabatic couplings, overlaps, etc).

% ========================================================================================================= %

\section{Seeding of the RNG}\label{met:seed}

The standard Fortran 90 random number generator (used for \ttt{sharc.x}, but not for the auxiliary scripts) is seeded by a sequence of integers of length $n$, where $n$ depends on the computer architecture. The input of \sharc, however, takes only a single RNG seed, which must reproducibly produce the same sequence of random numbers for the same input.

In order to generate the seed sequence from the single input $x$, the following procedure is applied:
\begin{itemize}
  \item Query for the number $n$,
  \item Generate a first seed sequence $\VEC{s}$ with $s_i=x+37i+17i^2$,
  \item Seed with the sequence $\VEC{s}$,
  \item Obtain a sequence $\VEC{r}$ of $n$ random numbers on the interval $[0,1[$,
  \item Generate a second seed sequence $\VEC{s}^\prime$ with $s_i^\prime=\text{int}\left(65536(r_i-\frac{1}{2})\right)$,
  \item Reseed with the sequence $\VEC{s}^\prime$.
\end{itemize}
The fifth step will generate a sequence of nearly uncorrelated numbers, distributed uniformly over the full range of possible integer values. 

% ========================================================================================================= %

\section{Selection of gradients and nonadiabatic couplings}\label{met:selection}

In order to increase performance, it is possible to omit the calculation of certain gradients and nonadiabatic couplings. An energy-gap-based algorithm selects at each time step a subset of all possible gradients and nonadiabatic couplings to be calculated. Given the diagonal energy $E^{\text{diag}}_\xi$ of the current active state $\xi$, the gradient $\VEC{g}^{\text{MCH}}_\alpha$ of MCH state $\alpha$ is calculated if:
\begin{equation}
  \left|
    E^{\text{diag}}_\xi - E^{\text{MCH}}_\alpha
  \right|
  <
  \varepsilon_\text{grad}
\end{equation}
where $\varepsilon_\text{grad}$ is the selection threshold.

Similarly, a nonadiabatic coupling vector $\VEC{K}^{\text{MCH}}_{\beta\alpha}$ is calculated if:
\begin{equation}
  \left|
    E^{\text{diag}}_\xi - E^{\text{MCH}}_\alpha
  \right|
  <
  \varepsilon_\text{nac}
  \qquad\text{and}\qquad
  \left|
    E^{\text{diag}}_\xi - E^{\text{MCH}}_\beta
  \right|
  <
  \varepsilon_\text{nac}
\end{equation}
with selection threshold $\varepsilon_\text{nac}$.

Neither $\VEC{g}^{\text{MCH}}_\alpha$ nor $\VEC{K}^{\text{MCH}}_{\beta\alpha}$ are ever calculated if $\alpha$ or $\beta$ are frozen states.

There is only one keyword (\ttt{eselect}) to set the selection threshold, so $\varepsilon_\text{grad}$ and $\varepsilon_\text{nac}$ are the same in most cases. 

% ========================================================================================================= %

\section{State ordering}\label{met:ordering}

The canonical ordering of MCH states of different $S$ and $M_S$ in \sharc\ is as follows. In the innermost loop, the quantum number is increased; then $M_S$ and finally $S$. Example:

\begin{example}
  \verb|nstates 3 0 3|
\end{example}

In this example, the order of states is given as:

\begin{tabular}{rlccc}
  \hline
  Number      &Label       &$S$ &$M_S$  &$n$\\
  \hline
  1&$S_0$       &0&0&1\\
  2&$S_1$       &0&0&2\\
  3&$S_2$       &0&0&3\\
  4&$T_1^-$       &2&-1&1\\
  5&$T_2^-$       &2&-1&2\\
  6&$T_3^-$       &2&-1&3\\
  7&$T_1^0$       &2&0&1\\
  8&$T_2^0$       &2&0&2\\
  9&$T_3^0$       &2&0&3\\
  10&$T_1^+$       &2&+1&1\\
  11&$T_2^+$       &2&+1&2\\
  12&$T_3^+$       &2&+1&3\\
  \hline
\end{tabular}

The canonical ordering of states is for example important in order to specify the initial state in the MCH basis (using the \ttt{state} keyword in the input file).

Note that the diagonal states do not follow the same prescription. Since the diagonal states are in general not eigenfunctions of the total spin operator, they do not have a well-defined multiplicity. 
Hence, the diagonal states are simply ordered by increasing energy.

% ========================================================================================================= %

\section{Surface Hopping}\label{met:hopping}

Given two coefficient vectors $\VEC{c}^{\text{diag}}(t)$ and $\VEC{c}^{\text{diag}}(t+\Delta t)$ and the corresponding propagator matrix $\VEC{R}^{\text{diag}}(t+\Delta t,t)$, the surface hopping probabilities are given by
\begin{equation}
  P_{\beta\rightarrow\alpha}=
  \left(
    1-
    \frac{
      \left|
        c_\beta^{\text{diag}}(t+\Delta t)
      \right|^2
    }{
      \left|
        c_\beta^{\text{diag}}(t)
      \right|^2
    }\right)
    \times
    \frac{
      \Re\left[
        c^{\text{diag}}_\alpha(t+\Delta t)
        R^*_{\alpha\beta}
        \left(
          c^{\text{diag}}_\beta(t)
        \right)^*
      \right]
    }{
      \left|
        c^{\text{diag}}_\beta(t)
      \right|^2
      -\Re\left[
        c^{\text{diag}}_\beta(t+\Delta t)
        R^*_{\beta\beta}
        \left(
          c^{\text{diag}}_\beta(t)
        \right)^*
      \right]
    }.
\end{equation}
where, however, $P_{\beta\rightarrow\beta}=0$ and all negative $P_{\beta\rightarrow\alpha}$ are set to zero.
This equation is the default in \sharc, and can be used with \ttt{hopping\_procedure sharc}.

Alternatively, the hopping probabilities can be obtained with the ``global flux surface hopping'' method by Prezhdo and coworkers \cite{Wang2014JCTC}.
The equation is:
\begin{equation}
  P_{\beta\rightarrow\alpha}=
  \left(
    1-
    \frac{
      \left|
        c_\beta^{\text{diag}}(t+\Delta t)
      \right|^2
    }{
      \left|
        c_\beta^{\text{diag}}(t)
      \right|^2
    }\right)
    \times
    \frac{
      |c^{\text{diag}}_\alpha(t+\Delta t)|^2-|c^{\text{diag}}_\alpha(t)|^2
    }{
      \sum_{i} \max\left[0,-(|c^{\text{diag}}_i(t+\Delta t)|^2-|c^{\text{diag}}_i(t)|^2)\right]
    }.
\end{equation}
As above, $P_{\beta\rightarrow\beta}=0$ and all negative $P_{\beta\rightarrow\alpha}$ are set to zero.
This equation and can be used with \ttt{hopping\_procedure gfsh}.

In any case, the hopping procedure itself obtains a uniform random number $r$ from the interval $[0,1]$. A hop to state $\alpha$ is performed, if
\begin{equation}
  \sum\limits_{i=1}^{\alpha-1} P_{\beta\rightarrow i} < r \le P_{\beta\rightarrow\alpha}+\sum\limits_{i=1}^{\alpha-1} P_{\beta\rightarrow i}
  \label{eq:cumuprob}
\end{equation}
See section~\ref{met:refl_frust} for further details on how frustrated hops (hops according to the hopping probabilities, but where not enough energy is available to execute the hop) are handled.

% ========================================================================================================= %

\section{Velocity Verlet}

The nuclear coordinates of atom $A$ are updated according to the Velocity Verlet algorithm \cite{Verlet1967PR}, based on the gradient of state $\beta$ at $\VEC{R}(t)$ and $\VEC{R}(t+\Delta t)$:
\begin{align}
  \VEC{a}_A(t)=&
  -\frac{1}{m_A}\nabla_{\VEC{R_A}}E_\beta(\VEC{R}(t))\\
  \VEC{a}_A(t+\Delta t)=&
  -\frac{1}{m_A}\nabla_{\VEC{R_A}}E_\beta(\VEC{R}(t+\Delta t))\\
  \VEC{R}_A(t+\Delta t)=&
  \VEC{R}_A(t)+\VEC{v}_A(t)\Delta t + \frac{1}{2}\VEC{a}_A(t)\Delta t^2\\
  \VEC{v}_A(t+\Delta t)=&
  \VEC{v}_A(t)+\frac{1}{2}\left[\VEC{a}_A(t)+\VEC{a}_A(t+\Delta t)\right]\Delta t
\end{align}

Currently, there are no other integrators for the nuclear motion implemented in \sharc.

% ========================================================================================================= %

\section{Wavefunction propagation}\label{met:propagate}

The electronic wave function is needed in order to carry out surface hopping. The electronic wave function is expanded in the basis of the so-called model space $\mathcal{S}$, which includes the few lowest states $|\psi^{\text{MCH}}_\alpha\rangle$ of the multiplicities under consideration (e.g.\ the 3 lowest singlet and 2 lowest triplet states). 
\begin{equation}
  \Psi_{\text{el}}(t)=\sum\limits_{\alpha\in\mathcal{S}} c^{\text{MCH}}_\alpha \left|\psi^{\text{MCH}}_\alpha\right\rangle
\end{equation}
All multiplet components are included explicitly, i.e., the inclusion of an MCH triplet state adds three explicit states to the model space (the three components of the triplet).

Within \sharc, the wave function is represented just by the vector $\VEC{c}^{\text{MCH}}$. The Hamiltonian $\VEC{H}^{\text{MCH}}$ is represented in matrix form with elements:
\begin{equation}
  H^{\text{MCH}}_{\beta\alpha}=\left\langle\psi^{\text{MCH}}_\beta\middle|\hat{H}_{\text{el}}^{\text{total}}\middle|\psi^{\text{MCH}}_\alpha\right\rangle
\end{equation}

From the MCH representation, the diagonal representation can be obtained by unitary transformation within the model space $\mathcal{S}$ ($\VEC{U}^\dagger\VEC{H}^{\text{MCH}}\VEC{U}=\VEC{H}^{\text{diag}}$ and $\VEC{U}^\dagger\VEC{c}^{\text{MCH}}=\VEC{c}^{\text{diag}}$):
\begin{equation}
  \Psi_{\text{el}}(t)=\sum\limits_{\alpha\in\mathcal{S}} c^{\text{diag}}_\alpha \left|\psi^{\text{diag}}_\alpha\right\rangle
\end{equation}
and
\begin{equation}
  H^{\text{diag}}_{\beta\alpha}=\left\langle\psi^{\text{diag}}_\beta\middle|\hat{H}_{\text{el}}^{\text{total}}\middle|\psi^{\text{diag}}_\alpha\right\rangle
\end{equation}

The propagation of the electronic wave function from time $t$ to $t+\Delta t$ can then be written as the product of a propagation matrix with the coefficients at time $t$:
\begin{equation}
  \VEC{c}^{\text{diag}}(t+\Delta t)=\VEC{R}^{\text{diag}}(t+\Delta t,t)\VEC{c}^{\text{diag}}(t)
\end{equation}
or
\begin{equation}
  \VEC{c}^{\text{diag}}(t+\Delta t)=\underbrace{\VEC{U}^\dagger(t+\Delta t)\VEC{R}^{\text{MCH}}(t+\Delta t,t)\VEC{U}(t)}_{\VEC{R}^{\text{diag}}(t+\Delta t,t)}\VEC{c}^{\text{diag}}(t)
\end{equation}

In order to calculate $\VEC{R}^{\text{MCH}}(t+\Delta t,t)$, \sharc\ uses (unitary) operator exponentials. 

\subsection{Propagation using nonadiabatic couplings}

Here we assume that in the dynamics the interaction between the electronic states is described by a matrix of nonadiabatic couplings $\VEC{K}^{\text{MCH}}(t)$, such that
\begin{equation}
  \left(\VEC{K}^{\text{MCH}}(t)\right)_{\beta\alpha}
  =
  \left\langle
    \psi_\beta(t)
  \middle|
    \frac{\partial}{\partial t}
  \middle|
    \psi_\alpha(t)
  \right\rangle
  \label{eq:ddt}
\end{equation}
or
\begin{equation}
  \left(\VEC{K}^{\text{MCH}}(t)\right)_{\beta\alpha}
  =
  \frac{\partial \VEC{R}}{\partial t}\cdot
  \left\langle
    \psi_\beta(t)
  \middle|
    \frac{\partial}{\partial \VEC{R}}
  \middle|
    \psi_\alpha(t)
  \right\rangle.
  \label{eq:ddr}
\end{equation}
In equation~\eqref{eq:ddt}, the time-derivative couplings are directly calculated by the quantum chemistry program (use \ttt{coupling ddt} in the \sharc\ input), while in~\eqref{eq:ddr} the matrix $\VEC{K}^{\text{MCH}}(t)$ is obtained from the scalar product of the nuclear velocity and the nonadiabatic coupling vectors (use \ttt{coupling ddr} in the input).

The propagation matrix can then be written as 
\begin{equation}
  \VEC{R}^{\text{MCH}}(t+\Delta t,t)=
  \hat{\mathfrak{T}}
  \exp\left[
    -\int\limits_{t}^{t+\Delta t}
    \left(
      \frac{\I}{\hbar}\VEC{H}^{\text{MCH}}(\tau)+\VEC{K}^{\text{MCH}}(\tau) 
    \right)\D\tau
  \right]
\end{equation}
with the time-ordering operator $\hat{\mathfrak{T}}$. For small time steps $\Delta t$, $\VEC{H}^{\text{MCH}}(\tau)$ and $\VEC{K}^{\text{MCH}}(\tau)$ can be interpolated linearly
\begin{equation}
  \VEC{R}^{\text{MCH}}(t+\Delta t,t)=
  \exp\left[
    -\frac{1}{2}\left(
      \frac{\I}{\hbar}\VEC{H}^{\text{MCH}}(t)+\frac{\I}{\hbar}\VEC{H}^{\text{MCH}}(t+\Delta t)
      +\VEC{K}^{\text{MCH}}(t)+\VEC{K}^{\text{MCH}}(t+\Delta t)
    \right)\Delta t
  \right]
\end{equation}
And in order to have a sufficiently small time step for this to work, the interval $(t,t+\Delta t)$ is further split into subtime steps $\Delta\tau=\frac{\Delta t}{n}$. 
\begin{align}
  \VEC{R}^{\text{MCH}}(t+\Delta t,t)=&
  \prod\limits_{i=1}^{n}
  \VEC{R}_i\\
  \VEC{R}_i=&
  \exp\left[
      -\left(
        \frac{\I}{\hbar}\VEC{H}_i
        +\VEC{K}_i
      \right)\Delta\tau
  \right]\\
  \VEC{H}_i=&
  \VEC{H}^{\text{MCH}}(t) + \frac{i}{n}
  \left(
    \VEC{H}^{\text{MCH}}(t+\Delta t)-\VEC{H}^{\text{MCH}}(t)
  \right)\label{eq:ham_propn}\\
  \VEC{K}_i=&
  \VEC{K}^{\text{MCH}}(t) + \frac{i}{n}
  \left(
    \VEC{K}^{\text{MCH}}(t+\Delta t)-\VEC{K}^{\text{MCH}}(t)
  \right)
\end{align}

\subsection{Propagation using overlap matrices}

In many situations, the nonadiabatic couplings in $\VEC{K}^{\text{MCH}}$ are very localized on the potential hypersurfaces. If this is the case, in the dynamics very short time steps are necessary to properly sample the nonadiabatic couplings. If too large time steps are used, part of the coupling may be missed, leading to wrong population transfer. The local diabatization algorithm gives more numerical stability in these situations. It can be requested with the line \ttt{coupling overlap} in the input file.

Within this algorithm, the change of the electronic states between time steps is described by the overlap matrix $\VEC{S}^{\text{MCH}}(t,t+\Delta t)$
\begin{equation}
  \left(\VEC{S}^{\text{MCH}}(t,t+\Delta t)\right)_{\beta\alpha}=
  \left\langle
    \psi_\beta(t)
  \middle|
    \psi_\alpha(t+\Delta t)
  \right\rangle
\end{equation}

With this, the propagator matrix can be written as
\begin{align}
  \VEC{R}^{\text{MCH}}(t+\Delta t,t)=&
  \VEC{S}^{\text{MCH}}(t,t+\Delta t)^\dagger\prod\limits_{i=1}^{n}
  \VEC{R}_i\\
  \VEC{R}_i=&
  \exp\left[
      -\frac{\I}{\hbar}\VEC{H}_i\Delta\tau
  \right]\\
  \VEC{H}_i=&
  \VEC{H}^{\text{MCH}}(t) + \frac{i}{n}
  \left(
    \VEC{H}^{\text{MCH}}_{\text{tra}}
    -\VEC{H}^{\text{MCH}}(t)
  \right)\label{eq:ham_propl}\\
  \VEC{H}^{\text{MCH}}_{\text{tra}}=&
    \VEC{S}^{\text{MCH}}(t,t+\Delta t)
    \VEC{H}^{\text{MCH}}(t+\Delta t)
    \VEC{S}^{\text{MCH}}(t,t+\Delta t)^\dagger
\end{align}








% ========================================================================================================= %
% ========================================================================================================= %
% ========================================================================================================= %



%%tth: \chapter{Bibliography}
\begin{unnumbered}
  \tthdump{
    \phantomsection
    \addcontentsline{toc}{chapter}{Bibliography}
  }
  \bibliography{mai_bibliography}
  \bibliographystyle{paper-sebastian}
\end{unnumbered}

% ========================================================================================================= %

%%tth: \chapter{List of Tables}
\begin{unnumbered}
  \tthdump{
    \phantomsection
    \addcontentsline{toc}{chapter}{List of Tables}
  }
  \listoftables
\end{unnumbered}

% ========================================================================================================= %

%%tth: \chapter{List of Figures}
\begin{unnumbered}
  \tthdump{
    \phantomsection
    \addcontentsline{toc}{chapter}{List of Figures}
  }
  \listoffigures
\end{unnumbered}

\end{document}
